<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Regression Discontinuity Designs (RDD) | Notes on Causal Models</title>
  <meta name="description" content="This is a collection of notes from open sources" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Regression Discontinuity Designs (RDD) | Notes on Causal Models" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a collection of notes from open sources" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Regression Discontinuity Designs (RDD) | Notes on Causal Models" />
  
  <meta name="twitter:description" content="This is a collection of notes from open sources" />
  

<meta name="author" content="DEA" />


<meta name="date" content="2024-08-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="instrumental-variables-iv.html"/>
<link rel="next" href="fixed-effects-and-panel-data-methods.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Causal Model Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Acknowledgement</a></li>
<li class="chapter" data-level="1" data-path="causal-models.html"><a href="causal-models.html"><i class="fa fa-check"></i><b>1</b> Causal Models</a>
<ul>
<li class="chapter" data-level="1.1" data-path="causal-models.html"><a href="causal-models.html#concepts"><i class="fa fa-check"></i><b>1.1</b> Concepts</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="causal-models.html"><a href="causal-models.html#goodness-of-fit"><i class="fa fa-check"></i><b>1.1.1</b> Goodness of Fit</a></li>
<li class="chapter" data-level="1.1.2" data-path="causal-models.html"><a href="causal-models.html#robustness-checks-and-validation-methods"><i class="fa fa-check"></i><b>1.1.2</b> Robustness checks and validation methods</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="causal-models.html"><a href="causal-models.html#directed-acyclic-graphs-dags"><i class="fa fa-check"></i><b>1.2</b> Directed Acyclic Graphs (DAGs)</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="causal-models.html"><a href="causal-models.html#confounder"><i class="fa fa-check"></i><b>1.2.1</b> Confounder</a></li>
<li class="chapter" data-level="1.2.2" data-path="causal-models.html"><a href="causal-models.html#collider"><i class="fa fa-check"></i><b>1.2.2</b> Collider</a></li>
<li class="chapter" data-level="1.2.3" data-path="causal-models.html"><a href="causal-models.html#what-to-do"><i class="fa fa-check"></i><b>1.2.3</b> What to do</a></li>
<li class="chapter" data-level="1.2.4" data-path="causal-models.html"><a href="causal-models.html#how-to-do"><i class="fa fa-check"></i><b>1.2.4</b> How to do</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="causal-models.html"><a href="causal-models.html#bad-controls"><i class="fa fa-check"></i><b>1.3</b> Bad Controls</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="causal-models.html"><a href="causal-models.html#key-points-on-bad-controls-by-joshua-angrist"><i class="fa fa-check"></i><b>1.3.1</b> Key Points on “Bad Controls” by Joshua Angrist:</a></li>
<li class="chapter" data-level="1.3.2" data-path="causal-models.html"><a href="causal-models.html#example-from-angrist-and-pischkes-mostly-harmless-econometrics"><i class="fa fa-check"></i><b>1.3.2</b> Example from Angrist and Pischke’s “Mostly Harmless Econometrics”:</a></li>
<li class="chapter" data-level="1.3.3" data-path="causal-models.html"><a href="causal-models.html#practical-advice"><i class="fa fa-check"></i><b>1.3.3</b> Practical Advice:</a></li>
<li class="chapter" data-level="1.3.4" data-path="causal-models.html"><a href="causal-models.html#summary"><i class="fa fa-check"></i><b>1.3.4</b> Summary:</a></li>
<li class="chapter" data-level="1.3.5" data-path="causal-models.html"><a href="causal-models.html#unobserved-variable-affecting-only-the-dependent-variable"><i class="fa fa-check"></i><b>1.3.5</b> Unobserved Variable Affecting Only the Dependent Variable</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="causal-models.html"><a href="causal-models.html#external-and-internal-validity-in-econometrics"><i class="fa fa-check"></i><b>1.4</b> External and Internal Validity in Econometrics</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="causal-models.html"><a href="causal-models.html#internal-validity"><i class="fa fa-check"></i><b>1.4.1</b> Internal Validity</a></li>
<li class="chapter" data-level="1.4.2" data-path="causal-models.html"><a href="causal-models.html#external-validity"><i class="fa fa-check"></i><b>1.4.2</b> External Validity</a></li>
<li class="chapter" data-level="1.4.3" data-path="causal-models.html"><a href="causal-models.html#balancing-internal-and-external-validity"><i class="fa fa-check"></i><b>1.4.3</b> Balancing Internal and External Validity</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="causal-models.html"><a href="causal-models.html#endogeneity"><i class="fa fa-check"></i><b>1.5</b> Endogeneity</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="causal-models.html"><a href="causal-models.html#sources-of-endogeneity"><i class="fa fa-check"></i><b>1.5.1</b> Sources of Endogeneity:</a></li>
<li class="chapter" data-level="1.5.2" data-path="causal-models.html"><a href="causal-models.html#consequences-of-endogeneity"><i class="fa fa-check"></i><b>1.5.2</b> Consequences of Endogeneity:</a></li>
<li class="chapter" data-level="1.5.3" data-path="causal-models.html"><a href="causal-models.html#methods-to-address-endogeneity"><i class="fa fa-check"></i><b>1.5.3</b> Methods to Address Endogeneity:</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="causal-models.html"><a href="causal-models.html#reduced-form-model"><i class="fa fa-check"></i><b>1.6</b> Reduced Form Model</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="causal-models.html"><a href="causal-models.html#characteristics-of-reduced-form-models"><i class="fa fa-check"></i><b>1.6.1</b> Characteristics of Reduced Form Models:</a></li>
<li class="chapter" data-level="1.6.2" data-path="causal-models.html"><a href="causal-models.html#uses-of-reduced-form-models"><i class="fa fa-check"></i><b>1.6.2</b> Uses of Reduced Form Models:</a></li>
<li class="chapter" data-level="1.6.3" data-path="causal-models.html"><a href="causal-models.html#example-of-a-reduced-form-model"><i class="fa fa-check"></i><b>1.6.3</b> Example of a Reduced Form Model:</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="causal-models.html"><a href="causal-models.html#standard-errors"><i class="fa fa-check"></i><b>1.7</b> Standard Errors</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="causal-models.html"><a href="causal-models.html#heteroskedasticity-consistent-standard-errors"><i class="fa fa-check"></i><b>1.7.1</b> heteroskedasticity-consistent standard errors</a></li>
<li class="chapter" data-level="1.7.2" data-path="causal-models.html"><a href="causal-models.html#why-use-sandwich-standard-errors"><i class="fa fa-check"></i><b>1.7.2</b> Why Use Sandwich Standard Errors?</a></li>
<li class="chapter" data-level="1.7.3" data-path="causal-models.html"><a href="causal-models.html#clustering-standard-errors"><i class="fa fa-check"></i><b>1.7.3</b> Clustering Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="causal-models.html"><a href="causal-models.html#types-of-biases"><i class="fa fa-check"></i><b>1.8</b> Types of Biases</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="causal-models.html"><a href="causal-models.html#selection-bias"><i class="fa fa-check"></i><b>1.8.1</b> 1. Selection Bias</a></li>
<li class="chapter" data-level="1.8.2" data-path="causal-models.html"><a href="causal-models.html#omitted-variable-bias"><i class="fa fa-check"></i><b>1.8.2</b> 2. Omitted Variable Bias</a></li>
<li class="chapter" data-level="1.8.3" data-path="causal-models.html"><a href="causal-models.html#measurement-bias"><i class="fa fa-check"></i><b>1.8.3</b> 3. Measurement Bias</a></li>
<li class="chapter" data-level="1.8.4" data-path="causal-models.html"><a href="causal-models.html#response-bias"><i class="fa fa-check"></i><b>1.8.4</b> 4. Response Bias</a></li>
<li class="chapter" data-level="1.8.5" data-path="causal-models.html"><a href="causal-models.html#attrition-bias"><i class="fa fa-check"></i><b>1.8.5</b> 5. Attrition Bias</a></li>
<li class="chapter" data-level="1.8.6" data-path="causal-models.html"><a href="causal-models.html#publication-bias"><i class="fa fa-check"></i><b>1.8.6</b> 6. Publication Bias</a></li>
<li class="chapter" data-level="1.8.7" data-path="causal-models.html"><a href="causal-models.html#survivorship-bias"><i class="fa fa-check"></i><b>1.8.7</b> 7. Survivorship Bias</a></li>
<li class="chapter" data-level="1.8.8" data-path="causal-models.html"><a href="causal-models.html#recall-bias"><i class="fa fa-check"></i><b>1.8.8</b> 8. Recall Bias</a></li>
<li class="chapter" data-level="1.8.9" data-path="causal-models.html"><a href="causal-models.html#confirmation-bias"><i class="fa fa-check"></i><b>1.8.9</b> 9. Confirmation Bias</a></li>
<li class="chapter" data-level="1.8.10" data-path="causal-models.html"><a href="causal-models.html#confounding-bias"><i class="fa fa-check"></i><b>1.8.10</b> 10. Confounding Bias</a></li>
<li class="chapter" data-level="1.8.11" data-path="causal-models.html"><a href="causal-models.html#endogeneity-bias"><i class="fa fa-check"></i><b>1.8.11</b> 11. Endogeneity Bias</a></li>
<li class="chapter" data-level="1.8.12" data-path="causal-models.html"><a href="causal-models.html#non-response-bias"><i class="fa fa-check"></i><b>1.8.12</b> 12. Non-Response Bias</a></li>
<li class="chapter" data-level="1.8.13" data-path="causal-models.html"><a href="causal-models.html#observer-bias"><i class="fa fa-check"></i><b>1.8.13</b> 13. Observer Bias</a></li>
<li class="chapter" data-level="1.8.14" data-path="causal-models.html"><a href="causal-models.html#overfitting-bias"><i class="fa fa-check"></i><b>1.8.14</b> 14. Overfitting Bias</a></li>
<li class="chapter" data-level="1.8.15" data-path="causal-models.html"><a href="causal-models.html#addressing-biases"><i class="fa fa-check"></i><b>1.8.15</b> Addressing Biases</a></li>
<li class="chapter" data-level="1.8.16" data-path="causal-models.html"><a href="causal-models.html#self-selection-bias"><i class="fa fa-check"></i><b>1.8.16</b> Self-Selection Bias</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="potential-outcomes.html"><a href="potential-outcomes.html"><i class="fa fa-check"></i><b>2</b> Potential Outcomes</a>
<ul>
<li class="chapter" data-level="2.1" data-path="potential-outcomes.html"><a href="potential-outcomes.html#key-concepts"><i class="fa fa-check"></i><b>2.1</b> Key Concepts</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="potential-outcomes.html"><a href="potential-outcomes.html#causal-effect"><i class="fa fa-check"></i><b>2.1.1</b> Causal Effect</a></li>
<li class="chapter" data-level="2.1.2" data-path="potential-outcomes.html"><a href="potential-outcomes.html#average-treatment-effect-ate"><i class="fa fa-check"></i><b>2.1.2</b> Average Treatment Effect (ATE)</a></li>
<li class="chapter" data-level="2.1.3" data-path="potential-outcomes.html"><a href="potential-outcomes.html#average-treatment-effect-on-the-treated-att"><i class="fa fa-check"></i><b>2.1.3</b> Average Treatment Effect on the Treated (ATT)</a></li>
<li class="chapter" data-level="2.1.4" data-path="potential-outcomes.html"><a href="potential-outcomes.html#the-fundamental-problem-of-causal-inference"><i class="fa fa-check"></i><b>2.1.4</b> The Fundamental Problem of Causal Inference</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="potential-outcomes.html"><a href="potential-outcomes.html#assumptions-for-identifying-causal-effects"><i class="fa fa-check"></i><b>2.2</b> Assumptions for Identifying Causal Effects</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="potential-outcomes.html"><a href="potential-outcomes.html#independence"><i class="fa fa-check"></i><b>2.2.1</b> <strong>Independence</strong></a></li>
<li class="chapter" data-level="2.2.2" data-path="potential-outcomes.html"><a href="potential-outcomes.html#stable-unit-treatment-value-assumption-sutva"><i class="fa fa-check"></i><b>2.2.2</b> Stable Unit Treatment Value Assumption (SUTVA)</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="potential-outcomes.html"><a href="potential-outcomes.html#methods-for-estimating-causal-effects"><i class="fa fa-check"></i><b>2.3</b> Methods for Estimating Causal Effects</a></li>
<li class="chapter" data-level="2.4" data-path="potential-outcomes.html"><a href="potential-outcomes.html#example"><i class="fa fa-check"></i><b>2.4</b> Example</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="potential-outcomes.html"><a href="potential-outcomes.html#simple-difference-method"><i class="fa fa-check"></i><b>2.4.1</b> Simple Difference Method</a></li>
<li class="chapter" data-level="2.4.2" data-path="potential-outcomes.html"><a href="potential-outcomes.html#conclusion-1"><i class="fa fa-check"></i><b>2.4.2</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="potential-outcomes.html"><a href="potential-outcomes.html#on-how-parameters-are-calculated"><i class="fa fa-check"></i><b>2.5</b> On how parameters are calculated</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="potential-outcomes.html"><a href="potential-outcomes.html#propensity-score-matching-psm-and-maximum-likelihood-estimation-mle"><i class="fa fa-check"></i><b>2.5.1</b> Propensity Score Matching (PSM) and Maximum Likelihood Estimation (MLE)</a></li>
<li class="chapter" data-level="2.5.2" data-path="potential-outcomes.html"><a href="potential-outcomes.html#logistic-regression"><i class="fa fa-check"></i><b>2.5.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="2.5.3" data-path="potential-outcomes.html"><a href="potential-outcomes.html#ordinary-least-squares-ols-regression"><i class="fa fa-check"></i><b>2.5.3</b> Ordinary Least Squares (OLS) Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="matching.html"><a href="matching.html"><i class="fa fa-check"></i><b>3</b> Matching</a>
<ul>
<li class="chapter" data-level="3.1" data-path="matching.html"><a href="matching.html#subclassification"><i class="fa fa-check"></i><b>3.1</b> Subclassification</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="matching.html"><a href="matching.html#example-2"><i class="fa fa-check"></i><b>3.1.1</b> Example</a></li>
<li class="chapter" data-level="3.1.2" data-path="matching.html"><a href="matching.html#step-by-step-example"><i class="fa fa-check"></i><b>3.1.2</b> Step-by-Step Example</a></li>
<li class="chapter" data-level="3.1.3" data-path="matching.html"><a href="matching.html#considerations"><i class="fa fa-check"></i><b>3.1.3</b> Considerations</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="matching.html"><a href="matching.html#exact-matching"><i class="fa fa-check"></i><b>3.2</b> Exact Matching</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="matching.html"><a href="matching.html#explanation"><i class="fa fa-check"></i><b>3.2.1</b> Explanation</a></li>
<li class="chapter" data-level="3.2.2" data-path="matching.html"><a href="matching.html#example-3"><i class="fa fa-check"></i><b>3.2.2</b> Example</a></li>
<li class="chapter" data-level="3.2.3" data-path="matching.html"><a href="matching.html#conclusion-2"><i class="fa fa-check"></i><b>3.2.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="matching.html"><a href="matching.html#approximate-matching-methods"><i class="fa fa-check"></i><b>3.3</b> Approximate Matching Methods</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="matching.html"><a href="matching.html#nearest-neighbor-covariate-matching"><i class="fa fa-check"></i><b>3.3.1</b> Nearest Neighbor Covariate Matching</a></li>
<li class="chapter" data-level="3.3.2" data-path="matching.html"><a href="matching.html#example-4"><i class="fa fa-check"></i><b>3.3.2</b> Example</a></li>
<li class="chapter" data-level="3.3.3" data-path="matching.html"><a href="matching.html#hypothetical-data"><i class="fa fa-check"></i><b>3.3.3</b> Hypothetical Data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="matching.html"><a href="matching.html#propensity-score-methods"><i class="fa fa-check"></i><b>3.4</b> Propensity Score Methods</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="matching.html"><a href="matching.html#concept"><i class="fa fa-check"></i><b>3.4.1</b> Concept</a></li>
<li class="chapter" data-level="3.4.2" data-path="matching.html"><a href="matching.html#steps"><i class="fa fa-check"></i><b>3.4.2</b> Steps</a></li>
<li class="chapter" data-level="3.4.3" data-path="matching.html"><a href="matching.html#example-5"><i class="fa fa-check"></i><b>3.4.3</b> Example</a></li>
<li class="chapter" data-level="3.4.4" data-path="matching.html"><a href="matching.html#assumptions-and-considerations"><i class="fa fa-check"></i><b>3.4.4</b> Assumptions and Considerations</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="matching.html"><a href="matching.html#inverse-probability-weighting-weighting-on-the-propensity-score"><i class="fa fa-check"></i><b>3.5</b> Inverse Probability Weighting (Weighting on the propensity score)</a></li>
<li class="chapter" data-level="3.6" data-path="matching.html"><a href="matching.html#nearest-neighbor-matching"><i class="fa fa-check"></i><b>3.6</b> Nearest-neighbor matching</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="matching.html"><a href="matching.html#example-in-r"><i class="fa fa-check"></i><b>3.6.1</b> Example in R</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="matching.html"><a href="matching.html#coarsened-exact-matching"><i class="fa fa-check"></i><b>3.7</b> Coarsened Exact Matching</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="matching.html"><a href="matching.html#example-6"><i class="fa fa-check"></i><b>3.7.1</b> Example</a></li>
<li class="chapter" data-level="3.7.2" data-path="matching.html"><a href="matching.html#steps-in-coarsened-exact-matching"><i class="fa fa-check"></i><b>3.7.2</b> Steps in Coarsened Exact Matching</a></li>
<li class="chapter" data-level="3.7.3" data-path="matching.html"><a href="matching.html#considerations-1"><i class="fa fa-check"></i><b>3.7.3</b> Considerations</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="matching.html"><a href="matching.html#ab-test-article-from-medium"><i class="fa fa-check"></i><b>3.8</b> A/B Test article from Medium</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="matching.html"><a href="matching.html#example-conversion-rate-of-an-e-commerce-website"><i class="fa fa-check"></i><b>3.8.1</b> Example: Conversion Rate of an E-Commerce Website</a></li>
<li class="chapter" data-level="3.8.2" data-path="matching.html"><a href="matching.html#example-ab-test"><i class="fa fa-check"></i><b>3.8.2</b> Example: A/B Test</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="matching.html"><a href="matching.html#task-1-load-the-data"><i class="fa fa-check"></i><b>3.9</b> Task 1: Load the data</a></li>
<li class="chapter" data-level="3.10" data-path="matching.html"><a href="matching.html#task-2-set-up-hypothesis"><i class="fa fa-check"></i><b>3.10</b> Task 2: Set up Hypothesis</a></li>
<li class="chapter" data-level="3.11" data-path="matching.html"><a href="matching.html#task-3-compute-the-difference-in-the-click-through-rate"><i class="fa fa-check"></i><b>3.11</b> Task 3: Compute the difference in the click-through rate</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="task-four-create-sample-distribution-using-bootsrapping.html"><a href="task-four-create-sample-distribution-using-bootsrapping.html"><i class="fa fa-check"></i><b>4</b> Task four : create sample distribution using bootsrapping</a>
<ul>
<li class="chapter" data-level="4.0.1" data-path="task-four-create-sample-distribution-using-bootsrapping.html"><a href="task-four-create-sample-distribution-using-bootsrapping.html#bootstrapping"><i class="fa fa-check"></i><b>4.0.1</b> Bootstrapping :</a></li>
<li class="chapter" data-level="4.0.2" data-path="task-four-create-sample-distribution-using-bootsrapping.html"><a href="task-four-create-sample-distribution-using-bootsrapping.html#example-7"><i class="fa fa-check"></i><b>4.0.2</b> Example :</a></li>
<li class="chapter" data-level="4.1" data-path="task-four-create-sample-distribution-using-bootsrapping.html"><a href="task-four-create-sample-distribution-using-bootsrapping.html#task-five-evaluate-the-null-hypothesis-and-draw-conclustions."><i class="fa fa-check"></i><b>4.1</b> Task five : Evaluate the null hypothesis and draw conclustions.</a></li>
<li class="chapter" data-level="4.2" data-path="task-four-create-sample-distribution-using-bootsrapping.html"><a href="task-four-create-sample-distribution-using-bootsrapping.html#alternative-random-sampling-code"><i class="fa fa-check"></i><b>4.2</b> alternative random sampling code</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ab-testing.html"><a href="ab-testing.html"><i class="fa fa-check"></i><b>5</b> AB Testing</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ab-testing.html"><a href="ab-testing.html#sources"><i class="fa fa-check"></i><b>5.1</b> Sources</a></li>
<li class="chapter" data-level="5.2" data-path="ab-testing.html"><a href="ab-testing.html#concepts-1"><i class="fa fa-check"></i><b>5.2</b> Concepts</a></li>
<li class="chapter" data-level="5.3" data-path="ab-testing.html"><a href="ab-testing.html#ai-summary"><i class="fa fa-check"></i><b>5.3</b> AI Summary</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="ab-testing.html"><a href="ab-testing.html#ab-testing-randomized-controlled-trials"><i class="fa fa-check"></i><b>5.3.1</b> A/B Testing (Randomized Controlled Trials)</a></li>
<li class="chapter" data-level="5.3.2" data-path="ab-testing.html"><a href="ab-testing.html#concepts-2"><i class="fa fa-check"></i><b>5.3.2</b> Concepts:</a></li>
<li class="chapter" data-level="5.3.3" data-path="ab-testing.html"><a href="ab-testing.html#comparison-and-usage"><i class="fa fa-check"></i><b>5.3.3</b> Comparison and Usage</a></li>
<li class="chapter" data-level="5.3.4" data-path="ab-testing.html"><a href="ab-testing.html#significance"><i class="fa fa-check"></i><b>5.3.4</b> Significance</a></li>
<li class="chapter" data-level="5.3.5" data-path="ab-testing.html"><a href="ab-testing.html#group-size"><i class="fa fa-check"></i><b>5.3.5</b> Group Size</a></li>
<li class="chapter" data-level="5.3.6" data-path="ab-testing.html"><a href="ab-testing.html#relationship-between-effect-size-significance-and-group-size"><i class="fa fa-check"></i><b>5.3.6</b> Relationship Between Effect Size, Significance, and Group Size</a></li>
<li class="chapter" data-level="5.3.7" data-path="ab-testing.html"><a href="ab-testing.html#conclusion-3"><i class="fa fa-check"></i><b>5.3.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ab-testing.html"><a href="ab-testing.html#size-of-the-control-group"><i class="fa fa-check"></i><b>5.4</b> Size of the Control Group</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="ab-testing.html"><a href="ab-testing.html#sample-size-calculation-formula"><i class="fa fa-check"></i><b>5.4.1</b> Sample Size Calculation Formula</a></li>
<li class="chapter" data-level="5.4.2" data-path="ab-testing.html"><a href="ab-testing.html#conclusion-4"><i class="fa fa-check"></i><b>5.4.2</b> Conclusion</a></li>
<li class="chapter" data-level="5.4.3" data-path="ab-testing.html"><a href="ab-testing.html#statistical-assumptions-for-randomized-controlled-trials-rcts"><i class="fa fa-check"></i><b>5.4.3</b> Statistical Assumptions for Randomized Controlled Trials (RCTs)</a></li>
<li class="chapter" data-level="5.4.4" data-path="ab-testing.html"><a href="ab-testing.html#robustness-checks-1"><i class="fa fa-check"></i><b>5.4.4</b> Robustness Checks</a></li>
<li class="chapter" data-level="5.4.5" data-path="ab-testing.html"><a href="ab-testing.html#validation-methods-1"><i class="fa fa-check"></i><b>5.4.5</b> Validation Methods</a></li>
<li class="chapter" data-level="5.4.6" data-path="ab-testing.html"><a href="ab-testing.html#conclusion-5"><i class="fa fa-check"></i><b>5.4.6</b> Conclusion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html"><i class="fa fa-check"></i><b>6</b> Difference-in-Differences (DiD) Methods</a>
<ul>
<li class="chapter" data-level="6.1" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#simple-difference-in-differences-did"><i class="fa fa-check"></i><b>6.1</b> Simple Difference-in-Differences (DiD)</a></li>
<li class="chapter" data-level="6.2" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#controversial-note"><i class="fa fa-check"></i><b>6.2</b> Controversial Note</a></li>
<li class="chapter" data-level="6.3" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#placebo-tests-for-parallel-trends"><i class="fa fa-check"></i><b>6.3</b> Placebo tests for parallel trends</a></li>
<li class="chapter" data-level="6.4" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#two-way-fixed-effects-model"><i class="fa fa-check"></i><b>6.4</b> Two-Way Fixed Effects Model</a></li>
<li class="chapter" data-level="6.5" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#event-study-methods"><i class="fa fa-check"></i><b>6.5</b> Event Study Methods</a></li>
<li class="chapter" data-level="6.6" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#importance-of-placebos-in-dd"><i class="fa fa-check"></i><b>6.6</b> Importance of Placebos in DD</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#triple-differences"><i class="fa fa-check"></i><b>6.6.1</b> Triple Differences</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#compositional-changes"><i class="fa fa-check"></i><b>6.7</b> Compositional Changes</a></li>
<li class="chapter" data-level="6.8" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#key-assumptions"><i class="fa fa-check"></i><b>6.8</b> Key Assumptions</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#implementation-steps"><i class="fa fa-check"></i><b>6.8.1</b> Implementation Steps</a></li>
<li class="chapter" data-level="6.8.2" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#advantages"><i class="fa fa-check"></i><b>6.8.2</b> Advantages</a></li>
<li class="chapter" data-level="6.8.3" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#limitations-1"><i class="fa fa-check"></i><b>6.8.3</b> Limitations</a></li>
<li class="chapter" data-level="6.8.4" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#q-how-would-you-test-the-parallel-trends-assumption"><i class="fa fa-check"></i><b>6.8.4</b> Q: How would you test the parallel trends assumption?</a></li>
<li class="chapter" data-level="6.8.5" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#q-how-would-you-address-potential-violations-of-the-parallel-trends-assumption"><i class="fa fa-check"></i><b>6.8.5</b> Q: How would you address potential violations of the parallel trends assumption?</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#notes"><i class="fa fa-check"></i><b>6.9</b> Notes</a>
<ul>
<li class="chapter" data-level="6.9.1" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#example-business"><i class="fa fa-check"></i><b>6.9.1</b> Example: Business</a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#extra-considerations"><i class="fa fa-check"></i><b>6.10</b> Extra Considerations</a></li>
<li class="chapter" data-level="6.11" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#synthetic-difference-in-differences-synthdid-method"><i class="fa fa-check"></i><b>6.11</b> Synthetic Difference-in-Differences (SynthDiD) method</a></li>
<li class="chapter" data-level="6.12" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#doubly-robust-models-in-econometrics"><i class="fa fa-check"></i><b>6.12</b> Doubly Robust Models in Econometrics</a>
<ul>
<li class="chapter" data-level="6.12.1" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#key-concepts-3"><i class="fa fa-check"></i><b>6.12.1</b> Key Concepts</a></li>
<li class="chapter" data-level="6.12.2" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#steps-in-doubly-robust-estimation"><i class="fa fa-check"></i><b>6.12.2</b> Steps in Doubly Robust Estimation</a></li>
<li class="chapter" data-level="6.12.3" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#advantages-1"><i class="fa fa-check"></i><b>6.12.3</b> Advantages</a></li>
<li class="chapter" data-level="6.12.4" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#examples-and-applications"><i class="fa fa-check"></i><b>6.12.4</b> Examples and Applications</a></li>
<li class="chapter" data-level="6.12.5" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#assumptions-and-considerations-1"><i class="fa fa-check"></i><b>6.12.5</b> Assumptions and Considerations</a></li>
<li class="chapter" data-level="6.12.6" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#conclusion-6"><i class="fa fa-check"></i><b>6.12.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="6.13" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#twoway-fixed-effects-with-differential-timing"><i class="fa fa-check"></i><b>6.13</b> Twoway Fixed Effects with Differential Timing</a></li>
<li class="chapter" data-level="6.14" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#bacon-decomposition"><i class="fa fa-check"></i><b>6.14</b> Bacon Decomposition</a>
<ul>
<li class="chapter" data-level="6.14.1" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#overview"><i class="fa fa-check"></i><b>6.14.1</b> Overview</a></li>
<li class="chapter" data-level="6.14.2" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#key-concepts-4"><i class="fa fa-check"></i><b>6.14.2</b> Key Concepts</a></li>
<li class="chapter" data-level="6.14.3" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#components-of-bacon-decomposition"><i class="fa fa-check"></i><b>6.14.3</b> Components of Bacon Decomposition</a></li>
<li class="chapter" data-level="6.14.4" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#formula-for-decomposition"><i class="fa fa-check"></i><b>6.14.4</b> Formula for Decomposition</a></li>
<li class="chapter" data-level="6.14.5" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#implications-and-interpretation"><i class="fa fa-check"></i><b>6.14.5</b> Implications and Interpretation</a></li>
<li class="chapter" data-level="6.14.6" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#example-8"><i class="fa fa-check"></i><b>6.14.6</b> Example</a></li>
<li class="chapter" data-level="6.14.7" data-path="difference-in-differences-did-methods.html"><a href="difference-in-differences-did-methods.html#conclusion-7"><i class="fa fa-check"></i><b>6.14.7</b> Conclusion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="synthetic-control-method-scm.html"><a href="synthetic-control-method-scm.html"><i class="fa fa-check"></i><b>7</b> Synthetic Control Method (SCM)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="synthetic-control-method-scm.html"><a href="synthetic-control-method-scm.html#ai-summary-1"><i class="fa fa-check"></i><b>7.1</b> AI Summary</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="synthetic-control-method-scm.html"><a href="synthetic-control-method-scm.html#synthetic-control-method-scm-1"><i class="fa fa-check"></i><b>7.1.1</b> Synthetic Control Method (SCM)</a></li>
<li class="chapter" data-level="7.1.2" data-path="synthetic-control-method-scm.html"><a href="synthetic-control-method-scm.html#placebo-tests"><i class="fa fa-check"></i><b>7.1.2</b> Placebo tests</a></li>
<li class="chapter" data-level="7.1.3" data-path="synthetic-control-method-scm.html"><a href="synthetic-control-method-scm.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>7.1.3</b> Leave-One-Out Cross-Validation</a></li>
<li class="chapter" data-level="7.1.4" data-path="synthetic-control-method-scm.html"><a href="synthetic-control-method-scm.html#detailed-example"><i class="fa fa-check"></i><b>7.1.4</b> Detailed Example</a></li>
<li class="chapter" data-level="7.1.5" data-path="synthetic-control-method-scm.html"><a href="synthetic-control-method-scm.html#conclusion-9"><i class="fa fa-check"></i><b>7.1.5</b> Conclusion</a></li>
<li class="chapter" data-level="7.1.6" data-path="synthetic-control-method-scm.html"><a href="synthetic-control-method-scm.html#data"><i class="fa fa-check"></i><b>7.1.6</b> Data</a></li>
<li class="chapter" data-level="7.1.7" data-path="synthetic-control-method-scm.html"><a href="synthetic-control-method-scm.html#data-setting"><i class="fa fa-check"></i><b>7.1.7</b> Data Setting</a></li>
<li class="chapter" data-level="7.1.8" data-path="synthetic-control-method-scm.html"><a href="synthetic-control-method-scm.html#requirements-for-synthetic-control-method"><i class="fa fa-check"></i><b>7.1.8</b> Requirements for Synthetic Control Method</a></li>
<li class="chapter" data-level="7.1.9" data-path="synthetic-control-method-scm.html"><a href="synthetic-control-method-scm.html#data-requirements-summary"><i class="fa fa-check"></i><b>7.1.9</b> Data Requirements Summary</a></li>
<li class="chapter" data-level="7.1.10" data-path="synthetic-control-method-scm.html"><a href="synthetic-control-method-scm.html#practical-considerations-1"><i class="fa fa-check"></i><b>7.1.10</b> Practical Considerations</a></li>
<li class="chapter" data-level="7.1.11" data-path="synthetic-control-method-scm.html"><a href="synthetic-control-method-scm.html#example-11"><i class="fa fa-check"></i><b>7.1.11</b> Example</a></li>
<li class="chapter" data-level="7.1.12" data-path="synthetic-control-method-scm.html"><a href="synthetic-control-method-scm.html#conclusion-10"><i class="fa fa-check"></i><b>7.1.12</b> Conclusion</a></li>
<li class="chapter" data-level="7.1.13" data-path="synthetic-control-method-scm.html"><a href="synthetic-control-method-scm.html#synthetic-did"><i class="fa fa-check"></i><b>7.1.13</b> Synthetic DID</a></li>
<li class="chapter" data-level="7.1.14" data-path="synthetic-control-method-scm.html"><a href="synthetic-control-method-scm.html#robustness-checks-3"><i class="fa fa-check"></i><b>7.1.14</b> Robustness Checks</a></li>
<li class="chapter" data-level="7.1.15" data-path="synthetic-control-method-scm.html"><a href="synthetic-control-method-scm.html#validation-methods-3"><i class="fa fa-check"></i><b>7.1.15</b> Validation Methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="instrumental-variables-iv.html"><a href="instrumental-variables-iv.html"><i class="fa fa-check"></i><b>8</b> Instrumental Variables (IV)</a>
<ul>
<li class="chapter" data-level="8.1" data-path="instrumental-variables-iv.html"><a href="instrumental-variables-iv.html#key-concepts-5"><i class="fa fa-check"></i><b>8.1</b> Key Concepts</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="instrumental-variables-iv.html"><a href="instrumental-variables-iv.html#requirements-for-a-valid-instrument"><i class="fa fa-check"></i><b>8.1.1</b> Requirements for a Valid Instrument</a></li>
<li class="chapter" data-level="8.1.2" data-path="instrumental-variables-iv.html"><a href="instrumental-variables-iv.html#the-iv-estimation-process"><i class="fa fa-check"></i><b>8.1.2</b> The IV Estimation Process</a></li>
<li class="chapter" data-level="8.1.3" data-path="instrumental-variables-iv.html"><a href="instrumental-variables-iv.html#example-12"><i class="fa fa-check"></i><b>8.1.3</b> Example</a></li>
<li class="chapter" data-level="8.1.4" data-path="instrumental-variables-iv.html"><a href="instrumental-variables-iv.html#assumptions-and-considerations-2"><i class="fa fa-check"></i><b>8.1.4</b> Assumptions and Considerations</a></li>
<li class="chapter" data-level="8.1.5" data-path="instrumental-variables-iv.html"><a href="instrumental-variables-iv.html#advantages-of-iv"><i class="fa fa-check"></i><b>8.1.5</b> Advantages of IV</a></li>
<li class="chapter" data-level="8.1.6" data-path="instrumental-variables-iv.html"><a href="instrumental-variables-iv.html#disadvantages-of-iv"><i class="fa fa-check"></i><b>8.1.6</b> Disadvantages of IV</a></li>
<li class="chapter" data-level="8.1.7" data-path="instrumental-variables-iv.html"><a href="instrumental-variables-iv.html#conclusion-11"><i class="fa fa-check"></i><b>8.1.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="instrumental-variables-iv.html"><a href="instrumental-variables-iv.html#difference-between-instrumental-variable-iv-method-and-two-stage-least-squares-2sls-method"><i class="fa fa-check"></i><b>8.2</b> Difference Between Instrumental Variable (IV) Method and Two-Stage Least Squares (2SLS) Method</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="instrumental-variables-iv.html"><a href="instrumental-variables-iv.html#instrumental-variable-iv-method"><i class="fa fa-check"></i><b>8.2.1</b> Instrumental Variable (IV) Method</a></li>
<li class="chapter" data-level="8.2.2" data-path="instrumental-variables-iv.html"><a href="instrumental-variables-iv.html#two-stage-least-squares-2sls-method"><i class="fa fa-check"></i><b>8.2.2</b> Two-Stage Least Squares (2SLS) Method</a></li>
<li class="chapter" data-level="8.2.3" data-path="instrumental-variables-iv.html"><a href="instrumental-variables-iv.html#summary-of-differences"><i class="fa fa-check"></i><b>8.2.3</b> Summary of Differences:</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="instrumental-variables-iv.html"><a href="instrumental-variables-iv.html#homogenous-treatment-effect"><i class="fa fa-check"></i><b>8.3</b> Homogenous Treatment Effect</a></li>
<li class="chapter" data-level="8.4" data-path="instrumental-variables-iv.html"><a href="instrumental-variables-iv.html#heterogenous-treatment-effect"><i class="fa fa-check"></i><b>8.4</b> Heterogenous Treatment Effect</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="regression-discontinuity-designs-rdd.html"><a href="regression-discontinuity-designs-rdd.html"><i class="fa fa-check"></i><b>9</b> Regression Discontinuity Designs (RDD)</a>
<ul>
<li class="chapter" data-level="9.0.1" data-path="regression-discontinuity-designs-rdd.html"><a href="regression-discontinuity-designs-rdd.html#validation-and-falsification"><i class="fa fa-check"></i><b>9.0.1</b> Validation and Falsification</a></li>
<li class="chapter" data-level="9.1" data-path="regression-discontinuity-designs-rdd.html"><a href="regression-discontinuity-designs-rdd.html#regression-discontinuity-designs-rdd-1"><i class="fa fa-check"></i><b>9.1</b> Regression Discontinuity Designs (RDD)</a></li>
<li class="chapter" data-level="9.2" data-path="regression-discontinuity-designs-rdd.html"><a href="regression-discontinuity-designs-rdd.html#key-concepts-6"><i class="fa fa-check"></i><b>9.2</b> Key Concepts</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="regression-discontinuity-designs-rdd.html"><a href="regression-discontinuity-designs-rdd.html#types-of-rdd"><i class="fa fa-check"></i><b>9.2.1</b> Types of RDD</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="regression-discontinuity-designs-rdd.html"><a href="regression-discontinuity-designs-rdd.html#sharp-rdd"><i class="fa fa-check"></i><b>9.3</b> Sharp RDD</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="regression-discontinuity-designs-rdd.html"><a href="regression-discontinuity-designs-rdd.html#key-concepts-7"><i class="fa fa-check"></i><b>9.3.1</b> Key Concepts</a></li>
<li class="chapter" data-level="9.3.2" data-path="regression-discontinuity-designs-rdd.html"><a href="regression-discontinuity-designs-rdd.html#assumptions-for-rdd"><i class="fa fa-check"></i><b>9.3.2</b> Assumptions for RDD</a></li>
<li class="chapter" data-level="9.3.3" data-path="regression-discontinuity-designs-rdd.html"><a href="regression-discontinuity-designs-rdd.html#estimation-in-sharp-rdd"><i class="fa fa-check"></i><b>9.3.3</b> Estimation in Sharp RDD</a></li>
<li class="chapter" data-level="9.3.4" data-path="regression-discontinuity-designs-rdd.html"><a href="regression-discontinuity-designs-rdd.html#fuzzy-rdd"><i class="fa fa-check"></i><b>9.3.4</b> Fuzzy RDD</a></li>
<li class="chapter" data-level="9.3.5" data-path="regression-discontinuity-designs-rdd.html"><a href="regression-discontinuity-designs-rdd.html#parametric-vs.-non-parametric-applications"><i class="fa fa-check"></i><b>9.3.5</b> Parametric vs. Non-Parametric Applications</a></li>
<li class="chapter" data-level="9.3.6" data-path="regression-discontinuity-designs-rdd.html"><a href="regression-discontinuity-designs-rdd.html#example-13"><i class="fa fa-check"></i><b>9.3.6</b> Example</a></li>
<li class="chapter" data-level="9.3.7" data-path="regression-discontinuity-designs-rdd.html"><a href="regression-discontinuity-designs-rdd.html#summary-2"><i class="fa fa-check"></i><b>9.3.7</b> Summary</a></li>
<li class="chapter" data-level="9.3.8" data-path="regression-discontinuity-designs-rdd.html"><a href="regression-discontinuity-designs-rdd.html#challenges-to-identification"><i class="fa fa-check"></i><b>9.3.8</b> Challenges to Identification</a></li>
<li class="chapter" data-level="9.3.9" data-path="regression-discontinuity-designs-rdd.html"><a href="regression-discontinuity-designs-rdd.html#examples"><i class="fa fa-check"></i><b>9.3.9</b> Examples</a></li>
<li class="chapter" data-level="9.3.10" data-path="regression-discontinuity-designs-rdd.html"><a href="regression-discontinuity-designs-rdd.html#types-of-rdd-1"><i class="fa fa-check"></i><b>9.3.10</b> Types of RDD</a></li>
<li class="chapter" data-level="9.3.11" data-path="regression-discontinuity-designs-rdd.html"><a href="regression-discontinuity-designs-rdd.html#parametric-vs.-non-parametric-applications-1"><i class="fa fa-check"></i><b>9.3.11</b> Parametric vs. Non-Parametric Applications</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="regression-discontinuity-designs-rdd.html"><a href="regression-discontinuity-designs-rdd.html#regression-kink-design"><i class="fa fa-check"></i><b>9.4</b> Regression Kink Design</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="fixed-effects-and-panel-data-methods.html"><a href="fixed-effects-and-panel-data-methods.html"><i class="fa fa-check"></i><b>10</b> Fixed Effects and Panel Data Methods</a>
<ul>
<li class="chapter" data-level="10.1" data-path="fixed-effects-and-panel-data-methods.html"><a href="fixed-effects-and-panel-data-methods.html#pooled-regression"><i class="fa fa-check"></i><b>10.1</b> Pooled Regression</a></li>
<li class="chapter" data-level="10.2" data-path="fixed-effects-and-panel-data-methods.html"><a href="fixed-effects-and-panel-data-methods.html#panel-data-methods"><i class="fa fa-check"></i><b>10.2</b> Panel Data Methods</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="fixed-effects-and-panel-data-methods.html"><a href="fixed-effects-and-panel-data-methods.html#fixed-effects-model"><i class="fa fa-check"></i><b>10.2.1</b> Fixed Effects Model</a></li>
<li class="chapter" data-level="10.2.2" data-path="fixed-effects-and-panel-data-methods.html"><a href="fixed-effects-and-panel-data-methods.html#random-effects-model"><i class="fa fa-check"></i><b>10.2.2</b> Random Effects Model</a></li>
<li class="chapter" data-level="10.2.3" data-path="fixed-effects-and-panel-data-methods.html"><a href="fixed-effects-and-panel-data-methods.html#example-economic-growth-and-education"><i class="fa fa-check"></i><b>10.2.3</b> Example: Economic Growth and Education</a></li>
<li class="chapter" data-level="10.2.4" data-path="fixed-effects-and-panel-data-methods.html"><a href="fixed-effects-and-panel-data-methods.html#conclusion-12"><i class="fa fa-check"></i><b>10.2.4</b> Conclusion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html"><i class="fa fa-check"></i><b>11</b> Ordinary Least Squares (OLS)</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#ordinary-least-squares-ols-1"><i class="fa fa-check"></i><b>11.1</b> Ordinary Least Squares (OLS)</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#linearity"><i class="fa fa-check"></i><b>11.1.1</b> <strong>Linearity</strong></a></li>
<li class="chapter" data-level="11.1.2" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#exogeneity"><i class="fa fa-check"></i><b>11.1.2</b> <strong>Exogeneity</strong></a></li>
<li class="chapter" data-level="11.1.3" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#homoscedasticity"><i class="fa fa-check"></i><b>11.1.3</b> <strong>Homoscedasticity</strong></a></li>
<li class="chapter" data-level="11.1.4" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#no-autocorrelation"><i class="fa fa-check"></i><b>11.1.4</b> <strong>No Autocorrelation</strong></a></li>
<li class="chapter" data-level="11.1.5" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#no-perfect-multicollinearity"><i class="fa fa-check"></i><b>11.1.5</b> <strong>No Perfect Multicollinearity</strong></a></li>
<li class="chapter" data-level="11.1.6" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#normality-of-errors-for-inference"><i class="fa fa-check"></i><b>11.1.6</b> <strong>Normality of Errors (for inference)</strong></a></li>
<li class="chapter" data-level="11.1.7" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#practical-considerations-and-tests"><i class="fa fa-check"></i><b>11.1.7</b> Practical Considerations and Tests</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#simple-linear-regression"><i class="fa fa-check"></i><b>11.2</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#summary-3"><i class="fa fa-check"></i><b>11.2.1</b> Summary</a></li>
<li class="chapter" data-level="11.2.2" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#interpreting-model-coefficients-in-ols-models"><i class="fa fa-check"></i><b>11.2.2</b> Interpreting Model Coefficients in OLS Models</a></li>
<li class="chapter" data-level="11.2.3" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#linear-models-linear-linear"><i class="fa fa-check"></i><b>11.2.3</b> 1. Linear Models (Linear-Linear)</a></li>
<li class="chapter" data-level="11.2.4" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#log-linear-models"><i class="fa fa-check"></i><b>11.2.4</b> 2. Log-Linear Models</a></li>
<li class="chapter" data-level="11.2.5" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#linear-log-models"><i class="fa fa-check"></i><b>11.2.5</b> 3. Linear-Log Models</a></li>
<li class="chapter" data-level="11.2.6" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#log-log-models"><i class="fa fa-check"></i><b>11.2.6</b> 4. Log-Log Models</a></li>
<li class="chapter" data-level="11.2.7" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#examples-of-dummy-and-continuous-variables"><i class="fa fa-check"></i><b>11.2.7</b> Examples of Dummy and Continuous Variables</a></li>
<li class="chapter" data-level="11.2.8" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#assumptions-and-considerations-3"><i class="fa fa-check"></i><b>11.2.8</b> Assumptions and Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#multivariate-regression"><i class="fa fa-check"></i><b>11.3</b> Multivariate Regression</a></li>
<li class="chapter" data-level="11.4" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#generalized-linear-model"><i class="fa fa-check"></i><b>11.4</b> Generalized Linear Model</a></li>
<li class="chapter" data-level="11.5" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#generalised-least-square"><i class="fa fa-check"></i><b>11.5</b> Generalised Least Square</a></li>
<li class="chapter" data-level="11.6" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#weighted-least-squares-wls"><i class="fa fa-check"></i><b>11.6</b> Weighted Least Squares (WLS)</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="resampling-methods.html"><a href="resampling-methods.html"><i class="fa fa-check"></i><b>12</b> Resampling methods</a>
<ul>
<li class="chapter" data-level="12.1" data-path="resampling-methods.html"><a href="resampling-methods.html#randomization-based-methods"><i class="fa fa-check"></i><b>12.1</b> Randomization-Based Methods</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="resampling-methods.html"><a href="resampling-methods.html#traditional-methods-vs.-randomization-based-methods"><i class="fa fa-check"></i><b>12.1.1</b> Traditional Methods vs. Randomization-Based Methods</a></li>
<li class="chapter" data-level="12.1.2" data-path="resampling-methods.html"><a href="resampling-methods.html#why-use-randomization-based-methods"><i class="fa fa-check"></i><b>12.1.2</b> Why Use Randomization-Based Methods?</a></li>
<li class="chapter" data-level="12.1.3" data-path="resampling-methods.html"><a href="resampling-methods.html#how-randomization-based-methods-work"><i class="fa fa-check"></i><b>12.1.3</b> How Randomization-Based Methods Work</a></li>
<li class="chapter" data-level="12.1.4" data-path="resampling-methods.html"><a href="resampling-methods.html#example-14"><i class="fa fa-check"></i><b>12.1.4</b> Example</a></li>
<li class="chapter" data-level="12.1.5" data-path="resampling-methods.html"><a href="resampling-methods.html#contribution-of-athey-and-imbens"><i class="fa fa-check"></i><b>12.1.5</b> Contribution of Athey and Imbens</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="resampling-methods.html"><a href="resampling-methods.html#bootstrapping-1"><i class="fa fa-check"></i><b>12.2</b> Bootstrapping</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html"><i class="fa fa-check"></i><b>13</b> Hypotheis Testing</a>
<ul>
<li class="chapter" data-level="13.1" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html#concepts-3"><i class="fa fa-check"></i><b>13.1</b> Concepts</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html#significance-level-α"><i class="fa fa-check"></i><b>13.1.1</b> Significance Level (α)</a></li>
<li class="chapter" data-level="13.1.2" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html#p-value"><i class="fa fa-check"></i><b>13.1.2</b> P-Value</a></li>
<li class="chapter" data-level="13.1.3" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html#type-i-error"><i class="fa fa-check"></i><b>13.1.3</b> Type I Error</a></li>
<li class="chapter" data-level="13.1.4" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html#type-ii-error"><i class="fa fa-check"></i><b>13.1.4</b> Type II Error</a></li>
<li class="chapter" data-level="13.1.5" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html#relationship-between-type-i-and-type-ii-errors"><i class="fa fa-check"></i><b>13.1.5</b> Relationship Between Type I and Type II Errors</a></li>
<li class="chapter" data-level="13.1.6" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html#importance-in-research-and-decision-making"><i class="fa fa-check"></i><b>13.1.6</b> Importance in Research and Decision Making</a></li>
<li class="chapter" data-level="13.1.7" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html#statistical-power"><i class="fa fa-check"></i><b>13.1.7</b> Statistical power</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html#null-hypothesis"><i class="fa fa-check"></i><b>13.2</b> Null Hypothesis</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html#fishers-sharp-null-hypothesis"><i class="fa fa-check"></i><b>13.2.1</b> Fisher’s Sharp Null Hypothesis</a></li>
<li class="chapter" data-level="13.2.2" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html#neymans-null-hypothesis"><i class="fa fa-check"></i><b>13.2.2</b> Neyman’s Null Hypothesis</a></li>
<li class="chapter" data-level="13.2.3" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html#key-differences"><i class="fa fa-check"></i><b>13.2.3</b> Key Differences</a></li>
<li class="chapter" data-level="13.2.4" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html#example-to-illustrate-the-difference"><i class="fa fa-check"></i><b>13.2.4</b> Example to Illustrate the Difference</a></li>
<li class="chapter" data-level="13.2.5" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html#conclusion-13"><i class="fa fa-check"></i><b>13.2.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html#permutation-tests"><i class="fa fa-check"></i><b>13.3</b> Permutation Tests</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html#when-and-why-to-use-permutation-tests"><i class="fa fa-check"></i><b>13.3.1</b> When and Why to Use Permutation Tests</a></li>
<li class="chapter" data-level="13.3.2" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html#how-to-perform-a-permutation-test"><i class="fa fa-check"></i><b>13.3.2</b> How to Perform a Permutation Test</a></li>
<li class="chapter" data-level="13.3.3" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html#example-calculation"><i class="fa fa-check"></i><b>13.3.3</b> Example Calculation</a></li>
<li class="chapter" data-level="13.3.4" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html#conclusion-14"><i class="fa fa-check"></i><b>13.3.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html#fischers-exact-test"><i class="fa fa-check"></i><b>13.4</b> Fischer’s Exact Test</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html#when-to-use-fishers-exact-test"><i class="fa fa-check"></i><b>13.4.1</b> When to Use Fisher’s Exact Test</a></li>
<li class="chapter" data-level="13.4.2" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html#how-fishers-exact-test-works"><i class="fa fa-check"></i><b>13.4.2</b> How Fisher’s Exact Test Works</a></li>
<li class="chapter" data-level="13.4.3" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html#step-by-step-example-1"><i class="fa fa-check"></i><b>13.4.3</b> Step-by-Step Example</a></li>
<li class="chapter" data-level="13.4.4" data-path="hypotheis-testing.html"><a href="hypotheis-testing.html#conclusion-15"><i class="fa fa-check"></i><b>13.4.4</b> Conclusion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html"><i class="fa fa-check"></i><b>14</b> Maximum Likelihood Estimation</a></li>
<li class="divider"></li>
<li><a href="https://davutemrah.github.io" target="blank">Back to Home Page</li>
<li><a href="https://davutemrah.github.io/notebooks/" target="blank">Back to Collections</li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes on Causal Models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression-discontinuity-designs-rdd" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Chapter 9</span> Regression Discontinuity Designs (RDD)<a href="regression-discontinuity-designs-rdd.html#regression-discontinuity-designs-rdd" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>(Comprehensive Source)[<a href="https://rdpackages.github.io" class="uri">https://rdpackages.github.io</a>]</p>
<p>In the absence of randomized treatment assignment, research designs that
allow for the rigorous study of non-experimental interventions are
particularly promising. One of these is the Regression Discontinuity
(RD) design</p>
<p>In the RD design, all units have a score, and a treatment is assigned to
those units whose value of the score exceeds a known cutoff or
threshold, and not assigned to those units whose value of the score is
below the cutoff. The key feature of the design is that the probability
of receiving the treatment changes abruptly at the known threshold.</p>
<p>I units are unable to sort arount the cutoff point, units with scores
barely below the cutoff can be used as a comparison group for units with
scores barely above it.</p>
<p>In order to study causal effects with an RD design, the score,
treatment, and cutoff must exist</p>
<p><strong>types of approaches</strong></p>
<p>A. Continuity based approach: this ensures the smoothness of the
regression functions. Use least squares and polynomials, global or
local to cutoff.</p>
<ul>
<li><p>The reason is that global polynomial approximations tend to deliver
a good approximation overall, but a poor approximation at boundary
points—</p></li>
<li><p>Local polynomial methods are much better</p></li>
</ul>
<p>The idea that the treatment assignment is “as good as” randomly assigned in a neighborhood of the cutoff has been often invoked in the continuity-based framework to describe the required identification assumptions in an intuitive way, and it has also been used to develop formal results. However, within the continuity-based framework, the formal derivation of identification and esti- mation results always ultimately relies on continuity and differentiability of regression functions, and the idea of local randomization is used as a heuristic device only.</p>
<p>In contrast, the local randomization approach to RD analysis formalizes the idea that the RD design behaves like a randomized experiment near the cutoff by imposing explicit randomization-type assumptions that are stronger than the continuity-based conditions.</p>
<ul>
<li><p>less sensitive to outliers or other extreme features of the data</p></li>
<li><p>Local polynomial methods implement linear regression fits using only
observations near the cutoff point, separately for control and
treatment units.</p></li>
<li><p>h: bandwidth that determines the size of the neighborhood around the
cutoff where the empirical RD analysis is conducted.</p></li>
<li><p>the weights are determined by a kernel function K(·)</p></li>
<li><p>goal: fit the local polynomial that approximates the unknown
regression functions around the cutoff.</p></li>
<li><p>Local polynomial estimation consists of the following basic steps.</p></li>
</ul>
<ol style="list-style-type: decimal">
<li>Choose a polynomial order p and a kernel function K(·).<br />
</li>
<li>Choose a bandwidth h.<br />
</li>
<li>For observations above the cutoff (i.e., observations with Xi ≥ c),
fit a weighted least squares regression of the outcome Yi</li>
<li>For observations below the cutoff (i.e., observations with Xi &lt; c),
fit a weighted least squares regression of the outcome Yi</li>
</ol>
<ul>
<li><p>Kernel: assigns weights to units based on the distance: Triangular,
uniform (simple linear regression), Epanechnikov</p></li>
<li><p>polynomial order: 0 constant fit; increasing order means more
accuracy but more variability, overfitting.</p></li>
</ul>
<p>in general the local linear estimator seems to deliver a good trade-off
between simplicity, precision, and stability in RD settings.</p>
<ul>
<li>bandwidth:</li>
</ul>
<p>Choosing a smaller h will reduce the misspecification error (also known
as “smoothing bias”) of the local polynomial approximation, but will
simultaneously tend to increase the variance of the estimated
coefficients because fewer observations will be available for
estimation.</p>
<p>the choice of bandwidth is said to involve a “bias-variance trade-off.”</p>
<p>MSE-optimal bandwidth for the local polynomial RD estimate,</p>
<p><strong>Example</strong></p>
<p>These are assuming uniform kernel, no weights and polynomial degree 1…</p>
<p>local poly-nomial point estimation is simply a weighted least-squares
fit.</p>
<ol style="list-style-type: upper-alpha">
<li></li>
</ol>
<ul>
<li>linear reg y on x for both sides</li>
<li>intercept 2 - intercept 1</li>
</ul>
<p>B. linear reg Y on X + T + X*T within bandwidth</p>
<p>coefficient of T</p>
<p>C. rdrobust with p=1, kernel=uniform</p>
<p>rdrobust has many more options to use fully non parametric</p>
<p>B. Local Randomization:</p>
<p>In a nutshell, the local randomization approach imposes conditions so that units above and below the cutoff whose score values lie in a small window around the cutoff are comparable to each other and thus can be studied “as if” they had been randomly assigned to treatment or control.</p>
<p>When the running variable is continuous, the local randomization approach typically requires stronger assumptions than the continuity-based approach; in these cases, it is natural to use the continuity-based approach for the main RD analysis, and to use the local randomization approach as a robustness check. But in settings where the running variable is discrete or other departures from the canonical RD framework occur, the local randomization approach no longer imposes the strongest assumptions and can be a natural and useful method for analysis.</p>
<p><a href="https://rdpackages.github.io/references/Cattaneo-Idrobo-Titiunik_2024_CUP.pdf" class="uri">https://rdpackages.github.io/references/Cattaneo-Idrobo-Titiunik_2024_CUP.pdf</a></p>
<div id="validation-and-falsification" class="section level3 hasAnchor" number="9.0.1">
<h3><span class="header-section-number">9.0.1</span> Validation and Falsification<a href="regression-discontinuity-designs-rdd.html#validation-and-falsification" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If the RD cutoff is known to the units that will be the beneficiaries of
the treatment, researchers must worry about the possibility of units
actively changing or manipulating the value of their score when they
miss the treatment barely</p>
<p>Naturally, the continuity assumptions that guarantee the validity of the
RD design are about unobservable features and as such are inherently
untestable.</p>
<p><strong>1. Predetermined Covariates and Placebo Outcomes</strong></p>
<p>One of the most important RD falsification tests involves examining whether, near the cutoff, treated units are similar to control units in terms of observable characteristics.</p>
<p>if units lack the ability to precisely manipulate the score value they receive, there should be no systematic differences between units with similar values of the score</p>
<ul>
<li>Predetermined Covariates: variables determined before treatment.</li>
<li>placebo outcomes: variables determined after treatment</li>
</ul>
<p>placebo outcomes arealways specific to each application. For example, for the study investigating the impact of clean water on child mortality, road accidents for child mortaility.</p>
<p>Outcomes would not be affected by treatment if there is no coincidence.</p>
<p>In the continuity-based approach, this principle means that for each predetermined covariate or placebo outcome, researchers should first choose an optimal bandwidth, and then use local polynomial techniques within that bandwidth to estimate the “treatment effect” and employ valid inference procedures such as the robust bias-corrected methods discussed previously.</p>
<p>The fundamental idea behind this test is that, since the pre- determined covariate (or placebo outcome) could not have been affected by the treatment, the null hypothesis of no treatment effect should not be rejected if the RD design is valid.</p>
<p>To implement this formal falsification test, we simply run rdrobust using each covariate of interest as the outcome variable.</p>
<p>(Read page 94)[<a href="https://rdpackages.github.io/references/Cattaneo-Idrobo-Titiunik_2020_CUP.pdf" class="uri">https://rdpackages.github.io/references/Cattaneo-Idrobo-Titiunik_2020_CUP.pdf</a>]</p>
<p><strong>2. Density of Running variable</strong></p>
<p>Check whether there is sorting.</p>
<p>Examine whether in a local neighborhood near the cutoff, the number of observations below the cutoff is surprisingly different from the number of observations above it.</p>
<p>if units do not have the ability to precisely manipulate the value of the score that they receive, the number of treated observations just above the cutoff should be approximately similar to the number of control observations below it.</p>
<p>No guideline on bandwidth to be inspected, several bandwidths may be presented.</p>
<p>Histogram would be helpful.</p>
<p>Formal test uses binomial test. Choose a small neighborhood around the cutoff, and perform a simple Bernoulli test within that neighborhood with a probability of “success” equal to 1/2. This strategy tests whether the number of treated observations in the chosen neighborhood is compatible with what would have been observed if units had been assigned to the treatment group (i.e., to being above the cutoff) with a 50% probability.</p>
<p>Or rddensity test.</p>
<ul>
<li><p>Both the continuity-based approach and the local randomization
approach rely on the assumption that units that receive very similar
score values on opposite sides of the cutoff are comparable to each
other in all relevant aspects, except for their treatment status.</p></li>
<li><p>The main distinction between these frameworks is how the idea of
comparability is formalized: in the continuity-based framework,
comparability is conceptualized as continuity of average (or some
other feature of) potential outcomes near the cutoff, while in the
local randomization framework, comparability is conceptualized as
conditions that mimic a randomized experiment in a neighborhood
around the cutoff.</p></li>
</ul>
<p><strong>3. Placebo Cutoffs</strong></p>
<p>Another useful falsification analysis examines treatment effects at artificial or placebo cutoff values.</p>
<p>Evidence of continuity away from the cutoff is, of course, neither necessary nor sufficient for continuity at the cutoff, but the presence of discontinuities away from the cutoff can be interpreted as potentially casting doubt on the RD design</p>
<p>This test replaces the true cutoff value by another value at which the treatment status does not really change, and performs estimation and inference using this artificial cutoff point. The expectation is that no significant treatment effect will occur at placebo cutoff values.</p>
<p>To avoid “contamination” due to real treatment effects, for artificial cutoffs above the real cutoff we use only treated observations, and for artificial cutoffs below the real cutoff we use only control observations. Restricting the observations in this way guarantees that the analysis of each placebo cutoff uses only observations with the same treatment status.</p>
<p><strong>4. Sensitivity to Observations near the cutoff</strong></p>
<p>If systematic manipulation of score values has occurred, it is natural to assume that the units closest to the cutoff are those most likely to have engaged in manipulation.</p>
<p>The idea behind this approach is to exclude such units and then repeat the estimation and inference analysis using the remaining sample. This idea is sometimes referred to as a “donut hole” approach.</p>
<p>In practice, it is natural to repeat this exercise a few times to assess the actual sensitivity for different amounts of excluded units.</p>
<p><strong>5. Sensitivity to Bandwidth Choice</strong></p>
<p>The method now investigates sensitivity as units are added or removed at the end points of the neighborhood.</p>
<p>Choosing the bandwidth is one of the most consequential decisions in RD analysis, because the bandwidth may affect the results and conclusions.</p>
<p>In the continuity-based approach, this falsification test is implemented by changing the bandwidth used for local polynomial estimation.</p>
<p>As h increases, bias of local polynomial estimator will increase and variance will decrease and CI will get narrower.</p>
</div>
<div id="regression-discontinuity-designs-rdd-1" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> Regression Discontinuity Designs (RDD)<a href="regression-discontinuity-designs-rdd.html#regression-discontinuity-designs-rdd-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Regression Discontinuity Designs (RDD) are quasi-experimental methods
used to estimate causal effects when a treatment is assigned based on a
cutoff point in a continuous assignment variable. The basic idea is to
compare observations just above and below the cutoff, assuming they are
similar except for the treatment.</p>
<p>RDD is particularly suited to visual analysis. By plotting the running
variable against the outcome variable, we can observe any “jumps” in the
probability of treatment at the cutoff. These jumps indicate the
treatment effect.</p>
</div>
<div id="key-concepts-6" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Key Concepts<a href="regression-discontinuity-designs-rdd.html#key-concepts-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p><strong>Assignment Variable</strong>: A continuous (running) variable that
determines treatment assignment based on a cutoff point.</p></li>
<li><p><strong>Cutoff Point</strong>: The threshold value of the assignment variable
that determines who receives the treatment.</p></li>
<li><p><strong>Treatment Group</strong>: Observations with assignment variable values
above (or below) the cutoff.</p></li>
<li><p><strong>Control Group</strong>: Observations with assignment variable values
below (or above) the cutoff.</p></li>
</ol>
<ul>
<li><p><strong>Local Average Treatment Effect (LATE)</strong>: In RDD, we focus on
estimating the treatment effect for observations near the cutoff.
Since the probability of receiving the treatment changes abruptly at
the cutoff, we compare outcomes for those just above and just below
this point to estimate LATE.</p></li>
<li><p><strong>No Overlap/Common Support</strong>: Unlike randomized controlled trials
(RCTs), RDD lacks overlap between treatment and control groups
across the entire range of the running variable. Instead, it relies
on extrapolation by comparing units with different values of the
running variable that are close to the cutoff. As we approach the
cutoff from either direction, the units become more comparable,
which allows us to estimate the causal effect.</p></li>
<li><p><strong>Handling Extrapolation Bias</strong>: All methods used in RDD aim to
address the bias arising from the need to extrapolate. These methods
ensure that the comparisons made are as clean and unbiased as
possible.</p></li>
</ul>
<div id="types-of-rdd" class="section level3 hasAnchor" number="9.2.1">
<h3><span class="header-section-number">9.2.1</span> Types of RDD<a href="regression-discontinuity-designs-rdd.html#types-of-rdd" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li><p><strong>Sharp RDD</strong>: Treatment assignment is strictly determined by the
cutoff. All units above (or below) the cutoff receive the treatment,
and none below (or above) do.</p></li>
<li><p><strong>Fuzzy RDD</strong>: Treatment assignment is probabilistic at the cutoff.
Not all units above (or below) the cutoff receive the treatment, and
some units below (or above) the cutoff may receive the treatment.</p></li>
</ol>
</div>
</div>
<div id="sharp-rdd" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Sharp RDD<a href="regression-discontinuity-designs-rdd.html#sharp-rdd" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In Sharp RDD, the treatment is perfectly assigned based on the cutoff
point. This can be represented as:</p>
<p><span class="math display">\[ D_i = \begin{cases}
1 &amp; \text{if } X_i \geq c \\
0 &amp; \text{if } X_i &lt; c
\end{cases}\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(D_i\)</span> is the treatment indicator.<br />
</p></li>
<li><p><span class="math inline">\(X_i\)</span> is the assignment variable.<br />
</p></li>
<li><p><span class="math inline">\(c\)</span> is the cutoff point.</p></li>
<li><p>Full compliance.</p></li>
<li><p>only one cutoff.</p></li>
<li><p>score is continuously distributed</p></li>
</ul>
<div id="key-concepts-7" class="section level3 hasAnchor" number="9.3.1">
<h3><span class="header-section-number">9.3.1</span> Key Concepts<a href="regression-discontinuity-designs-rdd.html#key-concepts-7" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The sharp RDD estimation is interpreted as an average causal effect of
the treatment as the running variable approaches the cutoff in the
limit, for it is only in the limit that we have overlap. This average
causal effect is the <strong>local average treatment effect (LATE)</strong>.</p>
<p>Notice the role that <strong>extrapolation</strong> plays in estimating treatment
effects with sharp RDD. If unit <span class="math inline">\(i\)</span> is just below <span class="math inline">\(c_0\)</span>, then <span class="math inline">\(D_i = 0\)</span>.
But if unit <span class="math inline">\(i\)</span> is just above <span class="math inline">\(c_0\)</span>, then <span class="math inline">\(D_i = 1\)</span>. But for any value
of <span class="math inline">\(X_i\)</span>, there are either units in the treatment group or the control
group, but not both. Therefore, the RDD does not have common support,
which is one of the reasons we rely on extrapolation for our estimation.</p>
<ol style="list-style-type: decimal">
<li><strong>Unit</strong> <span class="math inline">\(i\)</span>: Represents an individual observation in your dataset.</li>
<li><span class="math inline">\(c_0\)</span>: The cutoff value of the assignment variable <span class="math inline">\(X\)</span> that
determines treatment assignment.</li>
<li><span class="math inline">\(D_i\)</span>: The treatment indicator variable, where <span class="math inline">\(D_i = 1\)</span> if the unit
receives the treatment and <span class="math inline">\(D_i = 0\)</span> if it does not.</li>
<li><strong>Common Support</strong>: A situation where there are treated and control
units with similar values of the assignment variable <span class="math inline">\(X\)</span>.</li>
</ol>
<p>There is no common support because control and treated groups may not
have same value of running variable.</p>
<div id="explanation-1" class="section level4 hasAnchor" number="9.3.1.1">
<h4><span class="header-section-number">9.3.1.1</span> Explanation<a href="regression-discontinuity-designs-rdd.html#explanation-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In a sharp RDD:</p>
<p><strong>Treatment Assignment</strong>: Units are assigned to treatment or control
strictly based on whether their assignment variable <span class="math inline">\(X\)</span> is above or
below the cutoff <span class="math inline">\(c_0\)</span>.</p>
<ul>
<li><p>If <span class="math inline">\(X_i\)</span> (the assignment variable for unit <span class="math inline">\(i\)</span>) is just below <span class="math inline">\(c_0\)</span>,
then <span class="math inline">\(D_i = 0\)</span> (the unit is in the control group).</p></li>
<li><p>If <span class="math inline">\(X_i\)</span> is just above <span class="math inline">\(c_0\)</span>, then <span class="math inline">\(D_i = 1\)</span> (the unit is in the
treatment group).</p></li>
</ul>
</div>
<div id="no-common-support-in-rdd" class="section level4 hasAnchor" number="9.3.1.2">
<h4><span class="header-section-number">9.3.1.2</span> No Common Support in RDD<a href="regression-discontinuity-designs-rdd.html#no-common-support-in-rdd" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>No Overlap</strong>: For any given value of <span class="math inline">\(X_i\)</span>, units are either in
the treatment group or the control group, but not both. This means
that at any specific value of <span class="math inline">\(X_i\)</span>, you don’t have both treated and
untreated units.
<ul>
<li>For example, if <span class="math inline">\(X_i\)</span> is exactly <span class="math inline">\(c_0\)</span>, you don’t have units
both treated and untreated at that exact point (in practical
terms, it’s often the case we look just below and just above
<span class="math inline">\(c_0\)</span>).</li>
</ul></li>
</ul>
</div>
<div id="extrapolation-in-rdd" class="section level4 hasAnchor" number="9.3.1.3">
<h4><span class="header-section-number">9.3.1.3</span> Extrapolation in RDD<a href="regression-discontinuity-designs-rdd.html#extrapolation-in-rdd" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Extrapolation</strong>: To estimate the treatment effect at the cutoff, we
essentially need to compare the outcomes of units just below and just
above the cutoff. Since there are no units that have exactly the same
value of <span class="math inline">\(X\)</span> but different treatment statuses, we use the units close to
the cutoff to infer what would happen if a unit’s treatment status were
different.</p>
<p><strong>Example</strong>: Suppose the cutoff <span class="math inline">\(c_0\)</span> is 50. Units with <span class="math inline">\(X_i = 49.9\)</span> are
in the control group and units with <span class="math inline">\(X_i = 50.1\)</span> are in the treatment
group. We compare the outcomes of these units to estimate the treatment
effect.</p>
</div>
<div id="why-extrapolation-is-needed" class="section level4 hasAnchor" number="9.3.1.4">
<h4><span class="header-section-number">9.3.1.4</span> Why Extrapolation is Needed<a href="regression-discontinuity-designs-rdd.html#why-extrapolation-is-needed" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Local Comparisons</strong>: In RDD, we rely on the assumption that units just
below and just above the cutoff are very similar in all respects except
for the treatment. Thus, we “extrapolate” the behavior of one group to
understand the counterfactual of the other.</p>
<p><strong>Local Treatment Effect</strong>: This local comparison near the cutoff allows
us to estimate the causal effect of the treatment precisely at <span class="math inline">\(c_0\)</span>.</p>
</div>
<div id="summary-1" class="section level4 hasAnchor" number="9.3.1.5">
<h4><span class="header-section-number">9.3.1.5</span> Summary<a href="regression-discontinuity-designs-rdd.html#summary-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In summary, the lack of common support in RDD means we don’t have units
that are both treated and untreated at the same value of the assignment
variable. As a result, we rely on extrapolation, comparing units just
below and just above the cutoff to estimate the treatment effect. This
is because these units are assumed to be similar except for the
treatment, allowing us to infer what the outcome would be if their
treatment status were different.</p>
</div>
</div>
<div id="assumptions-for-rdd" class="section level3 hasAnchor" number="9.3.2">
<h3><span class="header-section-number">9.3.2</span> Assumptions for RDD<a href="regression-discontinuity-designs-rdd.html#assumptions-for-rdd" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="continuity-assumption-in-rdd" class="section level4 hasAnchor" number="9.3.2.1">
<h4><span class="header-section-number">9.3.2.1</span> Continuity Assumption in RDD<a href="regression-discontinuity-designs-rdd.html#continuity-assumption-in-rdd" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li><strong>Definition</strong>: The potential outcomes (both treated and untreated)
must be continuous at the cutoff. This ensures that any jump in the
outcome at the cutoff can be attributed to the treatment.</li>
</ol>
<p><strong>Expected Potential Outcomes</strong>: The assumption states that the expected
potential outcomes are continuous at the cutoff. In simpler terms, if we
plot the expected outcomes of the units just below and just above the
cutoff, the outcomes would form a smooth curve <strong>if there were no
treatment effect</strong>.</p>
<ul>
<li>If there were no treatment, there would not be a jump at the cutoff.</li>
</ul>
<p><strong>Ruling Out Competing Interventions</strong>: If expected potential outcomes
are continuous at the cutoff, it necessarily rules out competing
interventions or other factors that might cause a discontinuity at the
cutoff. This is crucial because we want to attribute any observed jump
in the outcome to the treatment alone, not to some other unobserved
factor.</p>
<p><strong>Omitted Variable Bias</strong>: Continuity explicitly rules out omitted
variable bias at the cutoff. This means that all other unobserved
determinants of the outcome variable <span class="math inline">\(Y\)</span> are smoothly related to the
running variable <span class="math inline">\(X\)</span>. Therefore, any abrupt change at the cutoff can be
confidently attributed to the treatment effect.</p>
<p><strong>Interpreting the Assumption Mathematically</strong>:</p>
<p><span class="math inline">\(E[Y^0 | X]\)</span> and <span class="math inline">\(E[Y^1 | X]\)</span> are the expected outcomes if the unit did
not receive and did receive the treatment, respectively.</p>
<p>The continuity assumption means that <span class="math inline">\(E[Y^0 | X]\)</span> and <span class="math inline">\(E[Y^1 | X]\)</span> would
not exhibit a sudden jump at the cutoff <span class="math inline">\(c_0\)</span> in the absence of
treatment.</p>
<p>If there is a jump at <span class="math inline">\(c_0\)</span>, it indicates the presence of the treatment
effect because in the absence of the treatment, <span class="math inline">\(E[Y^1 | X]\)</span> should not
change abruptly.</p>
<p><strong>Example to Illustrate Continuity Assumption</strong></p>
<p>Imagine we are studying the effect of a scholarship program on student
test scores, where the scholarship is given to students who score above
a certain cutoff on a preliminary test.</p>
<p><strong>Continuity without Treatment</strong>: If there were no scholarship, the
expected test scores of students just below and just above the cutoff
should be very similar and form a smooth curve.</p>
<p><strong>Jump Due to Treatment</strong>: If we observe a jump in test scores exactly
at the cutoff, this jump can be attributed to the effect of receiving
the scholarship, assuming the continuity assumption holds.</p>
<p><strong>Summary</strong></p>
<p>The continuity assumption in RDD is crucial for identifying the causal
effect of the treatment. It ensures that any observed discontinuity in
the outcome at the cutoff can be attributed solely to the treatment and
not to any other unobserved factors. This assumption rules out omitted
variable bias at the cutoff, ensuring the reliability of the estimated
treatment effect.</p>
<p>In essence, the continuity assumption guarantees that the treatment
effect is the only factor causing a jump in the outcome at the cutoff,
allowing us to make causal inferences from the RDD design.</p>
</div>
<div id="no-manipulation-sorting" class="section level4 hasAnchor" number="9.3.2.2">
<h4><span class="header-section-number">9.3.2.2</span> No Manipulation (sorting)<a href="regression-discontinuity-designs-rdd.html#no-manipulation-sorting" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Units cannot precisely manipulate the assignment variable to end up on
one side of the cutoff. This ensures that the units just above and below
the cutoff are comparable.</p>
</div>
</div>
<div id="estimation-in-sharp-rdd" class="section level3 hasAnchor" number="9.3.3">
<h3><span class="header-section-number">9.3.3</span> Estimation in Sharp RDD<a href="regression-discontinuity-designs-rdd.html#estimation-in-sharp-rdd" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Parametric Approach</strong>: Fit separate linear regressions on either side
of the cutoff and estimate the treatment effect as the difference in the
intercepts at the cutoff.</p>
<p><span class="math display">\[ Y_i = \alpha_1 + \beta_1 X_i + \epsilon_i \quad \text{if } X_i \geq c \]</span>
<span class="math display">\[ Y_i = \alpha_0 + \beta_0 X_i + \epsilon_i \quad \text{if } X_i &lt; c \]</span></p>
<p>The treatment effect is <span class="math inline">\(\alpha_1 - \alpha_0\)</span>.</p>
<p><strong>Non-Parametric Approach</strong>: Use local polynomial regression or kernel
regression to fit the data near the cutoff. This method is preferred
because it makes fewer assumptions about the functional form of the
relationship between the assignment variable and the outcome.</p>
<ul>
<li><p>cluster at running variable ( bad idea)</p></li>
<li><p>use heteroskedastic robust standard errors</p></li>
<li><p>kernel type, cutof window</p></li>
</ul>
</div>
<div id="fuzzy-rdd" class="section level3 hasAnchor" number="9.3.4">
<h3><span class="header-section-number">9.3.4</span> Fuzzy RDD<a href="regression-discontinuity-designs-rdd.html#fuzzy-rdd" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Fuzzy RDD, the probability of receiving treatment changes
discontinuously at the cutoff but is not perfectly deterministic. This
can be represented as:</p>
<p><span class="math display">\[ D_i = \begin{cases}
1 &amp; \text{with probability } p_1 \text{ if } X_i \geq c \\
0 &amp; \text{with probability } p_0 \text{ if } X_i &lt; c
\end{cases}
\]</span></p>
<p>Where <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_0\)</span> are the probabilities of receiving treatment above
and below the cutoff, respectively.</p>
<div id="estimation-in-fuzzy-rdd" class="section level4 hasAnchor" number="9.3.4.1">
<h4><span class="header-section-number">9.3.4.1</span> Estimation in Fuzzy RDD<a href="regression-discontinuity-designs-rdd.html#estimation-in-fuzzy-rdd" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>Instrumental Variables (IV) Approach</strong>: Use the cutoff as an
instrument for actual treatment receipt. The first stage estimates
the probability of treatment, and the second stage uses this to
estimate the treatment effect.</li>
</ul>
<p>First stage: <span class="math display">\[ D_i = \pi_0 + \pi_1 Z_i + \eta_i \]</span></p>
<p>Second stage: <span class="math display">\[ Y_i = \alpha + \beta \hat{D}_i + \epsilon_i \]</span></p>
<p>Where <span class="math inline">\(Z_i\)</span> is an indicator variable equal to 1 if <span class="math inline">\(X_i \geq c\)</span> and 0
otherwise.</p>
</div>
</div>
<div id="parametric-vs.-non-parametric-applications" class="section level3 hasAnchor" number="9.3.5">
<h3><span class="header-section-number">9.3.5</span> Parametric vs. Non-Parametric Applications<a href="regression-discontinuity-designs-rdd.html#parametric-vs.-non-parametric-applications" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Parametric Applications</strong>: Assume a specific functional form (e.g.,
linear) for the relationship between the assignment variable and the
outcome. Simpler to implement but relies heavily on the correct
specification of the model.</p>
<p><strong>Non-Parametric Applications</strong>: Make fewer assumptions about the
functional form. Typically use methods like local polynomial regression
or kernel regression to fit the data near the cutoff. More flexible and
robust but can be more complex to implement and interpret.</p>
</div>
<div id="example-13" class="section level3 hasAnchor" number="9.3.6">
<h3><span class="header-section-number">9.3.6</span> Example<a href="regression-discontinuity-designs-rdd.html#example-13" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose a school district awards scholarships to students who score
above a certain threshold on a standardized test.</p>
<ul>
<li><p><strong>Sharp RDD</strong>: All students who score above 80 receive the
scholarship, and none below 80 do. We compare students scoring just
above 80 to those just below to estimate the effect of receiving the
scholarship on academic outcomes.</p></li>
<li><p><strong>Fuzzy RDD</strong>: Students who score above 80 are more likely to
receive the scholarship, but not all do (e.g., due to additional
criteria or random factors). We use the score of 80 as an instrument
to estimate the causal effect of receiving the scholarship.</p></li>
</ul>
</div>
<div id="summary-2" class="section level3 hasAnchor" number="9.3.7">
<h3><span class="header-section-number">9.3.7</span> Summary<a href="regression-discontinuity-designs-rdd.html#summary-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Regression Discontinuity Designs are powerful tools for causal inference
in observational studies where treatment assignment is based on a
cutoff.</p>
<p>Sharp RDDs assume perfect treatment assignment based on the cutoff,
while Fuzzy RDDs allow for probabilistic treatment assignment.</p>
<p>Both parametric and non-parametric approaches can be used, with
non-parametric methods generally preferred for their flexibility. Key
assumptions include the continuity of potential outcomes and the
inability of units to manipulate their assignment variable precisely.</p>
<p>The reason RDD is so appealing to many is because of its ability to
convincingly eliminate selection bias.</p>
<p>Assignment variable, is often called the “running variable”—is an
observable confounder since it causes both Treatment and Outcome.</p>
<p>The assignment variable assigns treatment on the basis of a cutoff, we
are never able to observe units in both treatment and control for the
same value of X.</p>
<p>We can identify causal effects for those subjects whose score is in a
close neighborhood around some cutoff <span class="math inline">\(c_o\)</span>.</p>
</div>
<div id="challenges-to-identification" class="section level3 hasAnchor" number="9.3.8">
<h3><span class="header-section-number">9.3.8</span> Challenges to Identification<a href="regression-discontinuity-designs-rdd.html#challenges-to-identification" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The requirement for RDD to estimate a causal effect are the continuity
assumptions. That is, the expected potential outcomes change smoothly as
a function of the running variable through the cutoff. In words, this
means that the only thing that causes the outcome to change abruptly at
is the treatment. But, this can be violated in practice if any of the
following is true:</p>
<ul>
<li><p>The assignment rule is known in advance.</p></li>
<li><p>Agents are interested in adjusting.</p></li>
<li><p>Agents have time to adjust.</p></li>
<li><p>The cutoff is endogenous to factors that independently cause
potential outcomes to shift.</p></li>
<li><p>There is nonrandom heaping along the running variable.</p></li>
<li><p>Examples include retaking an exam, self-reporting income, and so on.</p></li>
<li><p>The cutoff is endogenous. An example would be age thresholds used
for policy, such as when a person turns 18 years old and faces more
severe penalties for crime. This age threshold triggers the
treatment (i.e., higher penalties for crime), but is also correlated
with variables that affect the outcomes, such as graduating from
high school and voting rights. Let’s tackle these problems
separately.</p></li>
</ul>
<p>Although assumptions may not be tested directly, indirect evidence may
be show to be persuasive.</p>
<div id="density-test" class="section level4 hasAnchor" number="9.3.8.1">
<h4><span class="header-section-number">9.3.8.1</span> Density Test<a href="regression-discontinuity-designs-rdd.html#density-test" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>density test is used to check whether units are sorting on the running
variable.</p>
<p>under the null, the density should be continuous at the cutoff point.
Under the alternative hypothesis, the density should increase at the
kink.</p>
</div>
<div id="covariate-balance-and-other-placebos" class="section level4 hasAnchor" number="9.3.8.2">
<h4><span class="header-section-number">9.3.8.2</span> Covariate balance and Other placebos<a href="regression-discontinuity-designs-rdd.html#covariate-balance-and-other-placebos" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For RDD to be valid in your study, there must not be an observable
discontinuous change in the average values of reasonably chosen
covariates around the cutoff.</p>
<p>As these are pretreatment characteristics, they should be invariant to
change in treatment assignment.</p>
<p>This test is basically what is sometimes called a placebo test. That is,
you are looking for there to be no effects where there shouldn’t be any.</p>
<p>So a third kind of test is an extension of that—just as there shouldn’t
be effects at the cutoff on pretreatment values, there shouldn’t be
effects on the outcome of interest at arbitrarily chosen cutoffs. Guido
W. Imbens and Lemieux (2008) suggest looking at one side of the
discontinuity, taking the median value of the running variable in that
section, and pretending it was a discontinuity, <span class="math inline">\(c^{i}_0\)</span>. Then test
whether there is a discontinuity in the outcome at <span class="math inline">\(c^{i}_0\)</span>. You do not
want to find anything.</p>
</div>
<div id="non-random-heaping-in-running-variable" class="section level4 hasAnchor" number="9.3.8.3">
<h4><span class="header-section-number">9.3.8.3</span> Non-random heaping in running variable<a href="regression-discontinuity-designs-rdd.html#non-random-heaping-in-running-variable" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Heaping occurs when there is an excess number of units at certain points
along the running variable. In this case, it seems to happen at regular
100-gram intervals, likely due to hospitals rounding to the nearest
integer.</p>
<p>This pattern is unlikely to occur naturally and is almost certainly
caused by either sorting or rounding. It could result from less
sophisticated scales or, more concerningly, from staff rounding a
child’s birth weight to 1,500 grams to make the child eligible for
increased medical attention.</p>
<p>In RDD, estimation compares means as we approach the threshold from
either side, so the estimates should not be overly influenced by the
observations at the threshold itself. One solution to address this issue
is the “donut hole” RDD, where units in the vicinity of 1,500 grams are
removed, and the model is re-estimated.</p>
</div>
</div>
<div id="examples" class="section level3 hasAnchor" number="9.3.9">
<h3><span class="header-section-number">9.3.9</span> Examples<a href="regression-discontinuity-designs-rdd.html#examples" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="close-election" class="section level4 hasAnchor" number="9.3.9.1">
<h4><span class="header-section-number">9.3.9.1</span> Close Election<a href="regression-discontinuity-designs-rdd.html#close-election" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Regression Discontinuity Design (RDD) can be effectively used in the
context of close elections to identify causal effects. The key idea is
that in very close elections, the outcome of the election (winning or
losing) can be considered as good as random. This randomness allows
researchers to estimate the causal effect of winning an election on
various outcomes.</p>
<p>They argue that just around that cutoff, random chance determined the
Democratic win—hence the random assignment of <span class="math inline">\(D_t\)</span></p>
<p><strong>Identifying the Impact of Political Office on Economic Policies:</strong></p>
<p><strong>Scenario</strong>: Consider a study aiming to determine whether holding
political office affects a politician’s subsequent policy decisions or
economic outcomes in their district.</p>
<p><strong>RDD Approach</strong>: Researchers focus on elections decided by a very small
margin of votes. For example, sample includes only observations where
the Democrat vote share at time is strictly between 48 percent and 52
percent.</p>
<p><strong>Assumption</strong>: In close elections, the distribution of voter
preferences is assumed to be similar on both sides of the cutoff
(winning or losing by a small margin). This similarity allows the
comparison of outcomes just above and just below the threshold.</p>
<p><strong>Application</strong>: By comparing districts where the incumbent barely won
to those where the incumbent barely lost, researchers can isolate the
effect of holding office on policy decisions and economic outcomes.</p>
</div>
<div id="education-policies" class="section level4 hasAnchor" number="9.3.9.2">
<h4><span class="header-section-number">9.3.9.2</span> Education Policies<a href="regression-discontinuity-designs-rdd.html#education-policies" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Identifying the Impact of Scholarship Programs:</strong></p>
<p><strong>Scenario</strong>: A scholarship program is awarded to students who score
above a certain threshold on an entrance exam.</p>
<p><strong>RDD Approach</strong>: Researchers compare students who score just above the
threshold (and receive the scholarship) to those who score just below
(and do not receive the scholarship).</p>
<p><strong>Assumption</strong>: Students near the cutoff are similar in all respects
except for receiving the scholarship.</p>
<p><strong>Application</strong>: By analyzing differences in educational attainment and
future earnings between the two groups, researchers can estimate the
causal impact of the scholarship program.</p>
</div>
<div id="health-interventions" class="section level4 hasAnchor" number="9.3.9.3">
<h4><span class="header-section-number">9.3.9.3</span> Health Interventions<a href="regression-discontinuity-designs-rdd.html#health-interventions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Identifying the Impact of Health Interventions:</strong></p>
<p><strong>Scenario</strong>: A public health intervention is provided to individuals
whose health risk score exceeds a certain threshold.</p>
<p><strong>RDD Approach</strong>: Researchers compare individuals who just qualify for
the intervention to those who just miss the qualification.</p>
<p><strong>Assumption</strong>: Individuals near the threshold are comparable in health
status except for receiving the intervention.</p>
<p><strong>Application</strong>: By examining health outcomes such as disease incidence
or hospitalization rates, researchers can infer the causal effect of the
health intervention.</p>
</div>
</div>
<div id="types-of-rdd-1" class="section level3 hasAnchor" number="9.3.10">
<h3><span class="header-section-number">9.3.10</span> Types of RDD<a href="regression-discontinuity-designs-rdd.html#types-of-rdd-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="sharp-rdd-1" class="section level4 hasAnchor" number="9.3.10.1">
<h4><span class="header-section-number">9.3.10.1</span> Sharp RDD<a href="regression-discontinuity-designs-rdd.html#sharp-rdd-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In sharp RDD, the treatment assignment is strictly determined by whether
the running variable crosses a threshold.</p>
<ul>
<li><strong>Example</strong>: A tax credit is given to families whose income is below
a certain cutoff. Families just below the cutoff receive the tax
credit, while those just above do not.</li>
</ul>
</div>
<div id="fuzzy-rdd-1" class="section level4 hasAnchor" number="9.3.10.2">
<h4><span class="header-section-number">9.3.10.2</span> Fuzzy RDD<a href="regression-discontinuity-designs-rdd.html#fuzzy-rdd-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In fuzzy RDD, the probability of treatment assignment jumps at the
threshold but not perfectly.</p>
<ul>
<li><strong>Example</strong>: Eligibility for a drug rehabilitation program is
determined by a cutoff on an addiction severity score, but not all
eligible individuals enroll in the program.</li>
</ul>
</div>
</div>
<div id="parametric-vs.-non-parametric-applications-1" class="section level3 hasAnchor" number="9.3.11">
<h3><span class="header-section-number">9.3.11</span> Parametric vs. Non-Parametric Applications<a href="regression-discontinuity-designs-rdd.html#parametric-vs.-non-parametric-applications-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="parametric-rdd" class="section level4 hasAnchor" number="9.3.11.1">
<h4><span class="header-section-number">9.3.11.1</span> Parametric RDD<a href="regression-discontinuity-designs-rdd.html#parametric-rdd" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Parametric RDD involves fitting a parametric model (e.g., a polynomial
regression) to estimate the relationship between the running variable
and the outcome.</p>
<ul>
<li><strong>Example</strong>: Using a polynomial regression model to estimate the
impact of an education intervention on test scores.</li>
</ul>
</div>
<div id="non-parametric-rdd" class="section level4 hasAnchor" number="9.3.11.2">
<h4><span class="header-section-number">9.3.11.2</span> Non-Parametric RDD<a href="regression-discontinuity-designs-rdd.html#non-parametric-rdd" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Non-parametric RDD uses local polynomial regression or other
non-parametric methods to estimate the treatment effect, focusing on
observations near the cutoff.</p>
<ul>
<li><strong>Example</strong>: Applying local linear regression to estimate the impact
of a health intervention on patient recovery rates.</li>
</ul>
</div>
</div>
</div>
<div id="regression-kink-design" class="section level2 hasAnchor" number="9.4">
<h2><span class="header-section-number">9.4</span> Regression Kink Design<a href="regression-discontinuity-designs-rdd.html#regression-kink-design" class="anchor-section" aria-label="Anchor link to header"></a></h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="instrumental-variables-iv.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="fixed-effects-and-panel-data-methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Causal Methods.pdf", "Causal Methods.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

[["index.html", "Notes on Causal Models Acknowledgement", " Notes on Causal Models DEA 2025-08-17 Acknowledgement I would like to acknowledge the following sources that have greatly contributed to the preparation of this notebook on causal models. I have collected and prepared this collection to keep myself up-to-date and to serve as a resource for reviewing all aspects of causal models. Please use it at your own risk and let me know if you have any comments, find any errors, or notice any typos. Here are the excellent online resources and references in no particular order: Program Evaluation Causal Inference for the Brave and True A guide on Data analysis The Mixtape The Effect What if "],["concepts.html", "Chapter 1 Concepts 1.1 Goodness of Fit 1.2 Validation Methods 1.3 Directed Acyclic Graphs (DAGs) 1.4 Bad Controls 1.5 Endogeneity 1.6 Reduced Form Model 1.7 Standard Errors 1.8 Types of Biases in Econometrics &amp; Statistics 1.9 Causality", " Chapter 1 Concepts 1.1 Goodness of Fit 1.1.1 R-squared (\\(R^2\\)) Measures the proportion of variation in \\(y\\) explained by the model. Useful for prediction tasks, but not for assessing causality. A high \\(R^2\\) can come from including irrelevant or even harmful controls. 1.1.2 Adjusted R-squared Adjusts for the number of predictors; penalizes models for adding unnecessary variables. Better for comparing models with different numbers of regressors, but still not a causal metric. 1.1.3 Root Mean Squared Error (RMSE) / Mean Absolute Error (MAE) Direct measures of prediction accuracy. Lower values indicate closer predictions to the actual \\(y\\). Especially relevant when the goal is forecasting rather than causal inference. 1.1.4 AIC / BIC (Information Criteria) Model selection criteria that balance fit and complexity. Useful when comparing non-nested models, but again, not measures of causal validity. 1.1.5 F-statistics and Joint Significance Tests Test whether groups of variables jointly contribute explanatory power. Helpful for model fit diagnostics but not sufficient for causal claims. 1.1.6 Balance Checks (Causal Context) In causal inference, we care more about whether confounders are balanced across treatment groups than about \\(R^2\\). Matching, weighting, or randomization checks are more informative than fit metrics. 1.1.7 Key Distinction: Prediction vs.¬†Causality Context Metrics that Matter Goal Prediction / Forecasting \\(R^2\\), Adj. \\(R^2\\), RMSE, MAE, AIC, BIC Maximize predictive accuracy Causal Inference Balance checks, robustness checks, IV strength, falsification tests Identify unbiased treatment effect üëâ Takeaway: Use \\(R^2\\) and related metrics for prediction, but in causal inference, what matters most is whether you‚Äôve properly handled confounders, closed backdoor paths, and used valid identification strategies. 1.2 Validation Methods Definition Validation methods are procedures used to confirm that the analytical approach and findings are credible, correctly specified, and reliable. The goal is to ensure that the methodology captures the true causal relationship and that the results are not artifacts of flawed design or bias. 1.2.1 Internal Validity Ensures the causal estimate is credible within the study design. Purpose: Rule out alternative explanations for the observed effect. Methods: Placebo / falsification tests ‚Üí Apply the method to periods or outcomes where no effect should exist. A true effect should not show up here. Pre-trend checks ‚Üí In Difference-in-Differences (DiD), verify that treatment and control groups followed parallel trends before treatment. Balance tests (PSM, matching) ‚Üí Confirm that covariates are similar between treated and control groups. Overidentification tests (IV) ‚Üí If multiple instruments exist, check that they give consistent estimates. Sensitivity analysis ‚Üí Evaluate robustness to unobserved confounders (e.g., Rosenbaum bounds). Resampling methods (bootstrap, cross-validation) ‚Üí Ensure stability of results against sampling variation. 1.2.2 External Validity Ensures findings can generalize beyond the study sample. Purpose: Assess whether results apply to other populations, settings, or time periods. Methods: Benchmark comparisons ‚Üí Compare effect sizes with prior literature or related experiments. Replication across subgroups ‚Üí Test if results hold in different populations (e.g., small vs.¬†large cities, East vs.¬†West Coast). Heterogeneity analysis ‚Üí Explore variation in treatment effects across demographics, regions, or time. Transportability / reweighting methods ‚Üí Adjust sample weights so that results better reflect the target population (e.g., balancing weights to correct skewed panels). Why Both Matter Internal validity ensures the effect you measured is truly causal. External validity ensures that causal effect is useful for decision-making across broader contexts. Together, they strengthen the credibility, reliability, and practical relevance of your causal inference. 1.2.3 Robustness Checks Purpose: Assess the stability of results under alternative assumptions or model choices. Examples: Varying model specifications (linear vs.¬†log, fixed effects vs.¬†random effects). Dropping outliers or trimming the sample. Using alternative control groups. Trying different functional forms (log of sales vs.¬†sales level). Changing time windows (short-run vs.¬†long-run effect). Focus: Do the results hold up if we ‚Äústress-test‚Äù the model? How They Relate Validation asks: Did we design this study right? Robustness asks: Would the results still hold if we made reasonable changes? 1.3 Directed Acyclic Graphs (DAGs) causality runs in one direction, it runs forward in time. There are no cycles in a DAG. To show reverse causality, one would need to create multiple nodes, most likely with two versions of the same node separated by a time index. To handle either simultaneity or reverse causality, it is recommended that you take a completely different approach to the problem than the one presented in this chapter. DAGs explain causality in terms of counterfactuals. That is, a causal effect is defined as a comparison between two states of the world‚Äîone state that actually happened when some intervention took on some value and another state that didn‚Äôt happen (the ‚Äúcounterfactual‚Äù) under some other intervention. Arrows represent a causal effect between two random variables moving in the intuitive direction of the arrow. The direction of the arrow captures the direction of causality. Causal effects can happen in two ways. They can either be direct (e.g., D -&gt; Y), or they can be mediated by a third variable (e.g., D -&gt; X -&gt; Y). When they are mediated by a third variable, we are capturing a sequence of events originating with , which may or may not be important to you depending on the question you‚Äôre asking. A complete DAG will have all direct causal effects among the variables in the graph as well as all common causes of any pair of variables in the graph. 1.3.1 Confounder Direct causal path: \\(D \\rightarrow Y\\) Backdoor (non-causal) path: \\(X \\rightarrow D\\) and \\(X \\rightarrow Y\\) Key Idea: A backdoor path creates spurious correlation between \\(D\\) (treatment) and \\(Y\\) (outcome) that is driven only by changes in \\(X\\) (the confounder). If we don‚Äôt control for \\(X\\), the correlation between \\(D\\) and \\(Y\\) mixes two sources: The true causal effect of \\(D\\) on \\(Y\\). The spurious association from \\(X\\), which influences both. This leads to omitted variable bias ‚Äî we mistake part of $ X$‚Äôs effect for $ D$‚Äôs effect. Definition: A variable \\(X\\) is a confounder if it jointly affects both \\(D\\) and \\(Y\\), making it harder to isolate the true causal effect. Fix: When \\(X\\) is observed and included in the model, the backdoor path is closed, leaving only the direct causal relationship between \\(D\\) and \\(Y\\). üëâ Think of it this way: Sometimes \\(Y\\) changes because \\(D\\) truly caused it. Other times, \\(Y\\) and \\(D\\) both move because \\(X\\) moved. By controlling for \\(X\\), we separate the causal part from the spurious part. 1.3.2 Collider Direct causal path: \\(D \\rightarrow Y\\) Backdoor path: \\(D \\rightarrow X \\leftarrow Y\\) Key Idea: A collider is a variable influenced by both the treatment \\(D\\) and the outcome \\(Y\\). At \\(X\\), the arrows from \\(D\\) and \\(Y\\) collide. This means the path \\(D \\rightarrow X \\leftarrow Y\\) is blocked by default. So unlike confounders, colliders do not create bias when left alone. Why It Matters: If you do nothing, the backdoor path through a collider is closed ‚Äî safe. But if you control for \\(X\\) (include it in a regression, stratify, etc.), you open the path. This creates a spurious correlation between \\(D\\) and \\(Y\\), introducing collider bias (a.k.a. selection bias). üëâ Rule of Thumb: Control confounders ‚Üí closes harmful backdoor paths. Do not control colliders ‚Üí they‚Äôre already blocking the path. 1.3.3 What To Do and How To Do It What To Do Open backdoor paths introduce omitted variable bias. Sometimes the bias can even flip the sign of the estimated effect. The goal: close all open backdoor paths so the relationship between \\(D\\) (treatment) and \\(Y\\) (outcome) reflects the true causal effect. How To Do It Control for Confounders A confounder jointly affects both \\(D\\) and \\(Y\\), creating an open backdoor path. Close this path by conditioning on the confounder using tools like: Subclassification Matching Regression (include as covariates) Weighting (e.g., inverse probability weights) Leave Colliders Alone A collider is influenced by both \\(D\\) and \\(Y\\). By default, a backdoor path through a collider is closed. Conditioning on a collider opens the path, introducing collider bias (a.k.a. selection bias). Strategy: do not control for colliders. Backdoor Criterion If a variable is a confounder ‚Üí control for it. If a variable is a collider ‚Üí exclude it from your model. Rule of Thumb Always map your variables in a causal diagram (DAG) before modeling. Ask: Does this variable affect both \\(D\\) and \\(Y\\)? ‚Üí confounder, control for it. Ask: Is this variable caused by both \\(D\\) and \\(Y\\)? ‚Üí collider, exclude it. 1.3.4 Example: Sample Selection and collider bias Imagine talent and beauty are independent traits in the general population. However, to become a movie star, you typically need both talent and beauty. The Collider Effect Here, ‚Äúbeing a movie star‚Äù is a collider, because it is influenced by both talent and beauty. When we condition on the collider (i.e., restrict our sample only to movie stars), we inadvertently open a backdoor path between talent and beauty. As a result, talent and beauty appear negatively correlated within the movie-star sample, even though they are independent in the full population. Why This Matters This is an example of sample selection bias: restricting the sample on a collider introduces spurious correlations. A random sample of the full population would correctly show no relationship between talent and beauty. But focusing only on those who ‚Äúpassed through the collider‚Äù (movie stars) creates a false correlation where none exists. ‚úÖ Key Lesson: When analyzing causal relationships, avoid conditioning on variables that act as colliders. Otherwise, you risk fabricating associations that don‚Äôt exist in reality. 1.4 Bad Controls Joshua Angrist (with Guido Imbens and J√∂rn-Steffen Pischke) popularized the idea of ‚Äúbad controls‚Äù in econometrics. These are control variables that should not be included in a regression because they distort rather than clarify the causal effect of interest. 1.4.1 Definition Bad controls are variables that are: Post-treatment (affected by the treatment). Or endogenous (correlated with unobserved factors in the error term). Including them in a model can create spurious relationships and bias causal estimates. 1.4.2 Why They‚Äôre Problematic Post-treatment controls soak up part of the treatment effect (blocking the causal path). Colliders open backdoor paths when conditioned on. Endogenous controls bias estimates because they capture unobserved shocks. üëâ The result: biased and inconsistent causal estimates. 1.4.3 Examples Education ‚Üí Earnings: Controlling for occupation (which is partly determined by education) is a bad control. Treatment ‚Üí Post-treatment earnings: Using post-treatment income in a regression about education biases the effect. Advertising campaign ‚Üí Sales: Controlling for brand awareness (which is influenced by the campaign) is a bad control. 1.4.4 Identifying Good vs.¬†Bad Controls Good controls = pre-treatment confounders: variables that affect both treatment and outcome but are not affected by the treatment. Bad controls = mediators, colliders, or any variable determined by treatment. 1.4.5 Best Practices When choosing controls, ask: Does this variable occur before the treatment? Does it predict both treatment and outcome? (Confounder ‚Üí include.) Could it be affected by treatment? (Mediator ‚Üí exclude.) Is it influenced by both treatment and outcome? (Collider ‚Üí exclude.) Use robustness checks to see if results hinge on questionable controls. 1.4.6 Practical Advice From Mostly Harmless Econometrics: Avoid controlling for outcomes of the treatment. Focus on pre-treatment variables that help isolate causal variation. Always think in terms of the causal diagram (DAG): Is the control blocking a backdoor path or accidentally opening one? ‚úÖ This version highlights: A clear definition. Why bad controls hurt causal inference. Classic and business-style examples. A decision rule (DAG-thinking). 1.4.7 Unobserved Variable Affecting Only the Dependent Variable Sometimes there are unobserved factors that influence only the dependent variable (Y) but not the independent variables (X). This situation is less harmful than when unobservables also affect X, but it still has consequences. 1.4.7.1 No Bias in Coefficients Since the unobserved variable doesn‚Äôt influence X, it does not create correlation between X and the error term. That means no endogeneity problem: OLS estimates of the coefficients on X remain unbiased and consistent. 1.4.7.2 Impact on Error Variance The unobserved factor shows up as ‚Äúextra noise‚Äù in the error term. This increases the variance of the error term, making coefficient estimates less precise. 1.4.7.3 Standard Errors and Precision Larger error variance ‚Üí larger standard errors on coefficient estimates. Consequence: wider confidence intervals and lower statistical power (harder to detect true effects). Practically: even if your estimates are unbiased, you‚Äôre less likely to find them ‚Äústatistically significant.‚Äù 1.4.7.4 Summary ‚úÖ Estimates remain unbiased. ‚ö†Ô∏è But they are less precise. Interpretation: the problem here is inefficiency, not bias. You‚Äôll need larger samples or stronger variation in X to compensate for the added noise. This way, you distinguish bias vs.¬†precision clearly ‚Äî which interviewers love, since many candidates conflate the two. 1.5 Endogeneity Endogeneity arises when an explanatory variable is correlated with the error term in a regression. This breaks the key OLS assumption of independence between regressors and the error term, leading to biased and inconsistent estimates. 1.5.1 Sources of Endogeneity Omitted Variable Bias Leaving out a variable that affects both the independent and dependent variables. Its effect is absorbed into the error term, creating correlation between regressors and the error. Measurement Error If an explanatory variable is measured with error, the ‚Äútrue‚Äù regressor is correlated with the measurement error (which sits in the error term). Simultaneity / Reverse Causality When the independent variable and dependent variable influence each other. Example: advertising spend ‚ÜîÔ∏é sales. 1.5.2 Consequences Biased estimates: Coefficients do not reflect the true causal effect. Inconsistent estimates: Even with large samples, estimates don‚Äôt converge to the true parameter. Threat to causal inference: We can‚Äôt trust the estimated treatment effect. 1.5.3 Solutions Instrumental Variables (IV) Find instruments correlated with the endogenous regressor but uncorrelated with the error term. Implement using Two-Stage Least Squares (2SLS): Stage 1: Regress endogenous regressor on instruments. Stage 2: Use predicted values in the main regression. Fixed Effects / Panel Data Difference out time-invariant unobservables. Example: individual FE in panel regressions. Difference-in-Differences (DiD) as a special case: compares changes over time across treated vs.¬†control units. Control Function Approach Include the residuals from the first-stage regression as an extra regressor to soak up endogeneity. Natural Experiments Use exogenous shocks (policies, disasters, lotteries) that create variation unrelated to the error term. 1.5.4 Summary Endogeneity is essentially about regressors being ‚Äúcontaminated‚Äù by correlation with the error term. If present ‚Üí OLS fails: biased + inconsistent. If addressed (via IV, FE, DiD, etc.) ‚Üí we can recover causal effects. 1.6 Reduced Form Model Reduced Form Models refer to econometric models where the endogenous variables are expressed solely in terms of exogenous variables and error terms. These models simplify the relationship between variables by avoiding the need to specify the underlying structural model, focusing instead on the observed correlations. 1.6.1 Characteristics of Reduced Form Models: Simplified Representation: Reduced form models express endogenous variables directly as functions of exogenous variables and error terms. Focus on Exogeneity: They rely on exogenous variation to identify causal effects, avoiding direct specification of the structural relationships between variables. 1.6.2 Uses of Reduced Form Models: Policy Evaluation: Reduced form models are often used in policy evaluation to estimate the causal impact of policies by leveraging exogenous variation. Instrumental Variables: In IV estimation, the first stage regression (predicting the endogenous variable with instruments) is a reduced form model. Natural Experiments: Reduced form models are frequently used in natural experiments where exogenous shocks provide a source of variation. 1.6.3 Example of a Reduced Form Model: Suppose we want to estimate the impact of education (\\(E\\)) on earnings (\\(Y\\)): Structural Model: \\[ Y = \\alpha + \\beta E + \\epsilon \\] Endogeneity Problem: Education (\\(E\\)) might be endogenous due to omitted variables like ability or family background. Reduced Form Model: Use an instrument \\(Z\\) (e.g., proximity to a college) that affects education but is exogenous with respect to earnings: \\(E = \\pi_0 + \\pi_1 Z + \\nu\\) The reduced form equation for earnings in terms of the instrument: \\(Y = \\gamma_0 + \\gamma_1 Z + \\eta\\) Here, \\(\\gamma_1\\) provides an estimate of the causal effect of $ Z $ on $ Y $, which, under certain conditions, can be used to infer the effect of \\(E\\) on \\(Y\\) through \\(Z\\). In summary, understanding and addressing endogeneity is crucial for accurate causal inference in econometrics. Reduced form models provide a simplified framework to estimate relationships using exogenous variation, often serving as a preliminary step before more complex structural modeling. 1.7 Standard Errors Homoskedasticity Assumption: In linear regression, we assume that the variance of the error term is constant across all levels of the independent variables, i.e., \\(Var(\\epsilon | X) = \\sigma^2\\). Violation: If there is heteroscedasticity (non-constant variance of errors), the OLS estimates remain unbiased, but they are no longer efficient, and the standard errors are biased, leading to unreliable hypothesis tests. Heteroscedasticity-Robust standard errors or Generalized Least Squares (GLS) can be used to address heteroscedasticity. Eiker-Huber-White: Heteroscedasticity-Robust standard errors Cluster-robust standard errors (geographic units) Without homoskedasticity assumption, OLS estimator will still be unbiased but not efficient. Robust standard error usage will not change the OLS estimator but will change the standard errors. Without constant variance, mean squared errors are not minimum anymore. Estimated standard errors are biased. In real life, errors will mostly be heteroskedastic Solution for heteroskedasticity is mostly known as ‚Äòrobust‚Äô standard errors. 1.7.1 heteroskedasticity-consistent standard errors Also known as robust standard errors or The sandwich standard error estimator, is a technique used to obtain valid standard errors in the presence of heteroskedasticity. These standard errors are ‚Äúrobust‚Äù because they do not assume that the error terms have constant variance (homoscedasticity), making them useful for hypothesis testing and confidence intervals when the usual OLS assumptions are violated. 1.7.2 Why Use Sandwich Standard Errors? In OLS regression, if the assumption of homoscedasticity is violated (i.e., the error variance is not constant), the usual standard errors of the estimated coefficients are biased. This bias can lead to incorrect inferences, such as invalid hypothesis tests and confidence intervals. Sandwich standard errors correct for this bias, providing more reliable inference. 1.7.2.1 How It Works The sandwich estimator adjusts the standard errors of the OLS estimates to account for heteroscedasticity. The name ‚Äúsandwich‚Äù comes from the structure of the formula, where the ‚Äúbread‚Äù parts are the matrices that involve the model‚Äôs design matrix, and the ‚Äúmeat‚Äù part is a matrix involving the residuals. 1.7.3 Clustering Standard Errors In the real world, though, you can never assume that errors are independent draws from the same distribution. You need to know how your variables were constructed in the first place in order to choose the correct error structure for calculating your standard errors. If you have aggregate variables, like class size, then you‚Äôll need to cluster at that level. If some treatment occurred at the state level, then you‚Äôll need to cluster at that level. When the units of analysis are clustered into groups and the researcher suspects that the errors are correlated within (but not across) groups, it may be appropriate to employ variance estimators that are robust to the clustered nature of the data. When we cluster standard errors at the state level, we allow for arbitrary serial correlation within state. multi way clustering 1.7.3.1 When Should You Adjust Standard Errors for Clustering? Abadie et al 2022 Formally, clustered standard errors adjust for the correlations induced by sampling the outcome variable from a data-generating process with unobserved cluster- level components. Source The authors argue that there are two reasons for clustering standard errors: 1- a sampling design reason, which arises because you have sampled data from a population using clustered sampling, and want to say something about the broader population; 2- and an experimental design reason, where the assignment mechanism for some causal treatment of interest is clustered. Let me go through each in turn, by way of examples, and end with some of their takeaways. A Sampling Design reason Consider running a simple Mincer earnings regression of the form: Log(wages) = a + byears of schooling + cexperience + d*experience^2 + e You present this model, and are deciding whether to cluster the standard errors. Referee 1 tells you ‚Äúthe wage residual is likely to be correlated within local labor markets, so you should cluster your standard errors by state or village.‚Äù. But referee 2 argues ‚ÄúThe wage residual is likely to be correlated for people working in the same industry, so you should cluster your standard errors by industry‚Äù, and referee 3 argues that ‚Äúthe wage residual is likely to be correlated by age cohort, so you should cluster your standard errors by cohort‚Äù. What should you do? Under the sampling perspective, what matters for clustering is how the sample was selected and whether there are clusters in the population of interest that are not represented in the sample. So, we can imagine different scenarios here: You want to say something about the association between schooling and wages in a particular population, and are using a random sample of workers from this population. Then there is no need to adjust the standard errors for clustering at all, even if clustering would change the standard errors. The sample was selected by randomly sampling 100 towns and villages from within the country, and then randomly sampling people in each; and your goal is to say something about the return to education in the overall population. Here you should cluster standard errors by village, since there are villages in the population of interest beyond those seen in the sample. This same logic makes it clear why you generally wouldn‚Äôt cluster by age cohort (it seems unlikely that we would randomly sample some age cohorts and not others, and then try and say something about all ages); and that we would only want to cluster by industry if the sample was drawn by randomly selecting a sample of industries, and then sampling individuals from within each. Even in the second case, Abadie et al.¬†note that both the usual robust (Eicker-Huber-White or EHW) standard errors, and the clustered standard errors (which they call Liang-Zeger or LZ standard errors) can both be correct, it is just that they are correct for different estimands. That is, if you are content on just saying something about the particular sample of individuals you have, without trying to generalize to the population, the EHW standard errors are all you need; but if you want to say something about the broader population, the LZ standard errors are necessary. The Experimental Design Reason for Clustering The second reason for clustering is the one we are probably more familiar with, which is when clusters of units, rather than individual units, are assigned to a treatment. Let‚Äôs take the same equation as above, but assume that we have a binary treatment that assigns more schooling to people. So now we have: Log(wages) = a +b*Treatment + e Then if the treatment is assigned at the individual level, there is no need to cluster (*). There has been much confusion about this, as Chris Blattman explored in two earlier posts about this issue (the fabulously titled clusterjerk and clusterjerk the sequel), and I still occasionally get referees suggesting I try clustering by industry or something similar in an individually-randomized experiment. This Abadie et al.¬†paper is now finally a good reference to explain why this is not necessary. (*) unless you are using multiple time periods, and then you will want to cluster by individual, since the unit of randomization is individual, and not individual-time period. What about if your treatment is assigned at the village level. Then cluster by village. This is also why you want to cluster difference-in-differences at the state-level when you have a source of variation that comes from differences across states, and why a ‚Äútreatment‚Äù like being on one side of a border vs the other is problematic (because you have only 2 clusters). 1.8 Types of Biases in Econometrics &amp; Statistics Bias occurs when estimates systematically deviate from the truth, threatening validity and reliability. Below are the main types, grouped by theme. 1.8.1 Sample &amp; Selection Biases Selection Bias: Sample is not representative because inclusion depends on outcome or treatment. Example: Studying education and earnings using only employed individuals. Self-Selection Bias: Individuals choose into groups in non-random ways. Example: Motivated people opt into job training, biasing program effects. Attrition Bias: Dropouts differ systematically from those who remain. Example: Only successful dieters remain in a long-term study. Survivorship Bias: Only ‚Äúsurvivors‚Äù are analyzed, failures ignored. Example: Measuring mutual fund returns using only funds still operating. 1.8.2 Specification &amp; Confounding Biases Omitted Variable Bias: Leaving out a confounder that affects both X and Y. Example: Estimating education‚Äôs effect on wages without controlling for ability. Confounding Bias: When the effect of X on Y is mixed with another variable‚Äôs effect. Example: Estimating smoking ‚Üí lung cancer without controlling for age. Endogeneity Bias: More general case where X is correlated with error term (due to omitted variables, measurement error, or simultaneity). 1.8.3 Measurement &amp; Response Biases Measurement Bias: Variables measured incorrectly. Systematic: A scale always adds 2 lbs. Random: Data entry mistakes. Recall Bias: Inaccurate memory of past events. Example: Patients misreport past diet. Response Bias: Participants misreport due to social desirability or misunderstanding. Example: Underreporting alcohol use in surveys. Observer Bias: Researcher expectations influence outcomes. Example: Therapist influences responses when testing a therapy. 1.8.4 Analytical &amp; Reporting Biases Publication Bias: Positive results more likely to be published. Example: Meta-analysis overstates effects because null results stay unpublished. Overfitting Bias: Model fits noise in training data, fails to generalize. Example: Complex regression with too many parameters. Confirmation Bias: Selectively seeking or interpreting evidence consistent with prior beliefs. 1.8.5 Addressing Bias Randomization ‚Üí Prevents selection &amp; confounding bias. Control Groups ‚Üí Benchmark against counterfactual. Instrumental Variables (IV) ‚Üí Correct for endogeneity. Panel Methods / DiD ‚Üí Handle unobserved heterogeneity. Propensity Score Matching (PSM) ‚Üí Balance observed covariates in non-experimental data. Heckman Selection Models ‚Üí Correct for self-selection. Blinding &amp; Survey Design ‚Üí Reduce response and observer bias. Robustness Checks &amp; Sensitivity Analyses ‚Üí Test stability of results. 1.9 Causality Let‚Äôs summarize the main definitions and perspectives so you can see them side by side: 1.9.1 Counterfactual (Potential Outcomes) Definition Core idea: A cause is something that changes the outcome relative to what would have happened otherwise. Formalized by: Rubin Causal Model (Neyman‚ÄìRubin framework). Definition: Treatment \\(T\\) causes outcome \\(Y\\) if \\(Y(1) \\neq Y(0)\\), where \\(Y(1)\\) is the potential outcome if treated, and \\(Y(0)\\) is the potential outcome if untreated. Key Challenge: We never observe both \\(Y(1)\\) and \\(Y(0)\\) for the same individual ‚Üí leads to the fundamental problem of causal inference. 1.9.2 Causal Graphs (Structural Causal Models / DAGs) Core idea: Causes are encoded in the structure of a system of equations or directed acyclic graphs (DAGs). Formalized by: Judea Pearl (Structural Causal Models). Definition: A variable \\(X\\) is a cause of \\(Y\\) if intervening on \\(X\\) (via the ‚Äúdo‚Äù operator, \\(\\text{do}(X=x)\\)) changes the distribution of \\(Y\\). Key Tool: Backdoor criterion, front-door criterion, do-calculus. 1.9.3 Experimental (Interventionist) Definition Core idea: A cause is something that can be manipulated and produces a systematic change in the outcome. Philosophical basis: Interventionist theories (e.g., Woodward). Definition: \\(X\\) causes \\(Y\\) if manipulating \\(X\\) while holding everything else constant changes \\(Y\\). Key Application: Randomized controlled trials (RCTs) embody this definition. 1.9.4 Econometric Definition Core idea: Causes are identified when changes in a regressor can be isolated as exogenous and not confounded. Formalized by: Econometrics tradition (Haavelmo, Angrist &amp; Pischke). Definition: \\(X\\) causes \\(Y\\) if variation in \\(X\\) that is independent of confounders systematically shifts \\(Y\\). Key Tools: IV, DiD, panel fixed effects, RCTs, natural experiments. 1.9.5 Philosophical (Humean / Regularity) Definition Core idea: A cause is something that is regularly followed by an effect. David Hume‚Äôs view: ‚ÄúWe may define a cause to be an object, followed by another, and where all the objects similar to the first are followed by objects similar to the second.‚Äù Limitations: Regularity doesn‚Äôt distinguish correlation from causation. 1.9.6 Granger Causality (Time Series) Core idea: In time series, \\(X\\) Granger-causes \\(Y\\) if past values of \\(X\\) improve predictions of \\(Y\\) beyond past values of \\(Y\\) alone. Definition: \\(X\\) Granger-causes \\(Y\\) if \\(P(Y\\_t | Y\\_{t-1}, X\\_{t-1}) \\neq P(Y\\_t | Y\\_{t-1})\\). Limitation: Not true causality‚Äîpredictive, not structural. 1.9.7 Summary Table Definition Key Idea Main Advocates Limitations Counterfactual Compare \\(Y(1)\\) vs.¬†\\(Y(0)\\) Rubin, Neyman Missing data problem Causal Graphs (SCM) Intervention via ‚Äúdo‚Äù operator Pearl Requires structural assumptions Experimental Manipulation changes outcomes Woodward, RCT tradition Not always feasible Econometric Exogenous variation identifies effects Haavelmo, Angrist &amp; Pischke Depends on valid instruments/design Philosophical Constant conjunction / regularity Hume Doesn‚Äôt separate correlation Granger Causality Predictive precedence in time Clive Granger Predictive, not structural üëâ Bottom line: In econometrics, we mostly rely on counterfactuals + exogenous variation (econometric definition). In statistics, the Rubin model dominates. In computer science/AI, Pearl‚Äôs SCM/DAGs dominate. In time series, Granger causality is used, but cautiously. "],["potential-outcomes.html", "Chapter 2 Potential Outcomes 2.1 Key Concepts 2.2 Assumptions for Identifying Causal Effects 2.3 Methods for Estimating Causal Effects 2.4 Example 2.5 On how parameters are calculated", " Chapter 2 Potential Outcomes The potential outcomes framework, often associated with the Rubin Causal Model (RCM), is a powerful method for defining and estimating causal effects. In this framework, a causal effect is understood through the comparison of potential outcomes under different treatment conditions. 2.1 Key Concepts Potential Outcomes: For each unit (e.g., individual, group, or entity), there are two potential outcomes: \\(Y{_i}^1\\): The outcome if the unit receives the treatment. \\(Y{_i}^0\\): The outcome if the unit does not receive the treatment. These outcomes are also known as counterfactual outcomes because they represent hypothetical scenarios that cannot be simultaneously observed. Observed Outcome: For each unit, we can only observe one of these potential outcomes depending on the treatment assignment. This is expressed using the switching equation: \\[ Y_{i} = D_i \\cdot Y{_i}^1 + (1 - D_i) \\cdot Y{_i}^0 \\] where \\(Y\\) is the observed outcome and \\(D\\) is the treatment indicator (\\(D = 1\\) if the unit receives the treatment and \\(D = 0\\) if the unit does not). 2.1.1 Causal Effect The individual causal effect for a unit \\(i\\) is defined as the difference between its two potential outcomes: \\[ \\text{Causal Effect}_i = Y_i ^1 - Y_i ^0 \\] However, because we can only observe one of these outcomes for each unit, we typically focus on average causal effects across a population. 2.1.2 Average Treatment Effect (ATE) The Average Treatment Effect (ATE) is the expected difference in outcomes if all units were treated versus if none were treated: \\[ \\text{ATE} = E[Y_i ^1] - E[Y_i ^0] \\] 2.1.3 Average Treatment Effect on the Treated (ATT) The Average Treatment Effect on the Treated (ATT) is the average causal effect for those units that actually received the treatment: \\[ \\text{ATT} = E[Y(1) | D = 1] - E[Y(0) | D = 1] \\] 2.1.4 The Fundamental Problem of Causal Inference A major challenge in causal inference is that we can never observe both potential outcomes for the same unit simultaneously. This is known as the fundamental problem of causal inference. Therefore, we rely on assumptions and statistical methods to estimate causal effects. 2.2 Assumptions for Identifying Causal Effects Several assumptions can help identify causal effects: 2.2.1 Independence The independence assumption, also known as the unconfoundedness or ignorability assumption, is crucial in causal inference: \\[[Y^0, Y^1] \\perp D\\] This notation means that the potential outcomes \\((Y^0, Y^1)\\) are independent of the treatment assignment \\(D\\). In other words, the treatment is assigned randomly with respect to the potential outcomes. This ensures that any difference in outcomes between treated and control groups can be attributed to the treatment itself, rather than other factors. It means there are no unobserved confounders. However, in real-world scenarios, human-based sorting and decision-making processes often violate this assumption. People self-select into treatments based on various observed and unobserved characteristics, leading to non-random assignment. As a result, na√Øve observational comparisons‚Äîwhich do not account for this non-randomness‚Äîare almost always incapable of accurately recovering causal effects. To address this issue, researchers use various methods such as randomized controlled trials (RCTs), matching techniques, instrumental variables, and regression adjustment to attempt to approximate random assignment and thus make valid causal inferences. 2.2.1.1 Conditional Independence \\[[Y^0, Y^1] \\perp D \\mid X \\] This assumption implies that conditional on covariates \\(X\\), the treatment assignment \\(D\\) is independent of the potential outcomes. Treatment can be assigned conditionally on covariates. For example state assign student to three classes randomly but schools chosen first, then students are assigned randomly later. The treatment assignment was only conditionally random. When treatment assignment had been conditional on observable variables, it is a situation of selection on observables. 2.2.2 Stable Unit Treatment Value Assumption (SUTVA) SUTVA is a critical assumption in causal inference and has two main components: No Interference: The potential outcomes for any unit are unaffected by the treatment status of other units. This means the treatment effect on one unit does not spill over to affect another unit. Consistency: The observed outcome for a unit under the treatment received is the same as the potential outcome under that treatment. This means that if a unit receives the treatment, its observed outcome should match the potential outcome we would expect if it had received that treatment. Implications of SUTVA Homogeneous Treatment: SUTVA implies that the treatment is administered uniformly across all units. In practice, this assumption can be violated if, for instance, the effectiveness of a treatment varies due to differences in how it is delivered. For example, if some doctors are better surgeons than others, the ‚Äúdose‚Äù of the treatment (surgery) is not homogeneous. No Externalities (No Spillovers): SUTVA assumes there are no externalities, meaning that the treatment of one unit does not affect the outcomes of other units. If unit 1 receives the treatment and this somehow affects unit 2‚Äôs outcome, this would be a violation of SUTVA. We are assuming away such spillover effects to ensure that the treatment effect can be isolated and accurately measured. No general equilibrium effects Violations of SUTVA can lead to biased estimates of causal effects, so it is essential to consider these assumptions carefully and take appropriate steps to address potential violations when conducting causal inference. 2.3 Methods for Estimating Causal Effects Several methods can be used to estimate causal effects under the potential outcomes framework: Randomized Controlled Trials (RCTs): Random assignment ensures that the treatment and control groups are comparable, allowing for unbiased estimation of the Average Treatment Effect (ATE). This is often considered the gold standard for causal inference. Matching: Pairing treated and control units with similar covariates to estimate the treatment effect. This method attempts to simulate a randomized experiment by creating a sample of units that received the treatment and a comparable sample that did not. Regression Adjustment: Using regression models to adjust for differences in covariates between treated and control groups. This method helps control for confounding variables by including them in the regression model to isolate the treatment effect. Instrumental Variables (IV): Using instruments that affect the treatment assignment but are not related to the potential outcomes, except through the treatment. This method is useful when there is concern about endogeneity or unobserved confounding variables. Difference-in-Differences (DiD): Comparing the changes in outcomes over time between treated and control groups to account for time-invariant unobserved heterogeneity. This method is useful for evaluating the effect of a treatment or intervention that is implemented at a specific point in time. Regression Discontinuity (RD): Exploiting a cutoff or threshold in the assignment of treatment to estimate the causal effect. Units just above and below the cutoff are assumed to be comparable, allowing for a local estimation of the treatment effect. Synthetic Control Method: Constructing a weighted combination of control units to create a synthetic control group that approximates the characteristics of the treated group. This method is particularly useful for case studies and evaluating the impact of interventions in a single treated unit. These methods provide a robust toolkit for estimating causal effects and addressing various challenges in observational data analysis. 2.4 Example Let‚Äôs consider an example to illustrate the potential outcomes framework: Scenario: We want to estimate the effect of a job training program (treatment) on participants‚Äô earnings. Potential Outcomes: \\(Y_i(1)\\): Earnings of individual \\(i\\) if they participate in the job training program. \\(Y_i(0)\\): Earnings of individual \\(i\\) if they do not participate in the job training program. Observed Outcome: If individual \\(i\\) participates in the program (\\(D_i = 1\\)), we observe \\(Y_i = Y_i(1)\\). If individual \\(i\\) does not participate (\\(D_i = 0\\)), we observe \\(Y_i = Y_i(0)\\). Objective: Estimate the ATE of the job training program on earnings: \\[ \\text{ATE} = E[Y(1)] - E[Y(0)] \\] In practice, we might use matching or regression adjustment to control for covariates that affect both participation in the program and earnings, helping us to estimate the causal effect more accurately. 2.4.1 Simple Difference Method The simple difference method is one of the basic approaches to estimating causal effects in observational studies. It compares the average outcomes of a treatment group and a control group. This method is straightforward but relies on the assumption that the two groups are comparable in all respects except for the treatment. 2.4.1.1 Key Concepts Treatment Group: The group that receives the treatment or intervention. Control Group: The group that does not receive the treatment or intervention. 2.4.1.2 Steps to Implement the Simple Difference Method Identify Treatment and Control Groups: Define the groups that have received the treatment (treatment group) and those that have not (control group). Calculate Average Outcomes: Compute the average outcome for the treatment group (\\(\\bar{Y}_T\\)). Compute the average outcome for the control group (\\(\\bar{Y}_C\\)). Compute the Difference: The estimated treatment effect is the difference between the average outcomes of the treatment and control groups: \\[\\hat{\\delta} = \\bar{Y}_T - \\bar{Y}_C \\] 2.4.1.3 Assumptions The simple difference method assumes that the treatment and control groups are comparable, meaning that any difference in outcomes is solely due to the treatment. This assumption is often referred to as the strong ignorability assumption. No Confounding Variables: There are no unobserved factors that influence both the treatment assignment and the outcome. Homogeneity: The treatment effect is constant across all individuals in the population. 2.4.1.4 Limitations Selection Bias: If individuals self-select into the treatment group based on characteristics that also affect the outcome, the estimate will be biased. Confounding Variables: If there are unobserved confounders that affect both the treatment and the outcome, the simple difference method will not provide a valid estimate of the causal effect. 2.4.1.5 Example Let‚Äôs illustrate the simple difference method with an example. Scenario: We want to estimate the effect of a job training program on participants‚Äô earnings. Data: Treatment group: Participants of the job training program. Control group: Non-participants of the job training program. Outcome: Earnings after the program. Average Outcomes: Average earnings for the treatment group (\\(\\bar{Y}_T\\)): $50,000 Average earnings for the control group (\\(\\bar{Y}_C\\)): $45,000 Compute the Difference: The estimated treatment effect: $ = {Y}_T - {Y}_C = 50,000 - 45,000 = 5,000 $ Interpretation: The job training program is estimated to increase earnings by $5,000 on average. 2.4.1.6 Addressing Limitations To address the limitations of the simple difference method, researchers can use more sophisticated techniques that control for confounding variables and selection bias: Randomized Controlled Trials (RCTs): Random assignment of treatment can ensure comparability between treatment and control groups. Matching Methods: Match treatment and control units based on observed covariates to create comparable groups. Regression Adjustment: Use regression models to control for observed covariates that may confound the relationship between treatment and outcome. Instrumental Variables (IV): Use instruments that are correlated with the treatment but not directly with the outcome to account for unobserved confounders. Difference-in-Differences (DiD): Compare changes in outcomes over time between treatment and control groups to account fortime-invariant unobserved heterogeneity. 2.4.2 Conclusion The simple difference method provides an intuitive way to estimate causal effects by comparing the average outcomes of treatment and control groups. However, its validity relies on the strong assumption that the groups are comparable in all respects except for the treatment. In practice, researchers often need to use more advanced techniques to address potential biases and confounding factors. 2.5 On how parameters are calculated 2.5.1 Propensity Score Matching (PSM) and Maximum Likelihood Estimation (MLE) Does PSM use Maximum Likelihood Estimation (MLE)? Yes, PSM typically uses logistic regression (or probit regression) to estimate propensity scores, and logistic regression uses MLE to estimate coefficients. Does every logistic regression use MLE? Yes, logistic regression commonly uses MLE to estimate the model parameters. 2.5.2 Logistic Regression 2.5.2.1 Objective Function and Loss Function Objective Function: The objective in logistic regression is to maximize the likelihood function, i.e., the probability of observing the given sample. Loss Function: The log-likelihood function is used as the loss function in logistic regression, which is minimized (or equivalently, the negative log-likelihood is maximized). 2.5.2.2 Calculating Coefficients Coefficients in logistic regression are estimated using MLE. The likelihood function for logistic regression is: \\[ L(\\beta) = \\prod_{i=1}^n P(y_i|\\mathbf{x}_i;\\beta)^{y_i}(1 - P(y_i|\\mathbf{x}_i;\\beta))^{1 - y_i} \\] where \\(P(y_i|\\mathbf{x}_i;\\beta) = \\frac{1}{1 + \\exp(-\\mathbf{x}_i^T \\beta)}\\). The log-likelihood function is: \\[ \\log L(\\beta) = \\sum_{i=1}^n \\left[ y_i \\log P(y_i|\\mathbf{x}_i;\\beta) + (1 - y_i) \\log (1 - P(y_i|\\mathbf{x}_i;\\beta)) \\right] \\] The parameters \\(\\beta\\) are estimated by maximizing this log-likelihood function. 2.5.2.3 Hypothesis Testing Z-test: Logistic regression typically uses z-tests to test hypotheses about the coefficients. Null hypothesis: The coefficient is equal to zero. The z-statistic is calculated as the coefficient estimate divided by its standard error: \\(z = \\frac{\\hat{\\beta}}{\\text{SE}(\\hat{\\beta})}\\) The p-value is derived from the standard normal distribution. 2.5.3 Ordinary Least Squares (OLS) Regression 2.5.3.1 Objective Function and Loss Function Objective Function: The objective in OLS regression is to minimize the sum of squared residuals. Loss Function: The loss function in OLS is the residual sum of squares (RSS): \\(RSS = \\sum_{i=1}^n (y_i - \\mathbf{x}_i^T \\beta)^2\\) 2.5.3.2 Calculating Coefficients Coefficients in OLS regression are estimated by minimizing the RSS. The normal equations derived from setting the gradient of RSS to zero are: \\(\\mathbf{\\hat{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\\) where \\(\\mathbf{X}\\) is the design matrix of predictors and \\(\\mathbf{y}\\) is the vector of observed outcomes. 2.5.3.3 Hypothesis Testing T-test: OLS regression typically uses t-tests to test hypotheses about the coefficients. Null hypothesis: The coefficient is equal to zero. The t-statistic is calculated as the coefficient estimate divided by its standard error: \\(t = \\frac{\\hat{\\beta}}{\\text{SE}(\\hat{\\beta})}\\) The p-value is derived from the t-distribution with \\(n - p - 1\\) degrees of freedom (where \\(p\\) is the number of predictors). "],["matching.html", "Chapter 3 Matching 3.1 Subclassification 3.2 Exact Matching 3.3 Approximate Matching Methods 3.4 Propensity Score Methods 3.5 Inverse Probability Weighting (Weighting on the propensity score) 3.6 Nearest-neighbor matching 3.7 Coarsened Exact Matching 3.8 A/B Test article from Medium 3.9 Task 1: Load the data 3.10 Task 2: Set up Hypothesis 3.11 Task 3: Compute the difference in the click-through rate", " Chapter 3 Matching 3.1 Subclassification Subclassification is a method used to satisfy the backdoor criterion by adjusting differences in means with strata-specific weights. These weights ensure that the distribution of means by strata matches the counterfactual‚Äôs strata distribution. This method addresses the problem when treatment assignment is random, conditional on observables. The assumption in mathematical notation is: \\[ [Y^0, Y^1] \\perp D \\mid X \\] This implies that, given covariates \\(X\\), the treatment assignment \\(D\\) is independent of the potential outcomes (random). Treatment can be assigned conditionally on covariates. For example, a state might assign students to three classes randomly, but first, schools are chosen, and then students are assigned randomly within those schools. 3.1.1 Example Consider a study investigating the impact of cigar type on mortality. Without considering age, it appears that cigar or pipe users have higher mortality rates, which is controversial. However, age is a crucial factor that influences both cigar type selection (treatment) and mortality rate (outcome), making it a confounder. In this method, my strategy for addressing covariate imbalance is to condition on age, ensuring that the age distribution is comparable between the treatment and control groups. 3.1.2 Step-by-Step Example Bin Age into Groups: Create age groups (e.g., 18-30, 31-45, 46-60, 61+). Calculate Percent Distribution: Determine the percentage of individuals in each age group for both treatment and control groups. Weighted Mortality Rate: Calculate the mortality rate for each age group within each treatment group. Use the age group percentages to calculate a weighted average mortality rate for each treatment group. 3.1.2.1 Implementation Suppose we have the following data: Age Group Treatment Control Treatment Deaths Control Deaths Treatment Total Control Total 18-30 20% 25% 10 15 100 150 31-45 30% 20% 20 10 150 100 46-60 25% 30% 30 20 125 150 61+ 25% 25% 40 30 125 125 Calculate Age-Specific Mortality Rates: \\[ \\text{Mortality Rate (18-30)} = \\frac{10}{100} = 10\\%, \\quad \\text{Control Mortality Rate (18-30)} = \\frac{15}{150} = 10\\% \\] (Repeat for other age groups.) Calculate Weighted Mortality Rates: \\[ \\text{Weighted Mortality Rate (Treatment)} = 0.20 \\times 10\\% + 0.30 \\times 13.3\\% + 0.25 \\times 24\\% + 0.25 \\times 32\\% \\] \\[ \\text{Weighted Mortality Rate (Control)} = 0.25 \\times 10\\% + 0.20 \\times 10\\% + 0.30 \\times 13.3\\% + 0.25 \\times 24\\% \\] 3.1.3 Considerations Choice of Variables: Deciding which variables to use for adjustment can be challenging. Including too many variables can lead to the ‚Äúcurse of dimensionality,‚Äù where the data becomes too sparse in higher dimensions. Common Support Assumption: This assumption requires that for each stratum, there exist observations in both the treatment and control groups. If the sample size is small, this assumption may be violated, making it difficult to compare groups effectively. By carefully choosing variables and ensuring sufficient sample sizes within strata, subclassification can effectively adjust for covariate imbalances and yield more accurate estimates of treatment effects. 3.2 Exact Matching Exact matching is a method used to estimate causal effects by pairing units in the treatment group with units in the control group that have identical values for certain covariates. This method helps in comparing the outcomes of similar units under different treatments to infer causal effects. Why? Because independence assumption is violated, and treatment assignment is not random. 3.2.1 Explanation Suppose we have a treatment group and a control group, and we want to estimate the treatment effect by finding exact matches based on a covariate. Matching Process: If a unit in the treatment group has a covariate value of 2, we look for a unit in the control group with the same covariate value of 2. If we find such a match, we use the outcome of the control unit to impute the counterfactual outcome for the treatment unit. Handling Multiple Matches: If there are multiple control units with the same covariate value as the treatment unit, we take the average of those control units‚Äô outcomes to impute the counterfactual for the treatment unit. Calculating the Average Treatment Effect (ATE): By imputing counterfactual outcomes for each unit in both the control and treatment groups based on matching covariates, we can calculate the ATE. Calculating the Average Treatment Effect on the Treated (ATT): Typically, we find exact matching control units for treatment units and calculate the ATT. This involves comparing only the matched pairs. In a typical example, control group is much larger than treatment group and it is much easier to find similar treated units within a larger control unit. 3.2.2 Example Let‚Äôs consider an example where we are studying the effect of a new teaching method on student performance. We have two groups: students who received the new teaching method (treatment group) and students who received the traditional method (control group). We will use the exact matching method based on a covariate, such as prior test scores. 3.2.2.1 Step-by-Step Process Identify Covariate: Prior test scores are used as the matching covariate. Exact Matching: Suppose a student in the treatment group has a prior test score of 85. We look for students in the control group with a prior test score of 85. If we find multiple students in the control group with a prior test score of 85, we average their outcomes. Impute Counterfactuals: For the treatment student with a prior test score of 85, we use the average outcome of the matched control students to impute the counterfactual outcome. Create Matched Sample: The matched sample consists of pairs of treatment and control units with the same covariate value. For example, if we have another treatment student with a prior test score of 90, we find control students with a prior test score of 90 and repeat the process. Calculate ATT: For each matched pair, we calculate the difference in outcomes. Average these differences to obtain the ATT. 3.2.2.2 Example Data Student Group Prior Test Score Outcome (Final Score) A Treatment 85 90 B Treatment 90 88 C Control 85 85 D Control 90 86 E Control 85 87 For Student A (Treatment, 85): Match with Students C and E (Control, 85). Average outcome: (85 + 87) / 2 = 86. Imputed counterfactual for A: 86. For Student B (Treatment, 90): Match with Student D (Control, 90). Imputed counterfactual for B: 86. 3.2.2.3 ATT Calculation Difference for Student A: 90 - 86 = 4 Difference for Student B: 88 - 86 = 2 \\[ \\text{ATT} = \\frac{4 + 2}{2} = 3 \\] 3.2.3 Conclusion Exact matching helps in creating a comparable control group for each treatment unit based on covariates. By doing so, we can more accurately estimate the causal effect of the treatment. However, this method requires sufficient overlap between the covariate distributions of the treatment and control groups, and the common support assumption must be satisfied. 3.3 Approximate Matching Methods When you have multiple covariates to match or do not have exact matches, you can use approximate matching methods to find the best possible matches. 3.3.1 Nearest Neighbor Covariate Matching When the number of matching covariates exceeds one, we need a new definition of distance to measure closeness between units. Multiple covariates not only introduce the curse-of-dimensionality problem but also complicate the measurement of distance. This poses challenges for finding a good match in the data and demands a large sample size for the matching discrepancies to be trivially small. 3.3.1.1 Euclidean Distance Definition: Euclidean distance is a common measure of distance between two points in a multi-dimensional space. Problem: The distance measure depends on the scale of the variables, which can distort the true closeness between points. 3.3.1.2 Normalized Euclidean Distance Definition: This is the Euclidean distance normalized by the variance of the variables. Advantage: Normalizing by variance adjusts for differences in scale among the covariates, making the distance measure more accurate. 3.3.1.3 Mahalanobis Distance Definition: Mahalanobis distance is a scale-invariant distance metric that takes into account the correlations between variables. Advantage: It adjusts for the scale and correlations of the covariates, providing a more accurate measure of distance. 3.3.2 Example Suppose we are studying the impact of a job training program on employment outcomes. We have multiple covariates, such as age, education level, and prior work experience. We want to use approximate matching to find control units that are similar to the treatment units based on these covariates. 3.3.2.1 Step-by-Step Process Identify Covariates: Age Education Level Prior Work Experience Calculate Distances: Euclidean Distance: \\[\\text{Distance} = \\sqrt{(X_1 - Y_1)^2 + (X_2 - Y_2)^2 + \\cdots + (X_n - Y_n)^2}\\] Normalized Euclidean Distance: \\[\\text{Distance} = \\sqrt{\\left(\\frac{X_1 - Y_1}{\\sigma_1}\\right)^2 + \\left(\\frac{X_2 - Y_2}{\\sigma_2}\\right)^2 + \\cdots + \\left(\\frac{X_n - Y_n}{\\sigma_n}\\right)^2}\\] Mahalanobis Distance: \\[\\text{Distance} = \\sqrt{(X - Y)^T S^{-1} (X - Y)}\\] where \\(S\\) is the covariance matrix of the covariates. Find Nearest Neighbors: For each treatment unit, calculate the distance to all control units using the chosen distance metric. Select the control unit with the smallest distance as the match for the treatment unit. Calculate Treatment Effect: Compare the outcomes of matched pairs to estimate the treatment effect. 3.3.3 Hypothetical Data Unit Group Age Education Level Prior Work Experience Outcome 1 Treatment 25 Bachelor‚Äôs 2 years Employed 2 Treatment 30 Master‚Äôs 5 years Employed 3 Control 26 Bachelor‚Äôs 1 year Unemployed 4 Control 29 Master‚Äôs 6 years Employed 3.3.3.1 Matching Process Calculate Normalized Euclidean Distances: Normalize the covariates by their variances. Compute distances between each treatment unit and all control units. Match Units: Match Unit 1 (Treatment) with Unit 3 (Control) based on the smallest normalized Euclidean distance. Match Unit 2 (Treatment) with Unit 4 (Control) based on the smallest normalized Euclidean distance. Estimate Treatment Effect: Compare outcomes of matched pairs: Unit 1 (Treatment) vs.¬†Unit 3 (Control): Employed vs.¬†Unemployed Unit 2 (Treatment) vs.¬†Unit 4 (Control): Employed vs.¬†Employed By using approximate matching methods like nearest neighbor matching with normalized Euclidean or Mahalanobis distances, we can more accurately estimate the treatment effect even when dealing with multiple covariates and the lack of exact matches. 3.4 Propensity Score Methods Propensity score methods are approximate matching techniques that use propensity scores as distance metrics. These methods offer several ways to perform matching based on propensity scores. Propensity score matching (PSM) is a widely used method, particularly in medical sciences, for addressing selection on observables. However, it has not been as widely adopted among economists as other non-experimental methods like regression discontinuity or difference-in-differences. This reluctance is largely due to skepticism about the conditional independence assumption (CIA) being achievable in any dataset. Economists are often more concerned about selection on unobservables than selection on observables, making them less likely to use matching methods. 3.4.1 Concept Propensity score matching is used when a conditioning strategy can satisfy the backdoor criterion. The method involves estimating a model (usually logit or probit) to predict the conditional probability of treatment based on covariates. The predicted values from this estimation, called propensity scores, collapse the covariates into a single scalar. Comparisons between the treatment and control groups are then based on these propensity scores. 3.4.2 Steps Estimate Propensity Scores: Use a logit or probit model to estimate the probability of receiving the treatment based on observed covariates. Match Units: Match treatment units with control units that have similar propensity scores. Assess Overlap: Ensure that there is common support, meaning there are units in both treatment and control groups across the range of propensity scores. Calculate Treatment Effect: Compare outcomes between matched treatment and control units to estimate the treatment effect. 3.4.3 Example Suppose we are studying the effect of a job training program on employment outcomes. We have the following covariates: age, education level, and prior work experience. We will use propensity score matching to estimate the effect of the program. 3.4.3.1 Step-by-Step Process Estimate Propensity Scores: Fit a logit model to predict the probability of receiving the job training based on age, education level, and prior work experience. Example logit model: \\[P(Treatment) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\cdot Age + \\beta_2 \\cdot Education + \\beta_3 \\cdot Experience)}} \\] Calculate Propensity Scores: Use the fitted logit model to calculate the propensity score for each unit. Match Units: Match each treatment unit with one or more control units that have similar propensity scores. Example matching method: Nearest neighbor matching. NN matching is greedy in the sense that each pairing occurs without reference to how other units will be or have been paired, and therefore does not aim to optimize any criterion. Nearest neighbor matching is the most common form of matching used. For large datasets (i.e., in 10,000s to millions), some matching methods will be too slow to be used at scale. Instead, users should consider generalized full matching, subclassification, or coarsened exact matching, which are all very fast and designed to work with large datasets. (Nice article on MatchIt)[https://cran.r-project.org/web/packages/MatchIt/vignettes/matching-methods.html] Assess Overlap: Check for common support to ensure there is overlap in propensity scores between the treatment and control groups. If there is no overlap, adjust the model or consider other methods. Calculate Treatment Effect: Compare employment outcomes between matched treatment and control units. Calculate the average treatment effect on the treated (ATT). 3.4.3.2 Example Data Unit Group Age Education Level Prior Work Experience Outcome Propensity Score 1 Treatment 25 Bachelor‚Äôs 2 years Employed 0.70 2 Treatment 30 Master‚Äôs 5 years Employed 0.85 3 Control 26 Bachelor‚Äôs 1 year Unemployed 0.65 4 Control 29 Master‚Äôs 6 years Employed 0.80 Match Units: Match Unit 1 (Treatment) with Unit 3 (Control) based on similar propensity scores (0.70 vs.¬†0.65). Match Unit 2 (Treatment) with Unit 4 (Control) based on similar propensity scores (0.85 vs.¬†0.80). Not very intuitive or even confusing: There are many ways to use PS for matching Calculate ATT: Compare outcomes of matched pairs: - Unit 1 (Treatment) vs.¬†Unit 3 (Control): Employed vs.¬†Unemployed - Unit 2 (Treatment) vs.¬†Unit 4 (Control): Employed vs.¬†Employed 3.4.4 Assumptions and Considerations Conditional Independence Assumption (CIA): Treatment assignment is independent of potential outcomes given the observed covariates. This assumption is crucial for PSM to provide unbiased estimates. Common Support: There must be overlap in the distribution of propensity scores between the treatment and control groups. Lack of common support can lead to biased estimates as some treatment units may have no comparable control units. Model Specification: The logit or probit model must be correctly specified to accurately estimate propensity scores. Including relevant covariates and interactions is important for achieving balance between groups. Sample Size: PSM requires a sufficiently large sample size to find good matches for each treatment unit. Smaller samples may lead to poor matches and biased estimates. Propensity score matching is a powerful tool for estimating causal effects in observational studies, but it relies heavily on the assumptions of CIA and common support. Proper model specification and adequate sample size are essential for obtaining reliable estimates. 3.5 Inverse Probability Weighting (Weighting on the propensity score) It is weighting treatment and control units according to, which is causing units with very small values of the propensity score to blow up and become unusually influential in the calculation of ATT. Thus, we will need to trim the data. A good rule of thumb, they note, is to keep only observations on the interval [0.1,0.9], which was performed at the end of the program. We still need to calculate standard errors, such as based on a bootstrapping method, The sensitivity of inverse probability weighting to extreme values of the propensity score has led some researchers to propose an alternative that can handle extremes a bit better. Most software packages have programs that will estimate the sample analog of these inverse probability weighted parameters that use the second method with normalized weights. 3.6 Nearest-neighbor matching An alternative, very popular approach to inverse probability weighting is matching on the propensity score. This is often done by finding a couple of units with comparable propensity scores from the control unit donor pool within some ad hoc chosen radius distance of the treated unit‚Äôs own propensity score. The researcher then averages the outcomes and then assigns that average as an imputation to the original treated unit as a proxy for the potential outcome under counterfactual control. Then effort is made to enforce common support through trimming. Nevertheless, nearest-neighbor matching, along with inverse probability weighting, is perhaps the most common method for estimating a propensity score model. Nearest-neighbor matching using the propensity score pairs each treatment unit with one or more comparable control group units , where comparability is measured in terms of distance to the nearest propensity score. This control group unit‚Äôs outcome is then plugged into a matched sample. Once we have the matched sample, we can calculate the ATT as We will focus on the ATT because of the problems with overlap that we discussed earlier. We can chose to match using K nearest neighbors. Nearest neighbors, in other words, will find the K nearest units in the control group, where ‚Äúnearest‚Äù is measured as closest on the propensity score itself. Unlike covariate matching, distance here is straightforward because of the dimension reduction afforded by the propensity score. We then average actual outcome, and match that average outcome to each treatment unit. Once we have that, we subtract each unit‚Äôs matched control from its treatment value, and then divide by N, the number of treatment units. 3.6.1 Example in R library(MatchIt) library(Zelig) m_out &lt;- matchit(treat ~ age + agesq + agecube + educ + educsq + marr + nodegree + black + hisp + re74 + re75 + u74 + u75 + interaction1, data = nsw_dw_cpscontrol, method = &quot;nearest&quot;, distance = &quot;logit&quot;, ratio =5) m_data &lt;- match.data(m_out) z_out &lt;- zelig(re78 ~ treat + age + agesq + agecube + educ + educsq + marr + nodegree + black + hisp + re74 + re75 + u74 + u75 + interaction1, model = &quot;ls&quot;, data = m_data) x_out &lt;- setx(z_out, treat = 0) x1_out &lt;- setx(z_out, treat = 1) s_out &lt;- sim(z_out, x = x_out, x1 = x1_out) summary(s_out) 3.7 Coarsened Exact Matching Coarsened Exact Matching (CEM) is a method based on the idea that exact matching can often be achieved by coarsening the data. Coarsening involves creating categorical variables (e.g., 0- to 10-year-olds, 11- to 20-year-olds), making it easier to find exact matches. Once exact matches are found, weights are calculated based on where a person fits within certain strata, and these weights are used in a simple weighted regression. This method can be implemented using the MatchIt library in R. 3.7.1 Example Consider the variable ‚Äúschooling,‚Äù which can be categorized as: - Less than high school - High school only - Some college - College graduate - Post-college Each observation is placed into one of these categories, creating strata for each unique combination of observations. Assign these strata to the original (uncoarsened) data, and drop any observation whose stratum doesn‚Äôt contain at least one treated and one control unit. Then, add weights based on stratum size and analyze the data without further matching. 3.7.2 Steps in Coarsened Exact Matching Coarsen the Data: Transform continuous or detailed categorical variables into coarser categories. Example: Age groups (0-10, 11-20, etc.) or education levels (less than high school, high school, etc.). Create Strata: For each unique combination of the coarsened variables, create a stratum. Each observation is assigned to a stratum based on its coarsened characteristics. Assign Weights: Calculate weights for each observation based on the stratum size. Observations in larger strata receive smaller weights and vice versa. Drop Unmatched Observations: Remove any strata that do not contain both treated and control units. Weighted Regression: Use the weights in a regression analysis to account for the matching process. 3.7.3 Considerations Balance: Coarsening can improve balance between treated and control groups but may result in some loss of information. Weight Calculation: Weights should reflect the relative sizes of the strata to ensure accurate representation in the analysis. Implementation: Ensure the coarsening process does not oversimplify the data, potentially masking important variations. By carefully coarsening the data and using appropriate weights, CEM allows for more accurate and reliable estimation of treatment effects, even when exact matching on the original variables is not feasible. 3.8 A/B Test article from Medium 3.8.1 Example: Conversion Rate of an E-Commerce Website Article Source Suppose an e-commerce website wants to test if implementing a new feature (e.g., layout or button) will significantly improve conversion rate. conversion rate: number of purchases divided by number of sessions/visits We can randomly show the new webpage to 50% of the users. Then, we have a test group and a control group. Once we have enough data points, we can test if the conversion rate in the treatment group is significantly higher (one side test) than that in the control group. The null hypothesis is that conversion rates are not significantly different in the two group. Sample Size for Comparing Two Means. One way to perform the test is to calculate daily conversion rates for both the treatment and the control groups. Since the conversion rate in a group on a certain day represents a single data point, the sample size is actually the number of days. Thus, we will be testing the difference between the mean of daily conversion rates in each group across the testing period. The formula for estimate minimum sample size is as follows: Sample Size Estimate for A/B Test In an A/B test, the sample size (\\(n\\)) required for each group can be estimated using the formula: \\[n = \\frac{{2 \\cdot (Z_{\\alpha/2} + Z_{\\beta})^2 \\cdot \\sigma^2}}{{\\delta^2}}\\] where: $ n : $ $ Z_{/2} : $ $ Z_{} : $ $ ^2 : $ $ : $ This formula helps in determining the sample size needed to achieve desired levels of significance and power in an A/B test. For our example, let‚Äôs assume that the mean daily conversion rate for the past 6 months is 0.15 and the sample standard deviation is 0.05. With the new feature, we expect to see a 3% absolute increase in conversion rate. Thus, for the conversion rate for the treatment group will be 0.18. We also assume that the sample standard deviations are the same for the two group. Our parameters are as follows. \\(\\mu_1 = 0.15\\) \\(\\mu_2 = 0.18\\) \\(\\sigma_1 = \\sigma_2 = 0.05\\) Assuming \\(\\alpha = 0.05\\) and \\(\\beta = 0.20\\) (\\(power = 0.80\\)), applying the formula, the required minimum sample size is 35 days. This is consistent with the result from this web calculator. Sample Size for Comparing Two Proportions The two-means approach considers each day+group as a data point. But what if we focus on individual users and visits? What if we want to know how many visits/sessions are required for the testing? In this case, the conversion rate for a group is basically all purchases divided by all sessions in that group. If each session is a Bernoulli trial (convert or not), each group follows a binomial distribution. To test the difference in conversion rate between the treatment and control groups, we need a test of two proportions. The formula for estimating the minimum required sample size is as follows. Summary: Sample Size Estimate for Comparing Proportions When comparing proportions in two independent groups, the sample size (\\(n\\)) required for each group can be estimated using the formula: \\[n = \\frac{{2 \\cdot (Z_{\\alpha/2} + Z_{\\beta})^2 \\cdot (p(1-p))}}{{\\delta^2}}\\] where: \\(n : \\text{ Sample size per group}\\) \\(Z_{\\alpha/2} : \\text{ Critical value for significance level}\\) \\(Z_{\\beta} : \\text{ Critical value for desired power}\\) \\(p : \\text{ Expected proportion in one group}\\) \\(\\delta : \\text{ Minimum detectable difference in proportions}\\) This formula helps in determining the sample size needed to detect a specified difference in proportions between two groups with desired levels of significance and power. Assuming 50‚Äì50 split, we have the following parameters: \\(p_1 = 0.15\\) \\(p_2 = 0.18\\) \\(k = 1\\) Using \\(\\alpha = 0.05\\) and \\(\\beta = 0.20\\), applying the formula, the required sample size is \\(1,892\\) sessions per group. 3.8.2 Example: A/B Test A/B testing is an experiment where two or more variants are evaluated using statistical analysis to determine which variation performs better for a given conversion goal. A/B testing is widely used by digital marketing agencies as it is the most effective method to determine the best content for converting visits into sign-ups and purchases. In this scenario, you will set up hypothesis testing to advise a digital marketing agency on whether to adopt a new ad they designed for their client. Assume you are hired by a digital marketing agency to conduct an A/B test on a new ad hosted on a website. Your task is to determine whether the new ad outperforms the existing one. The agency has run an experiment where one group of users was shown the new ad design, while another group was shown the old ad design. The users‚Äô interactions with the ads were observed, specifically whether they clicked on the ad or not. 3.9 Task 1: Load the data In this task, we will import our libraries and then load our dataset library(tidyverse) ## ‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ ## ‚úî dplyr 1.1.2 ‚úî readr 2.1.4 ## ‚úî forcats 1.0.0 ‚úî stringr 1.5.0 ## ‚úî ggplot2 3.4.4 ‚úî tibble 3.2.1 ## ‚úî lubridate 1.9.3 ‚úî tidyr 1.3.0 ## ‚úî purrr 1.0.1 ## ‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ ## ‚úñ dplyr::filter() masks stats::filter() ## ‚úñ dplyr::lag() masks stats::lag() ## ‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors library(readxl) df &lt;- read_excel(&#39;/Users/deayan/Desktop/GITHUB/10_Causal_Notes/__repo/data/AB_Test.xlsx&#39;) glimpse(df) ## Rows: 3,757 ## Columns: 2 ## $ group &lt;chr&gt; &quot;experiment&quot;, &quot;control&quot;, &quot;control&quot;, &quot;control&quot;, &quot;control&quot;, &quot;cont‚Ä¶ ## $ action &lt;chr&gt; &quot;view&quot;, &quot;view&quot;, &quot;view and click&quot;, &quot;view and click&quot;, &quot;view&quot;, &quot;vi‚Ä¶ df%&gt;% group_by(group, action)%&gt;% summarise(n = n(), .groups = &quot;drop&quot;) ## # A tibble: 4 √ó 3 ## group action n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 control view 1513 ## 2 control view and click 363 ## 3 experiment view 1569 ## 4 experiment view and click 312 3.10 Task 2: Set up Hypothesis experiment group: the group that is involved in the new experiment . i.e the group that received the new ad . Control group: the 2nd group that didn‚Äôt receive the new ad Click-through rate (CTR): the number of clicks advertisers receive on their ads per number of impressions. table(df$group) ## ## control experiment ## 1876 1881 df%&gt;%count(group) ## # A tibble: 2 √ó 2 ## group n ## &lt;chr&gt; &lt;int&gt; ## 1 control 1876 ## 2 experiment 1881 table(df) ## action ## group view view and click ## control 1513 363 ## experiment 1569 312 prop.table(table(df), 1) ## action ## group view view and click ## control 0.8065032 0.1934968 ## experiment 0.8341308 0.1658692 df %&gt;% count(group) %&gt;% mutate(prop = n / sum(n)) ## # A tibble: 2 √ó 3 ## group n prop ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 control 1876 0.499 ## 2 experiment 1881 0.501 x &lt;- df %&gt;% group_by(group, action)%&gt;% summarise(n = n(), .groups = &#39;drop&#39;)%&gt;% pivot_wider(names_from = action, values_from = n, values_fill = list(n = 0)) names(x) &lt;- c(&quot;group&quot;, &quot;view&quot;, &quot;view_click&quot;) x%&gt;% group_by(group)%&gt;% transmute(view1 = view/(view+view_click), view_click1 = view_click/(view+view_click)) ## # A tibble: 2 √ó 3 ## # Groups: group [2] ## group view1 view_click1 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 control 0.807 0.193 ## 2 experiment 0.834 0.166 The null hypothesis is what we assume to be true before we collect the data, and the alternative hypothesis is usually what we want to try and prove to be true. So in our experiment than null hypothesis is assuming that the old ad is better than than new one. Then we set the significance level \\(\\alpha\\). The significance level is the probability of rejecting the null hypothesis when it‚Äôs true. (Type I error rate) For example, a significance level of 0.05 indicates a 5% risk of concluding that a difference exists when there is no actual difference. Lower significance levels indicate that you require stronger evidence before you reject the null hypothesis. So we will set our significance level to be 0.05. And if we reject the null hypothesis as a result of our experiment, then by having significant level of 0.05 then we are 95% confident that we can reject the null hypothesis. So setting the significance level is about how confident you are while you reject the null hypothesis, the fourth step is calculating the corresponding P value. The definition of P value is the probability of observing your statistic, if the null hypothesis is true. And then we will draw a conclusion whether to go for the new ad or not. Hypothesis Testing steps: 1) Specify the Null Hypothesis. 2) Specify the Alternative Hypothesis. 3) Set the Significance Level (a) 4) Calculate the Corresponding P-Value. 5) Drawing a Conclusion Our Hypothesis Hypothesis is that the click through rate associated with the new ad is less than that associated with the old ad, which means that the old ad is better than than new one. And the alternative hypothesis will be the opposite. 3.11 Task 3: Compute the difference in the click-through rate This task we will compute the difference in the click through rate between the control and experiment groups. control_df &lt;- df[df$group == &quot;control&quot;, ] experiment_df &lt;- df[df$group == &quot;experiment&quot;, ] control_ctr &lt;- mean(ifelse(control_df$action==&quot;view and click&quot;, 1, 0)) experiment_ctr &lt;- mean(ifelse(experiment_df$action==&quot;view and click&quot;, 1, 0)) diff &lt;- experiment_ctr - control_ctr diff ## [1] -0.02762758 "],["task-four-create-sample-distribution-using-bootsrapping.html", "Chapter 4 Task four : create sample distribution using bootsrapping 4.1 Task five : Evaluate the null hypothesis and draw conclustions. 4.2 alternative random sampling code", " Chapter 4 Task four : create sample distribution using bootsrapping 4.0.1 Bootstrapping : The bootstrap method is a statistical technique for estimating quantities about a population by averaging estimates from multiple small data samples. Importantly, samples are constructed by drawing observations from a large data sample one at a time and returning them to the data sample after they have been chosen. This allows a given observation to be included in a given small sample more than once. This approach to sampling is called sampling with replacement. 4.0.2 Example : Bootstrapping in statistics, means sampling with replacement. So, if we have a group of individuals and , and want to bootstrap sample of ten individuals from this group , we could randomly sample any ten individuals but with bootsrapping, we are sampling with replacement so we could actually end up sampling 7 out of the ten individuals and three of the previously selected individuals might end up being sampled again. set.seed(1234) difference &lt;- numeric() size &lt;- dim(df)[1] for (i in 1:10000){ sample_index &lt;- sample(1:nrow(df), size = size, replace = TRUE) sample_df &lt;- df[sample_index, ] controls_df &lt;- sample_df[sample_df$group==&quot;control&quot;,] experiments_df &lt;- sample_df[sample_df$group==&quot;experiment&quot;,] controls_ctr &lt;- mean(ifelse(controls_df$action==&quot;view and click&quot;, 1, 0)) experiments_ctr &lt;- mean(ifelse(experiments_df$action==&quot;view and click&quot;, 1, 0)) difference &lt;- append(difference, experiments_ctr - controls_ctr) } 4.1 Task five : Evaluate the null hypothesis and draw conclustions. The central limit theorem states that if you have a population with mean Œº and standard deviation œÉ and take sufficiently large random samples from the population with replacement , then the distribution of the sample means will be approximately normally distributed. hist(difference) simulate the distribution under the null hypothesis (difference = 0) null_hypothesis &lt;- rnorm(n = length(difference), mean=0, sd=sd(difference)) hist(null_hypothesis) abline(v= diff, col = &quot;red&quot;) The definition of a p-value is the probability of observing your statistic (or one more extreme in favor of the alternative) if the null hypothesis is true. The confidence level is equivalent to 1 ‚Äì the alpha level. So, if your significance level is 0.05, the corresponding confidence level is 95%. i.e for P Value less than 0.05 we are 95% percent confident that we can reject the null hypothesis compute p-value mean((null_hypothesis &gt; diff)) ## [1] 0.9864 It says that we dont reject the null hypothesis. We can find more extreme values than our test statistics 98% of the time if the null hypothesis true. 4.2 alternative random sampling code df %&gt;% slice_sample(n = size, replace = T) ## # A tibble: 3,757 √ó 2 ## group action ## &lt;chr&gt; &lt;chr&gt; ## 1 experiment view ## 2 experiment view ## 3 experiment view ## 4 control view ## 5 control view ## 6 control view ## 7 experiment view ## 8 control view ## 9 experiment view ## 10 control view ## # ‚Ñπ 3,747 more rows "],["ab-testing.html", "Chapter 5 AB Testing 5.1 Sources 5.2 Concepts 5.3 AI Summary 5.4 Size of the Control Group", " Chapter 5 AB Testing It is now widely accepted that the gold standard technique to compute the causal effect of a treatment (a drug, ad, product, ‚Ä¶) on an outcome of interest (a disease, firm revenue, customer satisfaction, ‚Ä¶) is AB testing, a.k.a. randomized experiments. We randomly split a set of subjects (patients, users, customers, ‚Ä¶) into a treatment and a control group and give the treatment to the treatment group. This procedure ensures that, ex-ante, the only expected difference between the two groups is caused by the treatment. 5.1 Sources Sample Size Article Sample Size Calculator Sample Size Calculator in Excel 5.2 Concepts Contamination: One of the key assumptions in AB testing is that there is no contamination between treatment and control group. Giving a drug to one patient in the treatment group does not affect the health of patients in the control group. This might not be the case for example if we are trying to cure a contageous disease and the two groups are not isolated. In the industry, frequent violations of the contamination assumption are network effects - my utility of using a social network increases as the number of friends on the network increases - and general equilibrium effects - if I improve one product, it might decrease the sales of another similar product. Because of this reason, often experiments are carried out at a sufficiently large scale so that there is no contamination across groups, such as cities, states or even countries. Then another problem arises because of the larger scale: the treatment becomes more expensive. Giving a drug to 50% of patients in a hospital is much less expensive than giving a drug to 50% of cities in a country. Therefore, often only few units are treated but often over a longer period of time. In these settings, a very powerful method emerged around 10 years age: Synthetic Control. The idea of synthetic control is to exploit the temporal variation in the data instead of the cross-sectional one (across time instead of across units). This method is extremely popular in the industry - e.g.¬†in companies like Google, Uber, Facebook, Microsoft, Amazon - because it is easy to interpret and deals with a setting that emerges often at large scales. 5.3 AI Summary Studying A/B testing (also known as randomized controlled trials or RCTs) is a great way to understand experimental design in causal inference, particularly in contexts where random assignment is feasible and ethical considerations allow for manipulation of variables to observe their effects. Here‚Äôs a detailed overview to get you started: 5.3.1 A/B Testing (Randomized Controlled Trials) A/B testing, or randomized controlled trials (RCTs), is an experimental design where participants or subjects are randomly assigned to different groups (treatment and control) to test the effectiveness of a particular intervention or treatment. 5.3.1.1 Key Concepts Random Assignment: Participants are assigned to either the treatment group (exposed to the intervention) or the control group (not exposed) through randomization. This helps ensure that any differences observed between the groups are due to the intervention rather than pre-existing differences. Controlled Environment: The experiment is conducted in a controlled environment where researchers can manipulate variables and minimize external influences that could affect the outcomes. Causal Inference: A/B testing allows researchers to make causal inferences about the effect of the intervention on the outcome variable. By comparing outcomes between the treatment and control groups, researchers can estimate the causal impact of the intervention. 5.3.1.2 Steps in A/B Testing Hypothesis Formulation: Define a clear hypothesis about the expected effect of the intervention on the outcome variable. Random Assignment: Randomly assign participants or subjects to the treatment and control groups. Randomization helps ensure that the groups are comparable on average, reducing the risk of bias. Implementation of Intervention: Implement the intervention or treatment with the treatment group while keeping conditions unchanged for the control group. Outcome Measurement: Measure the outcome of interest for both the treatment and control groups after the intervention. This could be metrics like conversion rates, satisfaction scores, or health outcomes. Statistical Analysis: Compare the outcomes between the treatment and control groups using statistical methods (e.g., t-tests, regression analysis) to determine if there is a significant difference attributable to the intervention. Interpretation of Results: Interpret the results to determine whether the intervention had a causal effect on the outcome variable. Consider factors such as statistical significance, effect size, and practical significance. 5.3.1.3 Advantages of A/B Testing Causal Inference: Allows for strong causal claims about the impact of interventions. Control Over Variables: Researchers have control over experimental conditions, minimizing confounding factors. Versatility: Applicable across various fields including marketing, healthcare, education, and technology. 5.3.1.4 Example Application Imagine a company wants to test the effectiveness of two different website layouts (A and B) on user engagement: Hypothesis: Layout B will lead to higher user engagement compared to Layout A. Random Assignment: Users visiting the website are randomly assigned to either see Layout A or Layout B. Outcome Measurement: Engagement metrics such as click-through rates or time spent on the website are measured for both groups. Analysis: Statistical tests are conducted to compare engagement metrics between Layout A and Layout B. Conclusion: If Layout B shows significantly higher engagement metrics, the company may decide to implement Layout B on their website. 5.3.2 Concepts: 5.3.2.1 Effect Size Definition: Effect size quantifies the magnitude of the difference or relationship between variables in a study. It provides a standardized measure of the strength of an effect or phenomenon being studied, independent of sample size. Key Points: Standardized Measure: Effect size is expressed in standard deviation units or other standardized metrics, making it comparable across different studies. Interpretation: A larger effect size indicates a stronger relationship or more substantial difference between groups or conditions. Example: In an A/B test measuring website conversion rates: If Group A (control) has a conversion rate of 5% and Group B (treatment) has a conversion rate of 7%, the effect size could be quantified using metrics like Cohen‚Äôs d or relative risk increase to indicate the practical significance of the difference. Certainly! Cohen‚Äôs d and relative risk are commonly used effect size measures in different contexts, providing standardized ways to quantify and compare the magnitude of effects between groups or conditions in research studies. Here‚Äôs an explanation of each: 5.3.2.2 Cohen‚Äôs d Definition: Cohen‚Äôs d is a standardized measure of effect size that indicates the difference between two means (e.g., treatment group mean and control group mean) in terms of standard deviation units. Formula: $ d = $ where: - $ {X}_1$ and ${X}_2 $ are the means of the two groups, - $ s $ is the pooled standard deviation of the two groups. Interpretation: - Effect Size Magnitude: Cohen‚Äôs d values are interpreted as follows: - Small effect: d = 0.2 - Medium effect: d = 0.5 - Large effect: d = 0.8 Example: In a study comparing the effectiveness of two teaching methods on exam scores: - If the mean exam score in Method A (treatment) is 80 and in Method B (control) is 75, and the pooled standard deviation is 10, then Cohen‚Äôs d would be \\(\\frac{80 - 75}{10} = 0.5\\), indicating a medium effect size. 5.3.2.3 Relative Risk Definition: Relative risk (RR) is a measure of the strength of association between a risk factor (or exposure) and an outcome in epidemiology and medical research. It compares the risk of an outcome occurring in the exposed group versus the unexposed (or control) group. Formula: \\[ RR = \\frac{\\text{Risk in exposed group}}{\\text{Risk in unexposed group}} \\] Interpretation: - RR = 1: Indicates no association between exposure and outcome. - RR &gt; 1: Indicates higher risk in the exposed group compared to the unexposed group. - RR &lt; 1: Indicates lower risk in the exposed group compared to the unexposed group. Example: In a clinical trial evaluating a new drug for heart disease: - If the risk of heart attack among patients taking the new drug is 10% and among patients not taking the drug (control group) is 20%, then the relative risk would be \\(\\frac{0.10}{0.20} = 0.5\\). - This means patients taking the drug have half the risk of experiencing a heart attack compared to those not taking the drug. 5.3.3 Comparison and Usage Cohen‚Äôs d: Typically used in studies comparing means of continuous variables (e.g., exam scores, reaction times) between two groups. Relative Risk: Primarily used in studies of binary outcomes (e.g., disease incidence, event occurrence) to compare the risk of an outcome between exposed and unexposed groups. Both measures provide valuable insights into the strength and direction of effects in research studies. The choice between Cohen‚Äôs d and relative risk depends on the nature of the data (continuous or binary) and the specific research question being addressed. Researchers often use these effect size measures alongside significance testing to provide a comprehensive assessment of the findings‚Äô practical and statistical significance. 5.3.4 Significance Definition: Statistical significance determines whether the observed results in a study are likely to be due to the intervention (or other factors being studied) rather than random chance. It is typically assessed through hypothesis testing. Key Points: - Hypothesis Testing: Statistical tests (e.g., t-tests, ANOVA, chi-square tests) evaluate whether the observed differences between groups are statistically significant. Threshold: Results are deemed statistically significant if the probability (p-value) of observing such differences due to chance alone is below a predefined significance level (commonly set at 0.05). Does Not Equal Importance: Statistical significance does not necessarily equate to practical or clinical significance; it only indicates the reliability of the observed effect. Example: In a clinical trial evaluating a new drug: - If the treatment group shows a significantly lower incidence of adverse effects compared to the control group (p &lt; 0.05), it suggests that the drug may have a beneficial effect on reducing adverse reactions. 5.3.5 Group Size Definition: Group size refers to the number of participants or subjects included in each experimental group or condition in a study. It directly influences the statistical power and precision of the study‚Äôs results. Key Points: - Statistical Power: Larger group sizes generally increase the statistical power of a study, making it more likely to detect a true effect if one exists. - Precision: Larger group sizes reduce sampling variability and increase the precision of estimates (e.g., mean values, effect sizes). - Resource Allocation: Group size is often determined by practical considerations such as budget, time constraints, ethical considerations, and expected effect size. Example: In an A/B test comparing two marketing strategies: - If Group A consists of 1000 customers and Group B consists of 500 customers, the study‚Äôs power to detect differences between the groups will be influenced by the unequal group sizes. 5.3.6 Relationship Between Effect Size, Significance, and Group Size Effect Size and Significance: A larger effect size increases the likelihood of achieving statistical significance with smaller group sizes. Conversely, smaller effect sizes may require larger group sizes to achieve statistical significance. Group Size and Precision: Larger group sizes generally provide more precise estimates of effects and reduce the impact of random variability in the data. Balancing Factors: Researchers often balance effect size, significance level, and group size to achieve meaningful and reliable results within practical constraints. 5.3.7 Conclusion Understanding effect size, significance, and group size is crucial for interpreting and evaluating research findings accurately. Effect size measures the magnitude of effects, significance assesses the likelihood of results being due to chance, and group size influences the study‚Äôs statistical power and precision. Together, these concepts help researchers draw meaningful conclusions and inform decision-making based on empirical evidence. 5.3.7.1 Baseline conversion rate The baseline conversion rate is the current conversion rate for the page you are testing. Conversion rate is the number of conversions divided by the total number of visitors. 5.3.7.2 Minimum detectable effect (MDE) After baseline conversion rate, you need to decide how much change from the baseline (how big or small a lift) you want to detect. You wil need less traffic to detect big changes and more traffic to detect small changes. To demonstrate, let us use an example with a 20% baseline conversion rate and a 5% MDE. Based on these values, your experiment will be able to detect 80% of the time when a variation‚Äôs underlying conversion rate is actually 19% or 21% (20%, +/- 5% √ó 20%). If you try to detect differences smaller than 5%, your test is considered underpowered. Power is a measure of how well you can distinguish the difference you are detecting from no difference at all. So running an underpowered test is the equivalent of not being able to strongly declare whether your variations are winning or losing. 5.4 Size of the Control Group Calculating the appropriate size of the control group in an experiment involves several important factors to ensure the study has sufficient power to detect a true effect if one exists. Here are the key factors that influence control group size calculation: Effect Size (Œ¥) Effect size is a measure of the magnitude of the difference between groups or the strength of the relationship between variables. It quantifies the practical significance of the treatment effect. Influence on Sample Size: A larger effect size requires a smaller sample size to detect a significant difference. A smaller effect size requires a larger sample size to achieve the same level of power. Significance Level (Œ±) The significance level (Œ±) is the threshold for determining whether the observed effect is statistically significant. It represents the probability of committing a Type I error (rejecting a true null hypothesis). Common Values: - Œ± is typically set at 0.05, meaning there is a 5% chance of rejecting the null hypothesis when it is true. Influence on Sample Size: A lower Œ± (e.g., 0.01) requires a larger sample size to maintain the same power, as the test becomes more stringent. A higher Œ± (e.g., 0.10) allows for a smaller sample size but increases the risk of Type I errors. Power (1 - Œ≤) Power is the probability of correctly rejecting a false null hypothesis. It reflects the study‚Äôs ability to detect an effect if one exists. Common Values: - A common target for power is 0.80, indicating an 80% chance of detecting a true effect. Influence on Sample Size: - Higher power (e.g., 0.90) requires a larger sample size. - Lower power (e.g., 0.70) allows for a smaller sample size but increases the risk of Type II errors (failing to detect a true effect). Variability (œÉ) Variability refers to the spread or dispersion of data points within a population, often measured by the standard deviation (œÉ). Influence on Sample Size: - Higher variability (greater standard deviation) requires a larger sample size to detect a significant difference. - Lower variability allows for a smaller sample size as the effect is easier to detect against a less noisy background. Allocation Ratio Definition: The allocation ratio determines the proportion of participants assigned to the treatment group versus the control group. An equal allocation ratio (1:1) means equal numbers in both groups. Influence on Sample Size: - Unequal allocation ratios (e.g., 2:1 or 3:1) may be used based on study design or practical considerations but can affect the total sample size required to achieve the desired power. - An equal allocation ratio generally provides the most statistically efficient design, minimizing the total sample size required. Dropout Rate Definition: The dropout rate accounts for participants who may leave the study before its completion, affecting the effective sample size. Influence on Sample Size: - Anticipated dropouts should be factored into the initial sample size calculation to ensure the study retains adequate power despite participant loss. 5.4.1 Sample Size Calculation Formula For comparing two means, the sample size for each group can be calculated using: \\[ n = \\left( \\frac{(Z_{\\alpha/2} + Z_{\\beta}) \\cdot \\sigma}{\\delta} \\right)^2 \\] where: - \\(n\\) is the sample size per group, - \\(Z_{\\alpha/2}\\) is the critical value for the desired significance level, - \\(Z_{\\beta}\\) is the critical value for the desired power, - \\(\\sigma\\) is the standard deviation, - \\(\\delta\\) is the effect size. 5.4.2 Conclusion Determining the control group size involves considering the desired effect size, significance level, power, variability in the data, allocation ratio, and potential dropout rates. Properly calculating the control group size ensures that the study is adequately powered to detect meaningful effects, thereby enhancing the validity and reliability of the research findings. 5.4.3 Statistical Assumptions for Randomized Controlled Trials (RCTs) Randomized Controlled Trials (RCTs) are considered the gold standard in experimental research due to their ability to minimize bias and establish causality. However, the validity of RCT results depends on several key statistical assumptions: Randomization: Assumption: Participants are randomly assigned to treatment and control groups. Purpose: Ensures that the groups are comparable on average, reducing selection bias and balancing both known and unknown confounders. Independence: Assumption: Observations are independent of each other. Purpose: Ensures that the outcome of one participant does not influence the outcome of another, which is crucial for valid statistical inference. Consistency: Assumption: The treatment effect is consistent across all participants. Purpose: Ensures that the treatment effect observed in the sample can be generalized to the broader population. Exclusion of Confounders: Assumption: No confounding variables influence the treatment-outcome relationship. Purpose: Ensures that the observed effect is due to the treatment and not due to other external factors. Stable Unit Treatment Value Assumption (SUTVA): Assumption: The potential outcomes for any participant are not affected by the treatment assignment of other participants. Purpose: Prevents interference between participants, ensuring that each participant‚Äôs outcome is solely a result of their treatment assignment. No Systematic Differences in Measurement: Assumption: Measurement of outcomes is consistent and unbiased across treatment and control groups. Purpose: Ensures that outcome measures are not systematically biased by the treatment assignment. 5.4.4 Robustness Checks Robustness checks involve testing the stability and reliability of the study‚Äôs findings under various assumptions and conditions. They help to confirm that the results are not sensitive to specific assumptions or potential biases. Key robustness checks for RCTs include: Sensitivity Analysis: Purpose: Evaluates how the results change with different assumptions or parameters. Method: Adjusting key assumptions or parameters (e.g., different definitions of the outcome variable) to see if the results remain consistent. Subgroup Analysis: Purpose: Examines the effect of the treatment within different subgroups of the sample. Method: Dividing the sample into subgroups (e.g., by age, gender, or baseline risk) and checking if the treatment effect is consistent across these groups. Placebo Tests: Purpose: Tests whether the results hold when using a placebo treatment. Method: Using a placebo group to confirm that the observed effects are specifically due to the treatment and not to other factors. Alternative Specifications: Purpose: Tests the robustness of the results to different model specifications. Method: Using alternative statistical models or different functional forms to ensure results are not model-dependent. Attrition Analysis: Purpose: Examines the impact of participant dropout on the study results. Method: Analyzing the characteristics of dropouts and conducting analyses to understand if and how attrition might bias the results. 5.4.5 Validation Methods Validation methods are used to confirm the internal and external validity of the study‚Äôs findings. These methods help to ensure that the results are credible and can be generalized to other settings or populations. Internal Validity Checks: Balance Checks: Purpose: Ensures that randomization created comparable groups. Method: Comparing baseline characteristics between treatment and control groups to check for balance. Compliance Checks: Purpose: Ensures participants adhere to the assigned treatment. Method: Analyzing adherence rates and conducting per-protocol analyses if necessary. External Validity Checks: Population Representativeness: Purpose: Ensures that the study sample is representative of the broader population. Method: Comparing sample characteristics to the target population and discussing potential generalizability limitations. Replication Studies: Purpose: Confirms the findings by replicating the study in different settings or with different populations. Method: Conducting similar studies in various contexts to see if the results hold. Model Validation: Cross-Validation: Purpose: Assesses the predictive accuracy of the statistical model. Method: Using techniques like k-fold cross-validation to test the model‚Äôs performance on different subsets of the data. Out-of-Sample Validation: Purpose: Ensures the model performs well on new, unseen data. Method: Validating the model on a separate dataset that was not used for model training. 5.4.6 Conclusion The validity of RCT findings hinges on several key assumptions, and the credibility of these results is reinforced through robustness checks and validation methods. Robustness checks test the stability of findings under different conditions, while validation methods confirm the internal and external validity of the results, ensuring they are generalizable and reliable. Understanding and addressing these factors is crucial for conducting and interpreting high-quality research. "],["difference-in-differences-did-methods.html", "Chapter 6 Difference-in-Differences (DiD) Methods 6.1 Simple Difference-in-Differences (DiD) 6.2 Controversial Note 6.3 Placebo tests for parallel trends 6.4 Two-Way Fixed Effects Model 6.5 Event Study Methods 6.6 Importance of Placebos in DD 6.7 Compositional Changes 6.8 Key Assumptions 6.9 Notes 6.10 Extra Considerations 6.11 Synthetic Difference-in-Differences (SynthDiD) method 6.12 Doubly Robust Difference in Differences 6.13 Twoway Fixed Effects with Differential Timing 6.14 Bacon Decomposition", " Chapter 6 Difference-in-Differences (DiD) Methods Difference-in-Differences (DiD) is a quasi-experimental technique used in econometrics to estimate causal relationships. It compares the changes in outcomes over time between a treatment group and a control group. Some resource links Comprehensive resource Extra reading 2 Mixtape youtube series Books The Effect What if? [Mathaeus - personal](Extra reading - python](https://matheusfacure.github.io/python-causality-handbook/13-Difference-in-Differences.htm) 6.1 Simple Difference-in-Differences (DiD) Basic Idea: Difference-in-Differences (DiD) is a quasi-experimental design used in econometrics to estimate causal relationships. It compares the changes in outcomes over time between a treatment group and a control group. Treatment assignment is not random, but we observe both treated and untreated units before and after treatment. -Under certain structural assumptions, especially parallel outcome trends in the absence of treatment, we can recover the average treatment effect. Formula: The basic DiD estimator is: \\[ \\text{DiD} = (\\text{Y}_{\\text{post-treatment, treatment group}} - \\text{Y}_{\\text{pre-treatment, treatment group}}) - (\\text{Y}_{\\text{post-treatment, control group}} - \\text{Y}_{\\text{pre-treatment, control group}}) \\] Concept: DiD is used when we have data from before and after a treatment is applied to a treatment group, and we also have a control group that does not receive the treatment. The key assumption is that in the absence of treatment, the difference between the treatment and control groups would have remained constant over time (parallel trends assumption). Simple 2x2 DD collapses to true ATT when parallel trend holds true. ATT can be calculated through differencing outcomes but regression can be used instead if we want to control for some more covariates. if you need to avoid omitted variable bias through controlling for endogenous covariates that vary over time, then you may want to use regression. Such strategies are another way of saying that you will need to close some known critical backdoor. Another reason for the equation is that by controlling for more appropriate covariates, you can reduce residual variance and improve the precision of your DD estimate. Model: \\[ Y_{it} = \\alpha + \\beta_1 \\text{Post}_t + \\beta_2 \\text{Treated}_i + \\beta_3 (\\text{Post}_t \\times \\text{Treated}_i) + \\epsilon_{it} \\] where: \\(Y_{it}\\) is the outcome variable for entity \\(i\\) at time \\(t\\). \\(\\text{Post}_t\\) is a dummy variable equal to 1 for periods after the treatment and 0 otherwise. \\(\\text{Treated}_i\\) is a dummy variable equal to 1 for the treatment group and 0 for the control group. \\(\\beta_3\\) is the DiD estimator, representing the treatment effect (ATT). 6.2 Controversial Note The variables of interest in many of these setups only vary at a group level, such as the state, and outcome variables are often serially correlated. In Card and Krueger (1994), it is very likely for instance that employment in each state is not only correlated within the state but also serially correlated. Bertrand, Duflo, and Mullainathan (2004) point out that the conventional standard errors often severely understate the standard deviation of the estimators, and so standard errors are biased downward, ‚Äútoo small,‚Äù and therefore overreject the null hypothesis. Bertrand, Duflo, and Mullainathan (2004) propose the following solutions: Block bootstrapping standard errors. Aggregating the data into one pre and one post period. This approach ignores the time-series dimensions altogether, and if there is only one pre and post period and one untreated group, it‚Äôs as simple as it sounds. Clustering standard errors at the group level. You simply adjust standard errors by clustering at the group level, as we discussed in the earlier chapter, or the level of treatment. For state-level panels, that would mean clustering at the state level, which allows for arbitrary serial correlation in errors within a state over time. This is the most common solution employed. If number of groups is small, then you may use wild bootstrap technique, or randomization inference. 6.3 Placebo tests for parallel trends We can test palcebo effects in the pre-treatment years to show in the pretreatment years both groups have similar trends. However, this may not prove that those groups will behave similarly after the treatment in the absence of treatment. Just because they were similar before does not logically require they be the same after. Likewise, we are not obligated to believe that that counterfactual trends would be the same post-treatment because they had been similar pre-treatment without further assumptions about the predictive power of pre-treatment trends. But this is a nice attempt anyway. While the test is important, technically pre-treatment similarities are neither necessary nor sufficient to guarantee parallel counterfactual trends (Kahn-Lang and Lang 2019). Any DD is a combination of a comparison between the treatment and the never treated, an early treated compared to a late treated, and a late treated compared to an early treated. Thus only showing the comparison with the never treated is actually a misleading presentation of the underlying mechanization of identification using an twoway fixed-effects model with differential timing. 6.4 Two-Way Fixed Effects Model Concept: The two-way fixed effects model extends the simple DiD approach by controlling for time-invariant characteristics of the entities and common shocks over time. It adds fixed effects for both entities and time periods to control for unobserved heterogeneity. Model: \\[ Y_{it} = \\alpha_0 + \\beta_1\\text{Treat}_i + \\beta_2\\text{Post}_t + \\beta_3 (\\text{Post}_t \\times \\text{Treat}_i) + \\epsilon_{it} \\] where: \\(\\beta_1\\) represents entity fixed effects. \\(\\beta_2\\) represents time fixed effects. \\(\\beta_3\\) remains the DiD estimator. Example: Using the job training program example, this model would account for fixed characteristics of individuals (such as inherent employability) and time-specific effects (such as economic conditions). \\[ Y_{it} = \\alpha_i + \\gamma_t + \\beta_3 (\\text{Post}_t \\times \\text{Treated}_i) + \\epsilon_{it} \\] This controls for both individual-specific and time-specific unobserved heterogeneity, providing a more robust estimate of the treatment effect. 6.5 Event Study Methods Concept: Event studies extend DiD by examining the dynamics of the treatment effect over multiple periods before and after the treatment. They allow for the estimation of treatment effects at different time points relative to the treatment event. As with many contemporary DD designs, Miller et al.¬†(2019) evaluate the pre-treatment leads instead of plotting the raw data by treatment and control. Post-estimation, they plotted regression coefficients with 95% confidence intervals on their treatment leads and lags. Including leads and lags into the DD model allowed the reader to check both the degree to which the post-treatment treatment effects were dynamic, and whether the two groups were comparable on outcome dynamics pre-treatment. Typical Model: \\[ Y_{ist} = \\alpha_s + \\gamma_t + \\sum_{x=-q}^{-1} \\beta_x D_{sx} + \\sum_{x=0}^{m} \\delta_x D_{sx} + X_{ist} + \\epsilon_{ist} \\] You include \\(q\\) leads or anticipatory effects and \\(m\\) lags or post-treatment effects. 6.6 Importance of Placebos in DD It is a simple idea. For the minimum wage sttaudy, one candidate placebo falsification might simply be to use data for an alternative type of worker whose wages would not be affected by the binding minimum wage. This reasoning might lead us to consider the possibility that higher wage workers might function as a placebo. Many people like to be straightforward and simply fit the same DD design using high wage employment as the outcome. If the coefficient on minimum wages is zero when using high wage worker employment as the outcome, but the coefficient on minimum wages for low wage workers is negative, then we have provided stronger evidence that complements the earlier analysis we did when on the low wage workers. Another way to show placebo falsification. Triple DDD. 6.6.1 Triple Differences \\[ Y_{ijt} = \\alpha + \\beta_0X_{ist} + \\beta_1\\gamma_t + \\beta_2\\delta_j + \\beta_3 D_i + \\beta_4 (\\delta . \\gamma)_{jt} + \\beta_5 (\\gamma . D)_{ti} + \\beta_6 (\\delta . D)_{ij} + \\beta_7 (\\delta . \\gamma . D)_{ijt} + \\epsilon_{ijt} \\] where the parameter of interest is \\(\\beta_7\\). This requires a stacking of the data into a panel structure by group, as well as state. Second, the DDD model requires that you include all possible interactions across the group dummy \\(\\delta_j\\), post-treatment dummy \\(\\gamma_t\\) and treatment state dummy \\(D_i\\). The regression must include each dummy independently, each individual interaction, and the triple differences interaction. One of these will be dropped due to multicollinearity, but I include them in the equation so that you can visualize all the factors used in the product of these terms. 6.7 Compositional Changes DD can be applied to repeated cross-sections, as well as panel data. But one of the risks of working with the repeated cross-sections is that unlike panel data (e.g., individual-level panel data), repeated cross-sections run the risk of compositional changes. This kind of compositional change is a like an omitted variable bias built into the sample itself caused by time-variant unobservables. Diffusion of the Internet appears to be related to changing samples as younger music fans are early adopters. Identification of causal effects would need for the treatment itself to be exogenous to such changes in the composition. 6.8 Key Assumptions Parallel Trends Assumption: The treatment and control groups would have followed the same trend over time in the absence of the treatment. This is the most critical assumption. Common Shocks: Both groups are assumed to be subject to the same external factors over time. 6.8.1 Implementation Steps Identify Treatment and Control Groups: Clearly define which units are exposed to the treatment and which are not. Collect Data: Obtain data on the outcome of interest for both groups before and after the treatment. Check Parallel Trends: Visualize and statistically test if the pre-treatment trends of the groups are parallel. Estimate the Model: Use regression analysis to estimate the DiD effect. The basic regression model is: \\[ Y_{it} = \\alpha + \\beta_1 \\text{Post}_t + \\beta_2 \\text{Treatment}_i + \\beta_3 (\\text{Post}_t \\times \\text{Treatment}_i) + \\epsilon_{it} \\] where \\(\\beta_3\\) is the DiD estimator. 6.8.2 Advantages Controls for Time-Invariant Differences: Differences between the treatment and control groups that do not change over time are accounted for. Simple and Intuitive: The method is straightforward to understand and implement. 6.8.3 Limitations Violation of Parallel Trends: If the parallel trends assumption is violated, the DiD estimate can be biased. External Validity: The results are only valid for the sample and period studied. Simultaneous Interventions: Other changes occurring simultaneously with the treatment can confound the results. 6.8.4 Q: How would you test the parallel trends assumption? Visual Inspection Plot the outcome variable over time for both the treatment and control groups. If the trends are parallel before the intervention, it suggests that the parallel trends assumption holds. Statistical Tests Conduct a regression test to formally check for parallel trends. This involves using only the pre-treatment data and checking if the interaction between time and treatment is statistically significant. Steps: Restrict your data to pre-treatment periods. Regress the outcome on time, treatment, and their interaction. Check if the coefficient of the interaction term is statistically significant. Placebo Tests Conduct a placebo test by pretending that the treatment happened at a different time and check if you find a significant effect where none should exist. Steps: Choose a time period before the actual treatment period as the ‚Äúplacebo treatment period.‚Äù Perform a DiD analysis as if the treatment happened during this placebo period. Check for significant effects; finding none supports the parallel trends assumption. Event Study Analysis An event study involves plotting the estimated treatment effects at different time periods before and after the treatment to visually inspect if pre-treatment effects are close to zero. Steps: Create a series of dummy variables for each time period relative to the treatment. Regress the outcome on these time dummies and the interaction terms. Plot the coefficients of these interaction terms. 6.8.5 Q: How would you address potential violations of the parallel trends assumption? Pre-Treatment Trends Analysis Before conducting the DiD analysis, carefully examine the pre-treatment trends. If the trends are not parallel, you might need to reconsider your groups or the methodology. Visual Inspection: Plot the pre-treatment trends for the treatment and control groups. If they are not parallel, consider this a red flag. Statistical Testing: Perform a formal test by regressing the outcome on a time indicator, treatment indicator, and their interaction using only pre-treatment data. A significant interaction term suggests non-parallel trends. Control for Covariates Include control variables in your regression model to account for differences between the treatment and control groups that might affect the outcome variable. Collect relevant covariates that could influence the outcome. Include these covariates in your regression model: \\[ Y_{it} = \\alpha + \\beta_1 \\text{Post}_t + \\beta_2 \\text{Treatment}_i + \\beta_3 (\\text{Post}_t \\times \\text{Treatment}_i) + \\gamma X_{it} + \\epsilon_{it} \\] where \\(X_{it}\\) represents the covariates. Matching Use matching techniques to create a more comparable control group. Matching ensures that the treatment and control groups are similar in observed characteristics. Propensity Score Matching (PSM): Match treatment units with control units based on the propensity score, which is the probability of receiving treatment given covariates. Coarsened Exact Matching (CEM): Match units exactly on certain covariates. Synthetic Control Method Construct a synthetic control group that closely resembles the treatment group in the pre-treatment period. This method is particularly useful when you have one treatment unit and many potential control units. Select control units to construct a weighted combination (synthetic control) that mirrors the treatment unit‚Äôs pre-treatment characteristics. Compare the post-treatment outcomes of the treatment unit with the synthetic control. Difference-in-Differences-in-Differences (DiDiD) If you have an additional control group or variable, you can use DiDiD to control for potential violations. This method adds another layer of difference to control for unobserved confounders. Include a third group or dimension to add another difference. For example: \\[ Y_{it} = \\alpha + \\beta_1 \\text{Post}_t + \\beta_2 \\text{Treatment}_i + \\beta_3 (\\text{Post}_t \\times \\text{Treatment}_i) + \\beta_4 \\text{Group}_i + \\beta_5 (\\text{Group}_i \\times \\text{Post}_t) + \\beta_6 (\\text{Group}_i \\times \\text{Treatment}_i) + \\beta_7 (\\text{Group}_i \\times \\text{Post}_t \\times \\text{Treatment}_i) + \\epsilon_{it} \\] where \\(\\text{Group}_i\\) represents the additional dimension. Sensitivity Analysis Conduct sensitivity analyses to check how robust your results are to potential violations of the parallel trends assumption. Placebo Tests: Perform DiD analysis using periods before the actual treatment to ensure no significant effects are detected. Alternative Specifications: Use different model specifications or subsets of data to check the consistency of your results. Instrumental Variables (IV) If you have a valid instrument, use it to address endogeneity issues that might violate the parallel trends assumption. Identify an instrument that affects the treatment but not directly the outcome. Use Two-Stage Least Squares (2SLS) to estimate the treatment effect. By applying these strategies, you can address potential violations of the parallel trends assumption, ensuring more robust and credible results from your DiD analysis. 6.9 Notes Bertrand, Duflo, and Mullainathan (2004) point out that conventional robust standard errors usually overestimate the actual standard deviation of the estimator. The authors recommend clustering the standard errors at the level of randomization (e.g.¬†classes, counties, villages, ‚Ä¶). Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania (1994) by Card and Krueger. 6.9.1 Example: Business A firm wants to test the impact of a TV advertisement campaign on revenue. The firm releases the ad on a random sample of municipalities and track the revenue over time, before and after the ad campaign. 6.10 Extra Considerations Two-way Fixed Effects (TWFE) model can give wrong estimates. This is very likely especially if treatments are heterogeneous (differential treatment timings, different treatment sizes, different treatment statuses over time) that can contaminate the treatment effects. This can result from ‚Äúbad‚Äù treatment combinations biased the average treatment estimation to the point of even reversing the sign. The new DiD methods ‚Äúcorrect‚Äù for these TWFE biases by combining various estimation techniques, such as bootstrapping, inverse probability weights, matching, influence functions, and imputations, to handle parallel trends, negative weights, covariates, and controls. 6.11 Synthetic Difference-in-Differences (SynthDiD) method Source Article (Arkhangelsky, Athey, Hirshberg, Imbens, Wager, 2021) We present a new estimator for causal effects with panel data that builds on insights behind the widely used difference in differences and synthetic control methods. Relative to these methods we find, both theoretically and empirically, that this ‚Äúsynthetic difference in differences‚Äù estimator has desirable robustness properties, and that it performs well in settings where the conventional estimators are commonly used in practice. We study the asymptotic behavior of the estimator when the systematic part of the outcome model includes latent unit factors interacted with latent time factors, and we present conditions for consistency and asymptotic normality. 6.11.1 Introduction Reading One-liner: SDID blends Synthetic Control (SCM) and Difference-in-Differences (DiD) by learning unit weights (like SCM) and time weights (like DiD/event-study) to estimate an ATT in panel data. It reduces bias from level differences (SCM strength) and common shocks/time trends (DiD strength), and has a doubly-robust flavor: it‚Äôs consistent if either the SCM-style unit balancing or the DiD-style time balancing holds. 6.11.2 What problem it solves Plain DiD can be biased when treated units differ in levels/composition from controls even if trends are parallel. Plain SCM can be biased when there are uncontrolled time shocks or with short pre-periods where matching levels alone isn‚Äôt enough. SDID addresses both by balancing across units and across time. 6.11.3 How it works (intuition) Unit weights (SCM step): Learn non-negative weights \\(w\\) over control units so a weighted control ‚Äúsynthetic‚Äù group matches the treated group in the pre-period (levels/trends/predictors). Time weights (DiD step): Learn non-negative weights \\(\\lambda\\) over time so pre- vs.¬†post-period comparisons down-weight noisy periods and align common shocks. ATT via ‚Äúsynthetic‚Äù DiD: Compute a difference-in-differences using these weights: \\[ \\hat{\\tau}_{\\text{SDID}} = \\big(\\overline{Y}_{T,\\text{post}} - \\overline{Y}^{\\,w}_{C,\\text{post}}\\big) \\;-\\; \\big(\\overline{Y}^{\\,\\lambda}_{T,\\text{pre}} - \\overline{Y}^{\\,w,\\lambda}_{C,\\text{pre}}\\big) \\] where bars denote weighted averages over units/time. 6.11.4 How it differs from related methods SCM: Builds a synthetic control and compares post-treatment levels of treated vs.¬†synthetic. SDID instead compares changes (pre‚Üípost) of treated vs.¬†synthetic (a DiD on top of SCM). DiD: Compares treated vs.¬†control changes assuming parallel trends. SDID reweights controls (unit weights) and time (time weights) to make that assumption more plausible. 6.11.5 When to use Panel data with few to moderate pre-periods and many control units. One or a few treated units, or a group treated at the same time (cohort). Concern about level differences and time shocks violating plain DiD/SCM. 6.11.6 Assumptions (why it‚Äôs appealing) No interference/anticipation (standard SUTVA-style conditions). Doubly-robust flavor: Consistent if either the DiD/parallel-trends condition holds after learned weights, or an interactive fixed-effects/factor model holds so SCM-style balancing captures confounding. Enough pre-period signal to learn weights (does not require long pre-periods, but needs some). 6.11.7 Inference &amp; diagnostics Placebo/permutation tests: apply SDID to untreated units as if treated. Block bootstrap or analytic SEs for uncertainty. Pre-trend checks with event-style plots using learned weights. Sensitivity: vary donor pool, pre-window, or weight regularization. 6.11.8 Pros / Cons Pros Handles level differences (SCM) and time shocks (DiD). Good finite-sample performance; stable with moderate pre-periods. Clear visualization (treated vs.¬†synthetic paths; pre/post gaps). Cons Works best with limited treated cohorts (extensions needed for complex staggered adoption). Requires panel structure and some pre-period data. Sensitive to donor pool choice and weight regularization (check robustness). 6.11.9 TL;DR (contrast) SCM: match pre-period levels ‚Üí compare post levels. DiD: assume parallel trends ‚Üí compare changes. SDID: learn weights to balance units and time ‚Üí compare changes of synthetic vs.¬†treated. 6.11.10 An Example: Suppose that we are a company that sells plant-based food products, such as soy milk or soy yogurt, and we operate in multiple countries. Some countries implement new legislation that prohibits us from marketing our plant-based products as ‚Äòmilk‚Äô or ‚Äòyogurt‚Äô because it is claimed that only animal products can be marketed as ‚Äòmilk‚Äô or ‚Äòyogurt‚Äô. Thus, due to this new regulation in some countries, we have to market soy milk as soy drink instead of soy milk, etc. We want to know the impact of this legislation on our revenue as this might help guide our lobbying efforts and marketing activities in different countries. I simulated a balanced panel dataset that shows the revenue of our company in 30 different countries for 30 periods. Three of the countries implement this legislation in period 20. In the figure below, you can see a snapshot of the data. treat is a dummy variable indicating whether a country has implemented the legislation in a given period. revenueis the revenue in millions of EUR. You can find the simulation and estimation code in this Gist. # Install and load the required packages # devtools::install_github(&quot;synth-inference/synthdid&quot;) library(synthdid) library(ggplot2) library(fixest) # Fixed-effects regression library(data.table) # Set seed for reproducibility set.seed(12345) source(&#39;sim_data.R&#39;) # Import simulation function and some utilities dt &lt;- sim_data() head(dt) In Data, there are 30 units (3 units treated), 30 periods (10 periods treated), all units are treated at the same time. Next, we convert our panel data into a matrix required by the synthdid package. Given the outcome, treatment and control units and pretreatment periods, a synthetic control is created and treatment effect is estimated with synthdid_estimate function. # Convert the data into a matrix setup = panel.matrices(dt, unit = &#39;country&#39;, time = &#39;period&#39;, outcome = &#39;revenue&#39;, treatment = &#39;treat&#39;) # Estimate treatment effect using SynthDiD tau.hat = synthdid_estimate(setup$Y, setup$N0, setup$T0) print(summary(tau.hat)) To make inference, we also need to calculate the standard errors. I use jacknife method as I have more than one treated units. Placebo method is the only option if you have one treatment unit. Given the standard errors, I also calculate the 95% confidence interval for the treatment effect. I will report these in the figure below. When there are multiple treated units (more than one unit that received the treatment or intervention), one common approach to estimating standard errors is using the jackknife method. The jackknife method is a resampling technique where each observation (in this case, each treated unit) is systematically omitted from the dataset, and the analysis is repeated each time to estimate the variance of the treatment effect. This provides a robust estimate of the standard errors that accounts for the potential variability across different treated units. On the other hand, if there is only one treated unit (a single unit that received the treatment), using the jackknife method becomes impractical because there are not enough units to systematically leave out and still perform meaningful resampling. In such cases, the placebo method becomes a viable option. The placebo method involves creating placebo or synthetic treated units that mimic the characteristics of the treated unit but did not actually receive the treatment. By comparing the outcomes of the actual treated unit with those of the synthetic placebo units, researchers can estimate the variability and potential impact of the treatment effect more accurately. Therefore, the choice between the jackknife method and the placebo method depends on the number of treated units available for analysis within the synthetic control framework. Multiple treated units allow for the application of the jackknife method, whereas a single treated unit necessitates the use of the placebo method to estimate standard errors and make reliable inferences about the treatment effect. # Calculate standard errors se = sqrt(vcov(tau.hat, method=&#39;jackknife&#39;)) te_est &lt;- sprintf(&#39;Point estimate for the treatment effect: %1.2f&#39;, tau.hat) CI &lt;- sprintf(&#39;95%% CI (%1.2f, %1.2f)&#39;, tau.hat - 1.96 * se, tau.hat + 1.96 * se) # Plot treatment effect estimates plot(tau.hat) plot(tau.hat, se.method=&#39;jackknife&#39;) In the image below, the estimation results are displayed. Observe how the treated countries and the synthetic control exhibit fairly parallel trends on average (it might not look like a perfect parallel trends but that is not necessary for the sake of this example). The average for treated countries is more variable, primarily due to the presence of only three such countries, resulting in less smooth trends. Transparent gray lines represent different control countries. Following the treatment in period 20, a decline in revenue is observed in the treated countries, estimated to be 0.51 million EUR as indicated in the graph. This means that the new regulation has a negative impact on our company‚Äôs revenues and necessary actions should be taken to prevent further declines. # Check the number of treatment and control countries to report num_treated &lt;- length(unique(dt[treat==1]$country)) num_control &lt;- length(unique(dt$country))-num_treated # Create spaghetti plot with top 10 control units top.controls = synthdid_controls(tau.hat)[1:10, , drop=FALSE] plot(tau.hat, spaghetti.units=rownames(top.controls), trajectory.linetype = 1, line.width=.75, trajectory.alpha=.9, effect.alpha=.9, diagram.alpha=1, onset.alpha=.9, ci.alpha = .3, spaghetti.line.alpha =.2, spaghetti.label.alpha = .1, overlay = 1) + labs(x = &#39;Period&#39;, y = &#39;Revenue&#39;, title = &#39;Estimation Results&#39;, subtitle = paste0(te_est, &#39;, &#39;, CI, &#39;.&#39;), caption = paste0(&#39;The number of treatment and control units: &#39;, num_treated, &#39; and &#39;, num_control, &#39;.&#39;)) Let‚Äôs plot the weights use to estimate the synthetic control. # Plot control unit contributions synthdid_units_plot(tau.hat, se.method=&#39;jackknife&#39;) + labs(x = &#39;Country&#39;, y = &#39;Treatment effect&#39;, caption = &#39;The black horizontal line shows the actual effect; the gray ones show the endpoints of a 95% confidence interval.&#39;) ggsave(&#39;../figures/unit_weights.png&#39;) In the image below, you can observe how each country is weighted to construct the synthetic control. The treatment effects differ based on the untreated country selected as the control unit. # Check for pre-treatment parallel trends plot(tau.hat, overlay=1, se.method=&#39;jackknife&#39;) ggsave(&#39;../figures/results_simple.png&#39;) # Check the number of treatment and control countries to report num_treated &lt;- length(unique(dt[treat==1]$country)) num_control &lt;- length(unique(dt$country))-num_treated # Create spaghetti plot with top 10 control units top.controls = synthdid_controls(tau.hat)[1:10, , drop=FALSE] plot(tau.hat, spaghetti.units=rownames(top.controls), trajectory.linetype = 1, line.width=.75, trajectory.alpha=.9, effect.alpha=.9, diagram.alpha=1, onset.alpha=.9, ci.alpha = .3, spaghetti.line.alpha =.2, spaghetti.label.alpha = .1, overlay = 1) + labs(x = &#39;Period&#39;, y = &#39;Revenue&#39;, title = &#39;Estimation Results&#39;, subtitle = paste0(te_est, &#39;, &#39;, CI, &#39;.&#39;), caption = paste0(&#39;The number of treatment and control units: &#39;, num_treated, &#39; and &#39;, num_control, &#39;.&#39;)) ggsave(&#39;../figures/results.png&#39;) fe &lt;- feols(revenue~treat, dt, cluster = &#39;country&#39;, panel.id = &#39;country&#39;, fixef = c(&#39;country&#39;, &#39;period&#39;)) summary(fe) 6.11.11 Conclusion Now that we understand more about SynthDiD let‚Äôs talk about pros and cons of this method. There are some advantages and disadvantages to SynthDiD like every method. Here are some pros and cons to keep in mind when getting started with this method. 6.11.11.1 Advantages of SynthDiD method: The synthetic control method is usually used for a few treated and control units and needs long, balanced data before treatment. SynthDiD, on the other hand, works well even with a short data period before treatment, unlike the synthetic control method [4]. This method is being preferred especially because it doesn‚Äôt have a strict parallel trends assumption (PTA) requirement like DiD. SynthDiD guarantees a suitable quantity of control units, considers possible pre-intervention patterns, and may accommodate a degree of endogenous treatment timing [4]. 6.11.11.2 Disadvantages of SynthDiD method: Can be computationally expensive (even with only one treated group/block). Requires a balanced panel (i.e., you can only use units observed for all time periods) and that the treatment timing is identical for all treated units. Requires enough pre-treatment periods for good estimation, so, if you don‚Äôt have enough pre-treatment period might be better to use just the regular DiD. Computing and comparing the average treatment effects for subgroups is tricky. One option is to split the sample into subgroups and compute the average treatment effects for each subgroup. Implementing SynthDiD where the treatment timing varies might be tricky. In the case of staggered treatment timing, as one solution, one can estimate the average treatment effect for each treatment cohort and then aggregate cohort-specific average treatment effects to an overall average treatment effects. H ere are also some other points that you might want to know when getting started. Things to note: SynthDiD employs regularized ridge regression (L2) while ensuring that the resulting weights have a sum of one. In the process of pretreatment matching, SynthDiD tries to determine the average treatment effect across the entire sample. This approach might cause individual time period estimates to be less precise. Nonetheless, the overall average yields an unbiased evaluation. The standard errors for the treatment effects are estimated with jacknife or if a cohort has only one treated unit with placebo method. The estimator is considered consistent and asymptotically normal, given that the combination of the number of control units and pretreatment periods is sufficiently large relative to the combination of the number of treated units and posttreatment periods. In practice, pre-treatment variables play a minor role in Synthetic DiD, as lagged outcomes hold more predictive power, making the treatment of these variables less critical. Conclusion In this blog post, I introduce the SynthDiD method and discuss its relationship with traditional DiD and SCM. SynthDiD combines the strengths of both SCM and DiD, allowing for causal inference with large panels even when the pretreatment period is short. I demonstrate the method using the synthdid package in R. Although it has several advantages, such as not requiring a strict parallel trends assumption, it also has drawbacks, like being computationally expensive and requiring a balanced panel. Overall, SynthDiD is a valuable tool for researchers interested in estimating causal effects using observational data, providing an alternative to traditional DiD and SCM methods. 6.12 Doubly Robust Difference in Differences DRDID website This package estimates locally efficient doubly robust difference-in-differences estimator for the ATT. Implements the locally efficient doubly robust difference-in-differences (DiD) estimators for the average treatment effect proposed by Sant‚ÄôAnna and Zhao (2020). The estimator combines inverse probability weighting and outcome regression estimators (also implemented in the package) to form estimators with more attractive statistical properties. Two different estimation methods can be used to estimate the nuisance functions. The proposed estimators are consistent if either (but not necessarily both) a propensity score or outcome regression working models are correctly specified. 6.12.1 Details This estimator makes use of a logistic propensity score model for the probability of being in the treated group, and of a linear regression model for the outcome evolution among the comparison units. When only stationary repeated cross-section data are available (panel = FALSE), the drdid function implements the locally efficient doubly robust difference-in-differences (DiD) estimator for the average treatment effect on the treated (ATT) defined in equation (3.4) in Sant‚ÄôAnna and Zhao (2020). This estimator makes use of a logistic propensity score model for the probability of being in the treated group, and of (separate) linear regression models for the outcome of both treated and comparison units, in both pre and post-treatment periods. In short, the propensity score parameters are estimated using the inverse probability tilting estimator proposed by Graham, Pinto and Pinto (2012), and the outcome regression coefficients are estimated using weighted least squares,where the weights depend on the propensity score estimates; see Sant‚ÄôAnna and Zhao (2020) for details. 6.13 Twoway Fixed Effects with Differential Timing \\(y_{it} = \\alpha_0 + \\delta D_{it} + X_{it} + \\alpha_i + \\alpha_t + \\epsilon_{it}\\) When researchers estimate this regression these days, they usually use the linear fixed-effects model. These linear panel models have gotten the nickname ‚Äútwoway fixed effects‚Äù because they include both time fixed effects and unit fixed effects. 6.14 Bacon Decomposition The punchline of the Bacon decomposition theorem is that the twoway fixed effects estimator is a weighted average of all potential 2 x 2 DD estimates where weights are both based on group sizes and variance in treatment. 6.14.1 Overview Bacon Decomposition is a method introduced by Goodman-Bacon (2018) for decomposing the overall treatment effect estimated by a Two-Way Fixed Effects (TWFE) regression model in the context of Difference-in-Differences (DiD) settings with variation in treatment timing. The key insight from this decomposition is that the TWFE estimate in such settings can be understood as a weighted average of all possible 2x2 DiD estimates that can be constructed from the data. This decomposition helps identify the sources of bias, especially when treatment effects are heterogeneous or when there are differential pre-treatment trends. 6.14.2 Key Concepts Two-Way Fixed Effects (TWFE) Models: TWFE models are commonly used in DiD analyses to account for time-invariant differences between units and common shocks over time by including unit and time fixed effects. The model typically looks like: \\[ Y_{it} = \\alpha_i + \\lambda_t + \\beta D_{it} + \\epsilon_{it} \\] where \\(Y_{it}\\) is the outcome for unit \\(i\\) at time \\(t\\), \\(\\alpha_i\\) are unit fixed effects, \\(\\lambda_t\\) are time fixed effects, \\(D_{it}\\) is the treatment indicator, and \\(\\beta\\) is the treatment effect. Variation in Treatment Timing: In many DiD applications, units receive treatment at different times rather than simultaneously. This leads to multiple possible comparisons between treated and control units at different points in time. Bacon Decomposition: The decomposition breaks down the overall TWFE estimate into a weighted average of all possible 2x2 DiD estimates. Each of these estimates compares treated and untreated units in specific periods. The decomposition reveals that the overall estimate is influenced by: Comparisons between early-treated and late-treated units. Comparisons between treated and untreated units at different times. Comparisons within treated units (pre- and post-treatment). 6.14.3 Components of Bacon Decomposition Early vs.¬†Late Treated Units: Comparing units treated early with those treated later. This can introduce bias if there are differential trends among these groups. Treated vs.¬†Untreated Units: Standard DiD comparison where treated units are compared to untreated ones, assuming common trends between them. Within-Unit Comparisons: Comparing outcomes within the same unit before and after treatment. 6.14.4 Formula for Decomposition The overall TWFE estimate \\(\\hat{\\beta}_{TWFE}\\) can be decomposed as: \\[ \\hat{\\beta}_{TWFE} = \\sum_{k} w_k \\hat{\\beta}_k \\] where \\(\\hat{\\beta}_k\\) are the 2x2 DiD estimates, and \\(w_k\\) are the weights that depend on the relative timing of treatment and the distribution of the treated and control units over time. 6.14.5 Implications and Interpretation Heterogeneous Treatment Effects: When treatment effects vary over time or across units, the TWFE estimate can be biased. Bacon decomposition helps identify how much of the TWFE estimate is driven by comparisons that might be invalid due to treatment effect heterogeneity. Differential Pre-treatment Trends: If treated and control units follow different pre-treatment trends, this can also bias the TWFE estimate. Bacon decomposition highlights which comparisons are most affected by such trends. Policy Implications: Understanding the sources of bias through Bacon decomposition can inform better policy evaluations by revealing the need for more appropriate methods or robustness checks in the presence of staggered treatment adoption. 6.14.6 Example Consider a study evaluating the impact of a new education policy implemented in different schools at different times. Using a TWFE model, the overall treatment effect might be estimated as: \\[ \\hat{\\beta}_{TWFE} = 0.5 \\] Applying Bacon decomposition, we might find that: - Comparisons between schools treated in 2018 and those treated in 2020 contribute \\(0.3\\) to the estimate. - Comparisons between treated schools and untreated schools contribute \\(0.1\\). - Comparisons within schools before and after treatment contribute \\(0.1\\). If early-treated schools experienced a different trend in outcomes compared to late-treated schools, this could explain the significant contribution from early vs.¬†late comparisons, highlighting potential bias in the overall estimate. 6.14.7 Conclusion Bacon decomposition provides a nuanced understanding of the TWFE estimates in DiD settings with staggered treatment adoption. By breaking down the overall estimate into its constituent comparisons, researchers can identify and address potential biases due to heterogeneous treatment effects and differential trends, leading to more accurate and reliable causal inferences. "],["synthetic-control-method-scm.html", "Chapter 7 Synthetic Control Method (SCM) 7.1 AI Summary", " Chapter 7 Synthetic Control Method (SCM) The synthetic control method is a statistical technique that creates a ‚Äúsynthetic‚Äù control group by combining multiple control units that are similar to the treatment unit in all relevant characteristics. The synthetic control group is constructed to match the pre-treatment outcomes of the treated unit as closely as possible. The treatment effect is then estimated by comparing the post-treatment outcomes of the treated unit to those of the synthetic control group. Synthetic control allow us to do causal inference when we have as few as one treated unit and many control units and we observe them over time. The idea is simple: combine untreated units so that they mimic the behavior of the treated unit as closely as possible, without the treatment. Then use this ‚Äúsynthetic unit‚Äù as a control. The method first introduced by Abadie, Diamond and Hainmueller (2010) and has been called ‚Äúthe most important innovation in the policy evaluation literature in the last few years‚Äù. Moreover, it is widely used in the industry because of its simplicity and interpretability. 7.1 AI Summary 7.1.1 Synthetic Control Method (SCM) What is the Synthetic Control Method? SCM is a statistical method used to evaluate the effect of an intervention or treatment when a randomized control trial is not feasible. It constructs a synthetic version of the treated unit using a weighted combination of control units. 7.1.1.1 Real-World Examples Can you provide a real-world example where SCM has been used? SCM was famously used to evaluate the economic impact of California‚Äôs tobacco control program in 1988. By constructing a synthetic California from other states with similar pre-intervention characteristics, researchers estimated the program‚Äôs impact on cigarette sales. 7.1.1.2 Assumptions What are the key assumptions of SCM? No hidden confounders: Assumes that all relevant variables are included in the model. No anticipation effect: Assumes that the intervention did not affect the units before its implementation. Convex hull: Assumes that the treated unit lies within the convex hull of the donor pool. 7.1.1.3 Validation Methods How can you validate the results of an SCM? Placebo Tests: Apply the method to units that were not exposed to the intervention to check if significant effects are falsely detected. Leave-One-Out Cross-Validation: Assess the impact by leaving one control unit out of the synthetic control and observing if the results significantly change. Pre-Intervention Fit: Ensure the synthetic control closely tracks the treated unit before the intervention. 7.1.1.4 Robustness Checks What robustness checks can you perform for SCM? Sensitivity Analysis: Test the robustness of results to changes in the weights of control units. In-Sample Prediction: Check if the synthetic control accurately predicts the outcome for the treated unit in the pre-intervention period. Alternative Specifications: Use different sets of predictor variables to construct the synthetic control and see if the results hold. 7.1.1.5 Violating Situations What situations could violate the assumptions of SCM? Unobserved Confounders: If there are important variables that influence both the intervention and the outcome but are not included in the model. Anticipation Effects: If the units change their behavior in anticipation of the intervention. Poor Fit Pre-Intervention: If the synthetic control does not closely match the treated unit‚Äôs pre-intervention trends. 7.1.1.6 Detailed Explanation of a Key Concept Constructing a Synthetic Control: Choosing Donor Pool: Select units that were not exposed to the intervention but are similar to the treated unit. Variable Selection: Identify predictor variables that are strongly related to the outcome variable. Weighting Process: Assign weights to control units such that the weighted combination closely resembles the treated unit in terms of pre-intervention characteristics. 7.1.1.7 Example Implementation Case Study: Impact of a New Policy on Unemployment Rates Objective: Assess the impact of a new job training program on unemployment rates in a particular state. Steps: Select Donor Pool: Choose similar states without the job training program. Identify Predictors: Variables such as GDP growth, historical unemployment rates, industrial composition. Construct Synthetic Control: Calculate weights for each control state to create a synthetic state that mirrors the treated state‚Äôs pre-intervention unemployment rate. Analyze Results: Compare post-intervention unemployment rates between the treated state and its synthetic counterpart. 7.1.2 Placebo tests Placebo tests are a crucial part of validating the results of a Synthetic Control Method (SCM) analysis. They help ensure that the observed effect of an intervention is not due to random chance but rather to the intervention itself. Here‚Äôs a detailed explanation of how placebo tests are performed and interpreted: 7.1.2.1 Concept A placebo test involves applying the synthetic control methodology to units that were not subjected to the intervention. The idea is to check whether similar treatment effects are observed in these non-treated units, which should not exhibit a significant effect if the synthetic control method is working correctly. 7.1.2.2 Steps for Performing Placebo Tests Identify Non-Treated Units: Select units from the donor pool that were not exposed to the intervention. Create Placebo Interventions: Assign a placebo intervention to each non-treated unit. This means arbitrarily selecting a point in time as the ‚Äúintervention‚Äù date for these units. Construct Synthetic Controls for Placebo Units: For each non-treated unit with a placebo intervention, construct a synthetic control using the same pre-intervention period and predictor variables as for the actual treated unit. Compare Outcomes: Calculate the difference between the outcomes of the non-treated units and their synthetic controls in the post-intervention period. 7.1.2.3 Interpreting Placebo Tests Distribution of Placebo Effects: Examine the distribution of the placebo effects across all non-treated units. This helps determine if the observed effect for the treated unit is unusually large compared to the placebo effects. Significance Testing: If the effect observed in the actual treated unit is significantly larger than the effects observed in the placebo tests, this suggests that the effect is likely due to the intervention rather than random variation. Graphical Analysis: Plot the post-intervention gaps (difference between actual and synthetic outcomes) for both the treated unit and the placebo units. If the treated unit‚Äôs gap stands out from the placebo gaps, it strengthens the evidence for a causal effect of the intervention. 7.1.2.4 Example Imagine you are evaluating the impact of a new job training program introduced in State A in 2010. Select Donor Pool: Choose similar states (B, C, D, etc.) that did not introduce the job training program. Placebo Interventions: Assign placebo intervention years for states B, C, and D (e.g., 2010 for all states). Construct Synthetic Controls: For each state, construct a synthetic control using the same method applied to State A. Analyze Results: Compare the post-2010 unemployment rates of each state with their respective synthetic controls. If State A shows a significant drop in unemployment rates compared to its synthetic control, and the placebo tests for states B, C, and D show no significant changes, this strengthens the case that the job training program had a real impact on State A. Graphical Evidence: Plot the unemployment rate differences (treated - synthetic) for State A and placebo states. The graph should show a distinct deviation for State A after 2010, while the placebo states‚Äô lines should remain relatively flat. 7.1.2.5 Robustness By showing that non-treated units (placebo units) do not exhibit significant changes in the outcome, while the treated unit does, you provide strong evidence that the intervention (job training program) is responsible for the observed effect. 7.1.2.6 Conclusion Placebo tests are an essential validation tool in SCM. They help rule out the possibility that the observed effect is due to random variation or other factors unrelated to the intervention. By carefully constructing and analyzing placebo tests, you can enhance the credibility of your SCM analysis and provide robust evidence for causal inference. 7.1.3 Leave-One-Out Cross-Validation Leave-One-Out Cross-Validation (LOO-CV) is a robustness check used in the context of the Synthetic Control Method (SCM) to assess the sensitivity and stability of the results. This method systematically removes one unit from the donor pool at a time to examine how the exclusion affects the construction of the synthetic control and the estimated treatment effect. 7.1.3.1 Steps for Leave-One-Out Cross-Validation in SCM Initial Synthetic Control Construction: First, construct the synthetic control for the treated unit using the full donor pool of control units. Iterative Exclusion: Iteratively exclude one control unit at a time from the donor pool and reconstruct the synthetic control without the excluded unit. For a donor pool with \\(n\\) control units, this results in \\(n\\) different synthetic controls, each missing one different control unit. Calculate Treatment Effect: For each iteration, calculate the treatment effect as the difference between the outcome of the treated unit and the outcome of the synthetic control constructed without the excluded unit. Compare and Analyze: Compare the treatment effects from all iterations to assess the stability and robustness of the original treatment effect. 7.1.3.2 Interpreting Leave-One-Out Cross-Validation Consistency of Results: If the estimated treatment effects from all iterations are similar to the original estimate (constructed with the full donor pool), it indicates that the result is robust and not overly dependent on any single control unit. Sensitivity to Exclusion: If excluding certain control units significantly changes the estimated treatment effect, this suggests that the result may be sensitive to the composition of the donor pool. Identifying such units can provide insights into which control units are driving the results. Graphical Analysis: Plot the treatment effects from each iteration to visually inspect the variability. Ideally, the effects should cluster closely around the original estimate. 7.1.3.3 Example Consider an evaluation of the impact of a smoking ban on public health outcomes in City A. The donor pool includes Cities B, C, D, E, and F. Construct Full Synthetic Control: Using Cities B, C, D, E, and F, create a synthetic control for City A and calculate the treatment effect on public health outcomes. Iteratively Exclude Cities: Exclude City B, reconstruct the synthetic control using Cities C, D, E, and F, and calculate the new treatment effect. Repeat this process for Cities C, D, E, and F. Compare Results: Compare the treatment effect estimates obtained from each exclusion to the original estimate. Analysis: If the treatment effect estimates are similar regardless of which city is excluded, the result is robust. If excluding a particular city (e.g., City D) results in a significantly different treatment effect, this indicates that City D has a substantial influence on the synthetic control. 7.1.4 Detailed Example 7.1.4.1 Original Estimate: Full Donor Pool (Cities B, C, D, E, F): Treatment effect is a 5% reduction in public health issues. 7.1.4.2 Leave-One-Out Estimates: Excluding City B (Cities C, D, E, F): Treatment effect is a 4.8% reduction. Excluding City C (Cities B, D, E, F): Treatment effect is a 5.2% reduction. Excluding City D (Cities B, C, E, F): Treatment effect is a 3.0% reduction. Excluding City E (Cities B, C, D, F): Treatment effect is a 5.1% reduction. Excluding City F (Cities B, C, D, E): Treatment effect is a 4.9% reduction. 7.1.4.3 Analysis: The estimates are generally close to the original 5% reduction, except when City D is excluded, which yields a 3% reduction. This suggests that City D has a significant influence on the synthetic control and should be further examined to understand why its exclusion changes the result. 7.1.5 Conclusion Leave-One-Out Cross-Validation is a powerful tool to test the robustness and reliability of the synthetic control method‚Äôs results. By systematically excluding each control unit, it helps identify how dependent the estimated treatment effect is on individual control units. This ensures that the results are not driven by any single unit and provides a deeper understanding of the underlying data and the robustness of the findings. 7.1.6 Data The Synthetic Control Method (SCM) is a sophisticated tool used in causal inference to estimate the effect of an intervention or treatment when a randomized control trial is not feasible. For SCM to be effectively implemented, certain data settings and requirements need to be met. Here‚Äôs a detailed explanation of these requirements: 7.1.7 Data Setting Treated Unit: The unit (e.g., a region, city, country, organization) that is exposed to the intervention or treatment. Control Units (Donor Pool): A set of similar units that were not exposed to the intervention. These units will be used to construct the synthetic control. Outcome Variable: The variable of interest that the intervention is expected to affect. This could be economic indicators, health outcomes, crime rates, etc. Predictor Variables: A set of variables that are believed to influence the outcome variable. These predictors are used to create the synthetic control and should be available for both the treated and control units. Time Period: Data should cover a sufficiently long period before and after the intervention. The pre-intervention period is used to match the treated unit with the synthetic control, and the post-intervention period is used to evaluate the treatment effect. 7.1.8 Requirements for Synthetic Control Method Similarity in Pre-Intervention Period: The treated unit and the control units should have similar trends in the outcome variable and predictor variables during the pre-intervention period. This similarity ensures that the synthetic control can accurately replicate the treated unit‚Äôs trajectory had the intervention not occurred. Sufficient Number of Control Units: A reasonably large donor pool is necessary to construct a reliable synthetic control. The control units should be comparable to the treated unit in terms of the pre-intervention characteristics. Availability of Data: Detailed and reliable data for both the treated unit and the control units over the entire time period (both pre- and post-intervention) is crucial. Missing data can bias the results. No Anticipation Effect: It is assumed that the intervention does not affect the outcome variable before its actual implementation. This ensures that any observed changes in the outcome variable after the intervention can be attributed to the intervention itself. Convex Hull Condition: The treated unit should lie within the convex hull of the control units in the space of the predictor variables. This condition ensures that a weighted combination of control units can adequately represent the treated unit. Stationarity of Relationship: The relationship between predictor variables and the outcome variable should remain stable over time. If this relationship changes significantly, the synthetic control constructed from pre-intervention data might not be valid for post-intervention analysis. 7.1.9 Data Requirements Summary Outcome Data: Continuous data on the outcome variable for both the treated and control units over the entire study period. Predictor Data: Data on several predictor variables that influence the outcome variable. These should be available for both the treated and control units. Pre-Intervention and Post-Intervention Data: Sufficient data points before the intervention to accurately match the treated unit with the synthetic control, and enough data points after the intervention to assess its impact. 7.1.10 Practical Considerations Data Quality: High-quality, consistent data is crucial. Any inaccuracies or inconsistencies in the data can lead to unreliable results. Selection of Control Units: The control units should be selected based on their similarity to the treated unit and their relevance to the study. Including irrelevant or highly dissimilar units can distort the synthetic control. Choice of Predictors: The predictor variables should be carefully chosen based on theoretical and empirical understanding of what drives the outcome variable. Including irrelevant predictors can reduce the accuracy of the synthetic control. 7.1.11 Example Consider evaluating the impact of a new traffic policy in City A on reducing traffic accidents. Treated Unit: City A where the traffic policy was implemented. Control Units: A set of similar cities (Cities B, C, D, etc.) where the policy was not implemented. Outcome Variable: Number of traffic accidents. Predictor Variables: Variables such as population density, average income, road infrastructure quality, historical traffic accident rates, and other socio-economic factors. Time Period: Data covering several years before and after the implementation of the traffic policy. 7.1.12 Conclusion The success of the Synthetic Control Method relies heavily on the quality and appropriateness of the data used. Ensuring that the treated and control units are similar, having a sufficient number of control units, and having detailed and accurate data for the relevant time periods are crucial for producing reliable and valid estimates of the treatment effect. By carefully selecting the outcome and predictor variables and ensuring robust data quality, SCM can provide valuable insights into the causal effects of interventions. 7.1.12.1 Setting We assume that for a panel of i.i.d. subjects over time. we observed a set of variables that includes: a treatment assignment (treated) a response (revenue) a feature vector (population, density, employment and GDP) Moreover, one unit (Miami in our case) is treated at time (2013 in our case). We distinguish time periods before treatment and time periods after treatment. Crucially, treatment is not randomly assigned, therefore a difference in means between the treated unit(s) and the control group is not an unbiased estimator of the average treatment effect. Some Resources: (Synthetic Difference-in-Differences)[https://matheusfacure.github.io/python-causality-handbook/25-Synthetic-Diff-in-Diff.html] 7.1.13 Synthetic DID Advantages of SynthDiD method: The synthetic control method is usually used for a few treated and control units and needs long, balanced data before treatment. SynthDiD, on the other hand, works well even with a short data period before treatment, unlike the synthetic control method [4]. This method is being preferred especially because it doesn‚Äôt have a strict parallel trends assumption (PTA) requirement like DiD. SynthDiD guarantees a suitable quantity of control units, considers possible pre-intervention patterns, and may accommodate a degree of endogenous treatment timing [4]. Disadvantages of SynthDiD method: Can be computationally expensive (even with only one treated group/block). Requires a balanced panel (i.e., you can only use units observed for all time periods) and that the treatment timing is identical for all treated units. Requires enough pre-treatment periods for good estimation, so, if you don‚Äôt have enough pre-treatment period might be better to use just the regular DiD. Computing and comparing the average treatment effects for subgroups is tricky. One option is to split the sample into subgroups and compute the average treatment effects for each subgroup. Implementing SynthDiD where the treatment timing varies might be tricky. In the case of staggered treatment timing, as one solution, one can estimate the average treatment effect for each treatment cohort and then aggregate cohort-specific average treatment effects to an overall average treatment effects. Here are also some other points that you might want to know when getting started. Things to note: SynthDiD employs regularized ridge regression (L2) while ensuring that the resulting weights have a sum of one. In the process of pretreatment matching, SynthDiD tries to determine the average treatment effect across the entire sample. This approach might cause individual time period estimates to be less precise. Nonetheless, the overall average yields an unbiased evaluation. The standard errors for the treatment effects are estimated with jacknife or if a cohort has only one treated unit with placebo method. The estimator is considered consistent and asymptotically normal, given that the combination of the number of control units and pretreatment periods is sufficiently large relative to the combination of the number of treated units and posttreatment periods. In practice, pre-treatment variables play a minor role in Synthetic DiD, as lagged outcomes hold more predictive power, making the treatment of these variables less critical. Conclusion In this blog post, I introduce the SynthDiD method and discuss its relationship with traditional DiD and SCM. SynthDiD combines the strengths of both SCM and DiD, allowing for causal inference with large panels even when the pretreatment period is short. I demonstrate the method using the synthdid package in R. Although it has several advantages, such as not requiring a strict parallel trends assumption, it also has drawbacks, like being computationally expensive and requiring a balanced panel. Overall, SynthDiD is a valuable tool for researchers interested in estimating causal effects using observational data, providing an alternative to traditional DiD and SCM methods. Robustness checks and validation methods are essential aspects of evaluating the reliability and credibility of empirical research findings, including those derived from the Synthetic Control Method (SCM). Although they are related and sometimes overlap, they serve distinct purposes in the research process. Here‚Äôs a detailed explanation of the differences between them and why each is important: 7.1.14 Robustness Checks Definition: Robustness checks are procedures used to assess the sensitivity and stability of research findings to various assumptions, model specifications, and data perturbations. The goal is to determine whether the results hold under different conditions and to identify any potential weaknesses in the analysis. Purpose: - Assess Stability: Ensure that the findings are not unduly influenced by specific choices made in the analysis (e.g., selection of control units, predictor variables). - Identify Key Drivers: Determine which aspects of the model or data are most influential in driving the results. - Evaluate Generalizability: Check whether the results are consistent across different sub-samples or alternative model specifications. Examples of Robustness Checks: 1. Leave-One-Out Cross-Validation: Assess how the results change when each control unit is excluded one at a time. 2. Alternative Model Specifications: Test different sets of predictor variables or alternative functional forms of the model. 3. Sensitivity Analysis: Evaluate the impact of small changes in the data or assumptions on the estimated treatment effect. 4. In-Sample Prediction: Check the model‚Äôs performance in predicting the outcome variable within the pre-intervention period. 7.1.15 Validation Methods Definition: Validation methods are procedures used to confirm that the analytical approach and findings are credible and correctly specified. The goal is to ensure that the methodology accurately captures the causal relationship of interest and that the results are not artifacts of methodological flaws. Purpose: - Establish Credibility: Demonstrate that the research design and methods are sound and that the findings are credible. - Detect Biases: Identify and correct any biases or errors in the analysis that could distort the results. - Provide Evidence for Causal Claims: Strengthen the argument that the observed effects are truly caused by the intervention rather than other factors. Examples of Validation Methods: 1. Placebo Tests: Apply the methodology to units or time periods where no intervention occurred to ensure that no significant effects are detected. 2. Pre-Intervention Fit: Ensure that the synthetic control closely matches the treated unit‚Äôs trajectory in the pre-intervention period. 3. Falsification Tests: Use outcomes that should not be affected by the intervention to check for false positives. 4. External Validation: Compare the findings with results from other studies or alternative methodologies to check for consistency. 7.1.15.1 Self Driving Cars Experiment (Source)[https://matteocourthoud.github.io/post/synth/] Suppose you were a ride-sharing platform and you wanted to test the effect of self-driving cars in your fleet. As you can imagine, there are many limitations to running an AB/test for this type of feature. First of all, it‚Äôs complicated to randomize individual rides. Second, it‚Äôs a very expensive intervention. Third, and statistically most important, you cannot run this intervention at the ride level. The problem is that there are spillover effects from treated to control units: if indeed self-driving cars are more efficient, it means that they can serve more customers in the same amount of time, reducing the customers available to normal drivers (the control group). This spillover contaminates the experiment and prevents a causal interpretation of the results. For all these reasons, we select only one city. Given the synthetic vibe of the article we cannot but select‚Ä¶ (drum roll)‚Ä¶ Miami! We have information on the largest 46 U.S. cities for the period 2002-2019. The panel is balanced, which means that we observe all cities for all time periods. Self-driving cars were introduced in 2013. As expected, the groups are not balanced: Miami is more densely populated, poorer, larger and has lower employment rate than the other cities in the US in our sample. We are interested in understanding the impact of the introduction of self-driving cars on revenue. One initial idea could be to analyze the data as we would in an A/B test, comparing control and treatment group. We can estimate the treatment effect as a difference in means in revenue between the treatment and control group, after the introduction of self-driving cars. "],["instrumental-variables-iv.html", "Chapter 8 Instrumental Variables (IV) 8.1 Key Concepts 8.2 Difference Between Instrumental Variable (IV) Method and Two-Stage Least Squares (2SLS) Method 8.3 Homogenous Treatment Effect 8.4 Heterogenous Treatment Effect", " Chapter 8 Instrumental Variables (IV) Instrumental Variables (IV) is a method used in econometrics and statistics to estimate causal relationships when controlled experiments are not feasible and there is an endogeneity problem‚Äîtypically due to omitted variable bias, measurement error, or simultaneity. 8.1 Key Concepts Endogeneity: Occurs when an explanatory variable is correlated with the error term. This correlation violates the OLS assumption that the regressors are exogenous, leading to biased and inconsistent estimates. Instrumental Variable: A variable (or set of variables) that is correlated with the endogenous explanatory variable but uncorrelated with the error term. It helps to isolate the exogenous variation in the endogenous explanatory variable. 8.1.1 Requirements for a Valid Instrument A valid instrument must satisfy two key conditions: 1. Relevance: The instrument must be correlated with the endogenous explanatory variable. - Mathematically: $ (Z, X) $ - This ensures that the instrument can explain some of the variations in the endogenous regressor. Exogeneity: The instrument must be uncorrelated with the error term in the structural equation. Mathematically: $ (Z, u) = 0 $ This ensures that the instrument does not suffer from the same endogeneity problem as the endogenous regressor. 8.1.2 The IV Estimation Process The IV estimation is typically carried out in two stages, known as Two-Stage Least Squares (2SLS): First Stage: Regress the endogenous variable on the instrument(s) to obtain the predicted values of the endogenous variable. Equation: \\(X = \\pi_0 + \\pi_1 Z + v\\) Obtain the fitted values \\(\\hat{X}\\) from this regression. Second Stage: Regress the dependent variable on the predicted values from the first stage. Equation: \\(Y = \\beta_0 + \\beta_1 \\hat{X} + \\epsilon\\) The coefficient \\(\\beta_1\\) from this regression is the IV estimate of the effect of \\(X\\) on \\(Y\\). 8.1.3 Example 8.1.3.1 Context: Education and Earnings Endogeneity Problem: In estimating the effect of education on earnings, the variable ‚Äúyears of education‚Äù might be endogenous due to omitted variables like ability or family background. Instrument: Distance to the nearest college could be used as an instrument for education. Relevance: Distance to college affects the likelihood of obtaining more education. Exogeneity: Distance to college is unlikely to be directly related to individual earnings, apart from its effect on education. 8.1.3.2 Steps: First Stage: Regress ‚Äúyears of education‚Äù on ‚Äúdistance to college‚Äù. Equation: \\(\\text{Education} = \\pi_0 + \\pi_1 (\\text{Distance to College}) + v\\) Obtain the fitted values \\(\\widehat{\\text{Education}}\\). Second Stage: Regress ‚Äúearnings‚Äù on the fitted values from the first stage. Equation: \\(\\text{Earnings} = \\beta_0 + \\beta_1 (\\widehat{\\text{Education}}) + \\epsilon\\) The coefficient \\(\\beta_1\\) is the IV estimate of the effect of education on earnings. 8.1.4 Assumptions and Considerations Instrument Strength: Weak instruments can lead to biased estimates even in large samples. The relevance condition must be strong enough. Test: First stage F-statistic (rule of thumb: F &gt; 10). As F gets larger, then bias goes 0. Over-Identification Test: When there are multiple instruments, it‚Äôs possible to test for the exogeneity of instruments using over-identification tests like the Sargan or Hansen J test. Single vs.¬†Multiple Instruments: Using multiple instruments can improve the efficiency of the IV estimator, provided all instruments are valid. 8.1.5 Advantages of IV Provides consistent estimates in the presence of endogeneity. Can be used when randomized experiments are not feasible. 8.1.6 Disadvantages of IV Finding valid instruments can be difficult. Weak instruments can lead to biased and imprecise estimates. Interpretation of the IV estimate can be less straightforward, often reflecting a Local Average Treatment Effect (LATE) rather than the Average Treatment Effect (ATE). 8.1.7 Conclusion Instrumental Variables (IV) is a powerful method for addressing endogeneity in observational studies, allowing researchers to estimate causal effects more accurately. Understanding the conditions for valid instruments and the proper application of the 2SLS method is crucial for leveraging this technique effectively. For your interview, be prepared to discuss the theory, assumptions, applications, and potential pitfalls of IV estimation. 8.2 Difference Between Instrumental Variable (IV) Method and Two-Stage Least Squares (2SLS) Method The terms ‚ÄúInstrumental Variable (IV) method‚Äù and ‚ÄúTwo-Stage Least Squares (2SLS) method‚Äù are closely related but not identical. Here‚Äôs a detailed explanation of the differences: 8.2.1 Instrumental Variable (IV) Method Instrumental Variable (IV) method is a general approach used to address endogeneity in regression models. It involves using instruments‚Äîvariables that are correlated with the endogenous explanatory variable but uncorrelated with the error term. The IV method can be implemented in various ways, one of which is the 2SLS method. Purpose: To obtain consistent estimates when there is endogeneity due to omitted variable bias, measurement error, or simultaneity. Instruments: The choice of instruments is crucial. Valid instruments must satisfy two key conditions: Relevance: The instrument must be correlated with the endogenous explanatory variable. Exogeneity: The instrument must be uncorrelated with the error term in the regression model. 8.2.2 Two-Stage Least Squares (2SLS) Method Two-Stage Least Squares (2SLS) method is a specific implementation of the IV method. It involves a two-step estimation process to address endogeneity. Step 1 (First Stage): Regress the endogenous explanatory variable on the instruments to obtain the predicted values. \\[X = \\pi_0 + \\pi_1 Z + \\nu\\] Here, \\(X\\) is the endogenous variable, \\(Z\\) is the instrument, and \\(\\nu\\) is the error term. This regression yields the predicted values \\(\\hat{X}\\). Step 2 (Second Stage): Regress the dependent variable on the predicted values of the endogenous variable obtained from the first stage. \\[Y = \\alpha + \\beta \\hat{X} + \\epsilon\\] The 2SLS method ensures that the endogenous variable \\(X\\) is replaced by its predicted value \\(\\hat{X}\\), which is purged of the endogenous component, thus providing consistent estimates of \\(\\beta\\). 8.2.3 Summary of Differences: General Approach vs.¬†Specific Method: The IV method is a general approach for dealing with endogeneity, while 2SLS is a specific implementation of the IV method. Implementation: 2SLS involves a two-step process to obtain consistent estimates using instruments, whereas the IV method can be implemented in various ways, not just through 2SLS. Application: In practice, 2SLS is the most commonly used method for implementing the IV approach because it is straightforward and provides clear steps for estimation. 8.3 Homogenous Treatment Effect 8.4 Heterogenous Treatment Effect Note that the treatment effect differes by individual i. \\[Y^{1}_i - Y^{0}_i = \\delta_i\\] The main questions we have now are: what is IV estimating when we have heterogeneous treatment effects, and under what assumptions will IV identify a causal effect with heterogeneous treatment effects? we introduce a distinction between the internal validity of a study and its external validity. Internal validity means our strategy identified a causal effect for the population we studied. But external validity means the study‚Äôs finding applied to different populations (not in the study). There are considerably more assumptions necessary for identification once we introduce heterogeneous treatment effects‚Äîspecifically five assumptions. A stable unit treatment value assumption (SUTVA) that states that the potential outcomes for each person are unrelated to the treatment status of other individuals. 2.The independence assumption. The independence assumption is also sometimes called the ‚Äúas good as random assignment‚Äù assumption. It states that the IV is independent of the potential outcomes and potential treatment assignments. we can check pretreatment covariate balance. There is the exclusion restriction. The exclusion restriction states that any effect of Z on Y must be via the effect of Z on D. First stage. The monotonicity assumption. Monotonicity requires that the instrumental variable (weakly) operate in the same direction on all individual units. In other words, while the instrument may have no effect on some people, all those who are affected are affected in the same direction (i.e., positively or negatively, but not both). If all 5 assumptions hold truem then valid IV strategy estimates the local average treatment effect (LATE) of on D on Y. The LATE framework partitions the population of units with an instrument into potentially four mutually exclusive groups. Those groups are: 1. Compliers: This is the subpopulation whose treatment status is affected by the instrument in the correct direction. That is, \\(D^{1}_i=1\\) and \\(D^{0}_i=0\\). 2. Defiers: This is the subpopulation whose treatment status is affected by the instrument in the wrong direction. That is, \\(D^{1}_i=0\\) and \\(D^{0}_i=1\\). 3. Never takers: This is the subpopulation of units that never take the treatment regardless of the value of the instrument. So, \\(D^{1}_i=D^{0}_i=0\\). They simply never take the treatment 4. Always takers: This is the subpopulation of units that always take the treatment regardless of the value of the instrument. So, \\(D^{1}_i=D^{0}_i=1\\). They simply always take the instrument. CAVEAT With all five assumptions satisfied, IV estimates the average treatment effect for compliers, which is the parameter we‚Äôve called the local average treatment effect. Without further assumptions, LATE is not informative about effects on never-takers or always-takers because the instrument does not affect their treatment status. It matters because in most applications, we would be mostly interested in estimating the average treatment effect on the whole population, but that‚Äôs not usually possible with IV. "],["regression-discontinuity-designs-rdd.html", "Chapter 9 Regression Discontinuity Designs (RDD) 9.1 Regression Discontinuity Designs (RDD) 9.2 Key Concepts 9.3 Sharp RDD 9.4 Regression Kink Design", " Chapter 9 Regression Discontinuity Designs (RDD) (Comprehensive Source)[https://rdpackages.github.io] In the absence of randomized treatment assignment, research designs that allow for the rigorous study of non-experimental interventions are particularly promising. One of these is the Regression Discontinuity (RD) design In the RD design, all units have a score, and a treatment is assigned to those units whose value of the score exceeds a known cutoff or threshold, and not assigned to those units whose value of the score is below the cutoff. The key feature of the design is that the probability of receiving the treatment changes abruptly at the known threshold. I units are unable to sort arount the cutoff point, units with scores barely below the cutoff can be used as a comparison group for units with scores barely above it. In order to study causal effects with an RD design, the score, treatment, and cutoff must exist types of approaches A. Continuity based approach: this ensures the smoothness of the regression functions. Use least squares and polynomials, global or local to cutoff. The reason is that global polynomial approximations tend to deliver a good approximation overall, but a poor approximation at boundary points‚Äî Local polynomial methods are much better The idea that the treatment assignment is ‚Äúas good as‚Äù randomly assigned in a neighborhood of the cutoff has been often invoked in the continuity-based framework to describe the required identification assumptions in an intuitive way, and it has also been used to develop formal results. However, within the continuity-based framework, the formal derivation of identification and esti- mation results always ultimately relies on continuity and differentiability of regression functions, and the idea of local randomization is used as a heuristic device only. In contrast, the local randomization approach to RD analysis formalizes the idea that the RD design behaves like a randomized experiment near the cutoff by imposing explicit randomization-type assumptions that are stronger than the continuity-based conditions. less sensitive to outliers or other extreme features of the data Local polynomial methods implement linear regression fits using only observations near the cutoff point, separately for control and treatment units. h: bandwidth that determines the size of the neighborhood around the cutoff where the empirical RD analysis is conducted. the weights are determined by a kernel function K(¬∑) goal: fit the local polynomial that approximates the unknown regression functions around the cutoff. Local polynomial estimation consists of the following basic steps. Choose a polynomial order p and a kernel function K(¬∑). Choose a bandwidth h. For observations above the cutoff (i.e., observations with Xi ‚â• c), fit a weighted least squares regression of the outcome Yi For observations below the cutoff (i.e., observations with Xi &lt; c), fit a weighted least squares regression of the outcome Yi Kernel: assigns weights to units based on the distance: Triangular, uniform (simple linear regression), Epanechnikov polynomial order: 0 constant fit; increasing order means more accuracy but more variability, overfitting. in general the local linear estimator seems to deliver a good trade-off between simplicity, precision, and stability in RD settings. bandwidth: Choosing a smaller h will reduce the misspecification error (also known as ‚Äúsmoothing bias‚Äù) of the local polynomial approximation, but will simultaneously tend to increase the variance of the estimated coefficients because fewer observations will be available for estimation. the choice of bandwidth is said to involve a ‚Äúbias-variance trade-off.‚Äù MSE-optimal bandwidth for the local polynomial RD estimate, Example These are assuming uniform kernel, no weights and polynomial degree 1‚Ä¶ local poly-nomial point estimation is simply a weighted least-squares fit. linear reg y on x for both sides intercept 2 - intercept 1 B. linear reg Y on X + T + X*T within bandwidth coefficient of T C. rdrobust with p=1, kernel=uniform rdrobust has many more options to use fully non parametric B. Local Randomization: In a nutshell, the local randomization approach imposes conditions so that units above and below the cutoff whose score values lie in a small window around the cutoff are comparable to each other and thus can be studied ‚Äúas if‚Äù they had been randomly assigned to treatment or control. When the running variable is continuous, the local randomization approach typically requires stronger assumptions than the continuity-based approach; in these cases, it is natural to use the continuity-based approach for the main RD analysis, and to use the local randomization approach as a robustness check. But in settings where the running variable is discrete or other departures from the canonical RD framework occur, the local randomization approach no longer imposes the strongest assumptions and can be a natural and useful method for analysis. https://rdpackages.github.io/references/Cattaneo-Idrobo-Titiunik_2024_CUP.pdf 9.0.1 Validation and Falsification If the RD cutoff is known to the units that will be the beneficiaries of the treatment, researchers must worry about the possibility of units actively changing or manipulating the value of their score when they miss the treatment barely Naturally, the continuity assumptions that guarantee the validity of the RD design are about unobservable features and as such are inherently untestable. 1. Predetermined Covariates and Placebo Outcomes One of the most important RD falsification tests involves examining whether, near the cutoff, treated units are similar to control units in terms of observable characteristics. if units lack the ability to precisely manipulate the score value they receive, there should be no systematic differences between units with similar values of the score Predetermined Covariates: variables determined before treatment. placebo outcomes: variables determined after treatment placebo outcomes arealways specific to each application. For example, for the study investigating the impact of clean water on child mortality, road accidents for child mortaility. Outcomes would not be affected by treatment if there is no coincidence. In the continuity-based approach, this principle means that for each predetermined covariate or placebo outcome, researchers should first choose an optimal bandwidth, and then use local polynomial techniques within that bandwidth to estimate the ‚Äútreatment effect‚Äù and employ valid inference procedures such as the robust bias-corrected methods discussed previously. The fundamental idea behind this test is that, since the pre- determined covariate (or placebo outcome) could not have been affected by the treatment, the null hypothesis of no treatment effect should not be rejected if the RD design is valid. To implement this formal falsification test, we simply run rdrobust using each covariate of interest as the outcome variable. (Read page 94)[https://rdpackages.github.io/references/Cattaneo-Idrobo-Titiunik_2020_CUP.pdf] 2. Density of Running variable Check whether there is sorting. Examine whether in a local neighborhood near the cutoff, the number of observations below the cutoff is surprisingly different from the number of observations above it. if units do not have the ability to precisely manipulate the value of the score that they receive, the number of treated observations just above the cutoff should be approximately similar to the number of control observations below it. No guideline on bandwidth to be inspected, several bandwidths may be presented. Histogram would be helpful. Formal test uses binomial test. Choose a small neighborhood around the cutoff, and perform a simple Bernoulli test within that neighborhood with a probability of ‚Äúsuccess‚Äù equal to 1/2. This strategy tests whether the number of treated observations in the chosen neighborhood is compatible with what would have been observed if units had been assigned to the treatment group (i.e., to being above the cutoff) with a 50% probability. Or rddensity test. Both the continuity-based approach and the local randomization approach rely on the assumption that units that receive very similar score values on opposite sides of the cutoff are comparable to each other in all relevant aspects, except for their treatment status. The main distinction between these frameworks is how the idea of comparability is formalized: in the continuity-based framework, comparability is conceptualized as continuity of average (or some other feature of) potential outcomes near the cutoff, while in the local randomization framework, comparability is conceptualized as conditions that mimic a randomized experiment in a neighborhood around the cutoff. 3. Placebo Cutoffs Another useful falsification analysis examines treatment effects at artificial or placebo cutoff values. Evidence of continuity away from the cutoff is, of course, neither necessary nor sufficient for continuity at the cutoff, but the presence of discontinuities away from the cutoff can be interpreted as potentially casting doubt on the RD design This test replaces the true cutoff value by another value at which the treatment status does not really change, and performs estimation and inference using this artificial cutoff point. The expectation is that no significant treatment effect will occur at placebo cutoff values. To avoid ‚Äúcontamination‚Äù due to real treatment effects, for artificial cutoffs above the real cutoff we use only treated observations, and for artificial cutoffs below the real cutoff we use only control observations. Restricting the observations in this way guarantees that the analysis of each placebo cutoff uses only observations with the same treatment status. 4. Sensitivity to Observations near the cutoff If systematic manipulation of score values has occurred, it is natural to assume that the units closest to the cutoff are those most likely to have engaged in manipulation. The idea behind this approach is to exclude such units and then repeat the estimation and inference analysis using the remaining sample. This idea is sometimes referred to as a ‚Äúdonut hole‚Äù approach. In practice, it is natural to repeat this exercise a few times to assess the actual sensitivity for different amounts of excluded units. 5. Sensitivity to Bandwidth Choice The method now investigates sensitivity as units are added or removed at the end points of the neighborhood. Choosing the bandwidth is one of the most consequential decisions in RD analysis, because the bandwidth may affect the results and conclusions. In the continuity-based approach, this falsification test is implemented by changing the bandwidth used for local polynomial estimation. As h increases, bias of local polynomial estimator will increase and variance will decrease and CI will get narrower. 9.1 Regression Discontinuity Designs (RDD) Regression Discontinuity Designs (RDD) are quasi-experimental methods used to estimate causal effects when a treatment is assigned based on a cutoff point in a continuous assignment variable. The basic idea is to compare observations just above and below the cutoff, assuming they are similar except for the treatment. RDD is particularly suited to visual analysis. By plotting the running variable against the outcome variable, we can observe any ‚Äújumps‚Äù in the probability of treatment at the cutoff. These jumps indicate the treatment effect. 9.2 Key Concepts Assignment Variable: A continuous (running) variable that determines treatment assignment based on a cutoff point. Cutoff Point: The threshold value of the assignment variable that determines who receives the treatment. Treatment Group: Observations with assignment variable values above (or below) the cutoff. Control Group: Observations with assignment variable values below (or above) the cutoff. Local Average Treatment Effect (LATE): In RDD, we focus on estimating the treatment effect for observations near the cutoff. Since the probability of receiving the treatment changes abruptly at the cutoff, we compare outcomes for those just above and just below this point to estimate LATE. No Overlap/Common Support: Unlike randomized controlled trials (RCTs), RDD lacks overlap between treatment and control groups across the entire range of the running variable. Instead, it relies on extrapolation by comparing units with different values of the running variable that are close to the cutoff. As we approach the cutoff from either direction, the units become more comparable, which allows us to estimate the causal effect. Handling Extrapolation Bias: All methods used in RDD aim to address the bias arising from the need to extrapolate. These methods ensure that the comparisons made are as clean and unbiased as possible. 9.2.1 Types of RDD Sharp RDD: Treatment assignment is strictly determined by the cutoff. All units above (or below) the cutoff receive the treatment, and none below (or above) do. Fuzzy RDD: Treatment assignment is probabilistic at the cutoff. Not all units above (or below) the cutoff receive the treatment, and some units below (or above) the cutoff may receive the treatment. 9.3 Sharp RDD In Sharp RDD, the treatment is perfectly assigned based on the cutoff point. This can be represented as: \\[ D_i = \\begin{cases} 1 &amp; \\text{if } X_i \\geq c \\\\ 0 &amp; \\text{if } X_i &lt; c \\end{cases}\\] Where: \\(D_i\\) is the treatment indicator. \\(X_i\\) is the assignment variable. \\(c\\) is the cutoff point. Full compliance. only one cutoff. score is continuously distributed 9.3.1 Key Concepts The sharp RDD estimation is interpreted as an average causal effect of the treatment as the running variable approaches the cutoff in the limit, for it is only in the limit that we have overlap. This average causal effect is the local average treatment effect (LATE). Notice the role that extrapolation plays in estimating treatment effects with sharp RDD. If unit \\(i\\) is just below \\(c_0\\), then \\(D_i = 0\\). But if unit \\(i\\) is just above \\(c_0\\), then \\(D_i = 1\\). But for any value of \\(X_i\\), there are either units in the treatment group or the control group, but not both. Therefore, the RDD does not have common support, which is one of the reasons we rely on extrapolation for our estimation. Unit \\(i\\): Represents an individual observation in your dataset. \\(c_0\\): The cutoff value of the assignment variable \\(X\\) that determines treatment assignment. \\(D_i\\): The treatment indicator variable, where \\(D_i = 1\\) if the unit receives the treatment and \\(D_i = 0\\) if it does not. Common Support: A situation where there are treated and control units with similar values of the assignment variable \\(X\\). There is no common support because control and treated groups may not have same value of running variable. 9.3.1.1 Explanation In a sharp RDD: Treatment Assignment: Units are assigned to treatment or control strictly based on whether their assignment variable \\(X\\) is above or below the cutoff \\(c_0\\). If \\(X_i\\) (the assignment variable for unit \\(i\\)) is just below \\(c_0\\), then \\(D_i = 0\\) (the unit is in the control group). If \\(X_i\\) is just above \\(c_0\\), then \\(D_i = 1\\) (the unit is in the treatment group). 9.3.1.2 No Common Support in RDD No Overlap: For any given value of \\(X_i\\), units are either in the treatment group or the control group, but not both. This means that at any specific value of \\(X_i\\), you don‚Äôt have both treated and untreated units. For example, if \\(X_i\\) is exactly \\(c_0\\), you don‚Äôt have units both treated and untreated at that exact point (in practical terms, it‚Äôs often the case we look just below and just above \\(c_0\\)). 9.3.1.3 Extrapolation in RDD Extrapolation: To estimate the treatment effect at the cutoff, we essentially need to compare the outcomes of units just below and just above the cutoff. Since there are no units that have exactly the same value of \\(X\\) but different treatment statuses, we use the units close to the cutoff to infer what would happen if a unit‚Äôs treatment status were different. Example: Suppose the cutoff \\(c_0\\) is 50. Units with \\(X_i = 49.9\\) are in the control group and units with \\(X_i = 50.1\\) are in the treatment group. We compare the outcomes of these units to estimate the treatment effect. 9.3.1.4 Why Extrapolation is Needed Local Comparisons: In RDD, we rely on the assumption that units just below and just above the cutoff are very similar in all respects except for the treatment. Thus, we ‚Äúextrapolate‚Äù the behavior of one group to understand the counterfactual of the other. Local Treatment Effect: This local comparison near the cutoff allows us to estimate the causal effect of the treatment precisely at \\(c_0\\). 9.3.1.5 Summary In summary, the lack of common support in RDD means we don‚Äôt have units that are both treated and untreated at the same value of the assignment variable. As a result, we rely on extrapolation, comparing units just below and just above the cutoff to estimate the treatment effect. This is because these units are assumed to be similar except for the treatment, allowing us to infer what the outcome would be if their treatment status were different. 9.3.2 Assumptions for RDD 9.3.2.1 Continuity Assumption in RDD Definition: The potential outcomes (both treated and untreated) must be continuous at the cutoff. This ensures that any jump in the outcome at the cutoff can be attributed to the treatment. Expected Potential Outcomes: The assumption states that the expected potential outcomes are continuous at the cutoff. In simpler terms, if we plot the expected outcomes of the units just below and just above the cutoff, the outcomes would form a smooth curve if there were no treatment effect. If there were no treatment, there would not be a jump at the cutoff. Ruling Out Competing Interventions: If expected potential outcomes are continuous at the cutoff, it necessarily rules out competing interventions or other factors that might cause a discontinuity at the cutoff. This is crucial because we want to attribute any observed jump in the outcome to the treatment alone, not to some other unobserved factor. Omitted Variable Bias: Continuity explicitly rules out omitted variable bias at the cutoff. This means that all other unobserved determinants of the outcome variable \\(Y\\) are smoothly related to the running variable \\(X\\). Therefore, any abrupt change at the cutoff can be confidently attributed to the treatment effect. Interpreting the Assumption Mathematically: \\(E[Y^0 | X]\\) and \\(E[Y^1 | X]\\) are the expected outcomes if the unit did not receive and did receive the treatment, respectively. The continuity assumption means that \\(E[Y^0 | X]\\) and \\(E[Y^1 | X]\\) would not exhibit a sudden jump at the cutoff \\(c_0\\) in the absence of treatment. If there is a jump at \\(c_0\\), it indicates the presence of the treatment effect because in the absence of the treatment, \\(E[Y^1 | X]\\) should not change abruptly. Example to Illustrate Continuity Assumption Imagine we are studying the effect of a scholarship program on student test scores, where the scholarship is given to students who score above a certain cutoff on a preliminary test. Continuity without Treatment: If there were no scholarship, the expected test scores of students just below and just above the cutoff should be very similar and form a smooth curve. Jump Due to Treatment: If we observe a jump in test scores exactly at the cutoff, this jump can be attributed to the effect of receiving the scholarship, assuming the continuity assumption holds. Summary The continuity assumption in RDD is crucial for identifying the causal effect of the treatment. It ensures that any observed discontinuity in the outcome at the cutoff can be attributed solely to the treatment and not to any other unobserved factors. This assumption rules out omitted variable bias at the cutoff, ensuring the reliability of the estimated treatment effect. In essence, the continuity assumption guarantees that the treatment effect is the only factor causing a jump in the outcome at the cutoff, allowing us to make causal inferences from the RDD design. 9.3.2.2 No Manipulation (sorting) Units cannot precisely manipulate the assignment variable to end up on one side of the cutoff. This ensures that the units just above and below the cutoff are comparable. 9.3.3 Estimation in Sharp RDD Parametric Approach: Fit separate linear regressions on either side of the cutoff and estimate the treatment effect as the difference in the intercepts at the cutoff. \\[ Y_i = \\alpha_1 + \\beta_1 X_i + \\epsilon_i \\quad \\text{if } X_i \\geq c \\] \\[ Y_i = \\alpha_0 + \\beta_0 X_i + \\epsilon_i \\quad \\text{if } X_i &lt; c \\] The treatment effect is \\(\\alpha_1 - \\alpha_0\\). Non-Parametric Approach: Use local polynomial regression or kernel regression to fit the data near the cutoff. This method is preferred because it makes fewer assumptions about the functional form of the relationship between the assignment variable and the outcome. cluster at running variable ( bad idea) use heteroskedastic robust standard errors kernel type, cutof window 9.3.4 Fuzzy RDD In Fuzzy RDD, the probability of receiving treatment changes discontinuously at the cutoff but is not perfectly deterministic. This can be represented as: \\[ D_i = \\begin{cases} 1 &amp; \\text{with probability } p_1 \\text{ if } X_i \\geq c \\\\ 0 &amp; \\text{with probability } p_0 \\text{ if } X_i &lt; c \\end{cases} \\] Where \\(p_1\\) and \\(p_0\\) are the probabilities of receiving treatment above and below the cutoff, respectively. 9.3.4.1 Estimation in Fuzzy RDD Instrumental Variables (IV) Approach: Use the cutoff as an instrument for actual treatment receipt. The first stage estimates the probability of treatment, and the second stage uses this to estimate the treatment effect. First stage: \\[ D_i = \\pi_0 + \\pi_1 Z_i + \\eta_i \\] Second stage: \\[ Y_i = \\alpha + \\beta \\hat{D}_i + \\epsilon_i \\] Where \\(Z_i\\) is an indicator variable equal to 1 if \\(X_i \\geq c\\) and 0 otherwise. 9.3.5 Parametric vs.¬†Non-Parametric Applications Parametric Applications: Assume a specific functional form (e.g., linear) for the relationship between the assignment variable and the outcome. Simpler to implement but relies heavily on the correct specification of the model. Non-Parametric Applications: Make fewer assumptions about the functional form. Typically use methods like local polynomial regression or kernel regression to fit the data near the cutoff. More flexible and robust but can be more complex to implement and interpret. 9.3.6 Example Suppose a school district awards scholarships to students who score above a certain threshold on a standardized test. Sharp RDD: All students who score above 80 receive the scholarship, and none below 80 do. We compare students scoring just above 80 to those just below to estimate the effect of receiving the scholarship on academic outcomes. Fuzzy RDD: Students who score above 80 are more likely to receive the scholarship, but not all do (e.g., due to additional criteria or random factors). We use the score of 80 as an instrument to estimate the causal effect of receiving the scholarship. 9.3.7 Summary Regression Discontinuity Designs are powerful tools for causal inference in observational studies where treatment assignment is based on a cutoff. Sharp RDDs assume perfect treatment assignment based on the cutoff, while Fuzzy RDDs allow for probabilistic treatment assignment. Both parametric and non-parametric approaches can be used, with non-parametric methods generally preferred for their flexibility. Key assumptions include the continuity of potential outcomes and the inability of units to manipulate their assignment variable precisely. The reason RDD is so appealing to many is because of its ability to convincingly eliminate selection bias. Assignment variable, is often called the ‚Äúrunning variable‚Äù‚Äîis an observable confounder since it causes both Treatment and Outcome. The assignment variable assigns treatment on the basis of a cutoff, we are never able to observe units in both treatment and control for the same value of X. We can identify causal effects for those subjects whose score is in a close neighborhood around some cutoff \\(c_o\\). 9.3.8 Challenges to Identification The requirement for RDD to estimate a causal effect are the continuity assumptions. That is, the expected potential outcomes change smoothly as a function of the running variable through the cutoff. In words, this means that the only thing that causes the outcome to change abruptly at is the treatment. But, this can be violated in practice if any of the following is true: The assignment rule is known in advance. Agents are interested in adjusting. Agents have time to adjust. The cutoff is endogenous to factors that independently cause potential outcomes to shift. There is nonrandom heaping along the running variable. Examples include retaking an exam, self-reporting income, and so on. The cutoff is endogenous. An example would be age thresholds used for policy, such as when a person turns 18 years old and faces more severe penalties for crime. This age threshold triggers the treatment (i.e., higher penalties for crime), but is also correlated with variables that affect the outcomes, such as graduating from high school and voting rights. Let‚Äôs tackle these problems separately. Although assumptions may not be tested directly, indirect evidence may be show to be persuasive. 9.3.8.1 Density Test density test is used to check whether units are sorting on the running variable. under the null, the density should be continuous at the cutoff point. Under the alternative hypothesis, the density should increase at the kink. 9.3.8.2 Covariate balance and Other placebos For RDD to be valid in your study, there must not be an observable discontinuous change in the average values of reasonably chosen covariates around the cutoff. As these are pretreatment characteristics, they should be invariant to change in treatment assignment. This test is basically what is sometimes called a placebo test. That is, you are looking for there to be no effects where there shouldn‚Äôt be any. So a third kind of test is an extension of that‚Äîjust as there shouldn‚Äôt be effects at the cutoff on pretreatment values, there shouldn‚Äôt be effects on the outcome of interest at arbitrarily chosen cutoffs. Guido W. Imbens and Lemieux (2008) suggest looking at one side of the discontinuity, taking the median value of the running variable in that section, and pretending it was a discontinuity, \\(c^{i}_0\\). Then test whether there is a discontinuity in the outcome at \\(c^{i}_0\\). You do not want to find anything. 9.3.8.3 Non-random heaping in running variable Heaping occurs when there is an excess number of units at certain points along the running variable. In this case, it seems to happen at regular 100-gram intervals, likely due to hospitals rounding to the nearest integer. This pattern is unlikely to occur naturally and is almost certainly caused by either sorting or rounding. It could result from less sophisticated scales or, more concerningly, from staff rounding a child‚Äôs birth weight to 1,500 grams to make the child eligible for increased medical attention. In RDD, estimation compares means as we approach the threshold from either side, so the estimates should not be overly influenced by the observations at the threshold itself. One solution to address this issue is the ‚Äúdonut hole‚Äù RDD, where units in the vicinity of 1,500 grams are removed, and the model is re-estimated. 9.3.9 Examples 9.3.9.1 Close Election Regression Discontinuity Design (RDD) can be effectively used in the context of close elections to identify causal effects. The key idea is that in very close elections, the outcome of the election (winning or losing) can be considered as good as random. This randomness allows researchers to estimate the causal effect of winning an election on various outcomes. They argue that just around that cutoff, random chance determined the Democratic win‚Äîhence the random assignment of \\(D_t\\) Identifying the Impact of Political Office on Economic Policies: Scenario: Consider a study aiming to determine whether holding political office affects a politician‚Äôs subsequent policy decisions or economic outcomes in their district. RDD Approach: Researchers focus on elections decided by a very small margin of votes. For example, sample includes only observations where the Democrat vote share at time is strictly between 48 percent and 52 percent. Assumption: In close elections, the distribution of voter preferences is assumed to be similar on both sides of the cutoff (winning or losing by a small margin). This similarity allows the comparison of outcomes just above and just below the threshold. Application: By comparing districts where the incumbent barely won to those where the incumbent barely lost, researchers can isolate the effect of holding office on policy decisions and economic outcomes. 9.3.9.2 Education Policies Identifying the Impact of Scholarship Programs: Scenario: A scholarship program is awarded to students who score above a certain threshold on an entrance exam. RDD Approach: Researchers compare students who score just above the threshold (and receive the scholarship) to those who score just below (and do not receive the scholarship). Assumption: Students near the cutoff are similar in all respects except for receiving the scholarship. Application: By analyzing differences in educational attainment and future earnings between the two groups, researchers can estimate the causal impact of the scholarship program. 9.3.9.3 Health Interventions Identifying the Impact of Health Interventions: Scenario: A public health intervention is provided to individuals whose health risk score exceeds a certain threshold. RDD Approach: Researchers compare individuals who just qualify for the intervention to those who just miss the qualification. Assumption: Individuals near the threshold are comparable in health status except for receiving the intervention. Application: By examining health outcomes such as disease incidence or hospitalization rates, researchers can infer the causal effect of the health intervention. 9.3.10 Types of RDD 9.3.10.1 Sharp RDD In sharp RDD, the treatment assignment is strictly determined by whether the running variable crosses a threshold. Example: A tax credit is given to families whose income is below a certain cutoff. Families just below the cutoff receive the tax credit, while those just above do not. 9.3.10.2 Fuzzy RDD In fuzzy RDD, the probability of treatment assignment jumps at the threshold but not perfectly. Example: Eligibility for a drug rehabilitation program is determined by a cutoff on an addiction severity score, but not all eligible individuals enroll in the program. 9.3.11 Parametric vs.¬†Non-Parametric Applications 9.3.11.1 Parametric RDD Parametric RDD involves fitting a parametric model (e.g., a polynomial regression) to estimate the relationship between the running variable and the outcome. Example: Using a polynomial regression model to estimate the impact of an education intervention on test scores. 9.3.11.2 Non-Parametric RDD Non-parametric RDD uses local polynomial regression or other non-parametric methods to estimate the treatment effect, focusing on observations near the cutoff. Example: Applying local linear regression to estimate the impact of a health intervention on patient recovery rates. 9.4 Regression Kink Design "],["fixed-effects-and-panel-data-methods.html", "Chapter 10 Fixed Effects and Panel Data Methods 10.1 Pooled Regression 10.2 Panel Data Methods", " Chapter 10 Fixed Effects and Panel Data Methods 10.1 Pooled Regression Pooled regression refers to combining cross-sectional and time series data into a single dataset and estimating a common model without accounting for individual or time-specific effects. This method assumes that all individual observations (from different time periods and entities) share the same underlying regression model. Model Form: \\[ Y_{it} = \\beta_0 + \\beta_1 X_{it} + \\epsilon_{it} \\] where: - \\(Y_{it}\\) is the dependent variable for entity \\(i\\) at time \\(t\\). - \\(X_{it}\\) is the independent variable for entity \\(i\\) at time \\(t\\). - \\(\\epsilon_{it}\\) is the error term. Limitations: Homogeneity Assumption: Assumes that the same relationship between \\(X\\) and \\(Y\\) holds for all individuals and over time, which may not be realistic. Ignored Heterogeneity: Fails to account for unobserved heterogeneity across entities or over time. 10.2 Panel Data Methods Panel data methods leverage the structure of data that follows the same entities over multiple time periods, allowing for more sophisticated modeling that accounts for individual heterogeneity. Advantages: Control for Unobserved Heterogeneity: By following the same entities over time, we can control for time-invariant characteristics of those entities. More Observations: Increases the number of data points, improving the efficiency of estimations. Dynamic Analysis: Allows studying the dynamics of change over time. 10.2.1 Fixed Effects Model The Fixed Effects (FE) model is a popular method in panel data analysis that controls for unobserved heterogeneity by allowing each entity to have its own intercept. This method removes time-invariant characteristics from the data, effectively focusing on within-entity variation. Model Form: \\[ Y_{it} = \\alpha_i + \\beta X_{it} + \\epsilon_{it} \\] where: - \\(\\alpha_i\\) is the entity-specific intercept. - \\(Y_{it}\\) and \\(X_{it}\\) are as defined above. Why Fixed Effects Work: Control for Unobserved Heterogeneity: By allowing each entity its own intercept, the FE model controls for all time-invariant differences between entities. This is particularly useful if there are omitted variables that are correlated with the included independent variables. Elimination of Bias: Time-invariant characteristics (e.g., cultural factors, institutional differences) are differenced out, reducing the risk of omitted variable bias. Within Transformation: The FE model can be estimated by demeaning the data: \\[ \\overline{Y}_{i} = \\frac{1}{T} \\sum_{t=1}^{T} Y_{it} \\] \\[ \\overline{X}_{i} = \\frac{1}{T} \\sum_{t=1}^{T} X_{it} \\] \\[ \\overline{\\epsilon}_{i} = \\frac{1}{T} \\sum_{t=1}^{T} \\epsilon_{it} \\] Then subtract these averages from the original equation: \\[ Y_{it} - \\overline{Y}_{i} = \\beta (X_{it} - \\overline{X}_{i}) + (\\epsilon_{it} - \\overline{\\epsilon}_{i}) \\] This transformation removes \\(\\alpha_i\\) from the equation, allowing for consistent estimation of \\(\\beta\\). 10.2.2 Random Effects Model Random Effects (RE) model assumes that the entity-specific intercepts are random and uncorrelated with the independent variables. It is more efficient than FE if the assumption holds, but biased if the intercepts are correlated with the regressors. Model Form: \\[ Y_{it} = \\alpha + \\beta X_{it} + u_i + \\epsilon_{it} \\] where \\(u_i\\) is the random effect. Hausman Test: Used to decide between FE and RE models. It tests whether the unique errors (random effects) are correlated with the regressors. If the null hypothesis (no correlation) is rejected, the FE model is preferred. 10.2.3 Example: Economic Growth and Education Suppose we want to study the impact of education on economic growth using panel data from different countries over several years. Pooled Regression: Assumes the relationship between education and economic growth is the same across all countries and years. Model: \\(\\text{Growth}_{it} = \\beta_0 + \\beta_1 \\text{Education}_{it} + \\epsilon_{it}\\) Limitation: Ignores country-specific factors (e.g., institutional quality) that might affect growth. Fixed Effects: Accounts for country-specific unobserved factors that are constant over time (e.g., cultural factors, geographical characteristics). Model: \\(\\text{Growth}_{it} = \\alpha_i + \\beta_1 \\text{Education}_{it} + \\epsilon_{it}\\) Interpretation: The coefficient \\(\\beta_1\\) shows the effect of education on economic growth within the same country over time, controlling for time-invariant factors. Random Effects: Assumes that the unobserved country-specific effects are uncorrelated with the independent variables. Model: \\(\\text{Growth}_{it} = \\alpha + \\beta_1 \\text{Education}_{it} + u_i + \\epsilon_{it}\\) Interpretation: The coefficient \\(\\beta_1\\) shows the overall effect of education on economic growth, assuming that the country-specific effects are random. 10.2.4 Conclusion Pooled Regression: Simple but ignores heterogeneity. Fixed Effects: Controls for unobserved time-invariant heterogeneity but only uses within-entity variation. Random Effects: More efficient if the assumption holds but biased if unobserved effects are correlated with the regressors. Understanding these methods and their assumptions is crucial for correctly modeling and interpreting data in econometrics. For your interview, be prepared to explain these concepts, their applications, and when to use each method based on the data characteristics and research question. "],["doubledebiased-machine-learning-double-ml.html", "Chapter 11 Double/Debiased Machine Learning (Double ML) 11.1 Motivation 11.2 Key Components", " Chapter 11 Double/Debiased Machine Learning (Double ML) Core Idea: Use machine learning (ML) for high-dimensional confounding control, while applying econometric techniques (orthogonalization + sample splitting) to obtain valid causal effect estimates. 11.1 Motivation Challenge: In high-dimensional settings (hundreds of covariates \\(X\\)), ML methods excel at prediction but tend to be biased for causal inference because they can overfit nuisance parameters (propensity scores, outcome regressions). Goal: Estimate causal effects consistently and efficiently, even when the covariate space is large and flexible ML methods are used. 11.2 Key Components Orthogonalization (Neyman Orthogonality) Construct estimating equations (moment conditions) that are insensitive to small errors in nuisance parameter estimation. Example: residualize both the treatment and outcome with respect to \\(X\\), then regress the residualized outcome on the residualized treatment. Sample Splitting / Cross-Fitting Divide the sample into folds. Estimate nuisance functions (propensity scores, outcome models) on one fold, and plug them into treatment effect estimation on another fold. Rotate across folds and average the results. Prevents overfitting bias and ensures valid inference. Estimation of Treatment Effect After residualization, regress the residualized outcome on the residualized treatment. This yields an unbiased and asymptotically normal estimator of the causal effect. 11.2.1 Practical Workflow Split the data into \\(K\\) folds. For each fold: Estimate nuisance functions (\\(\\hat{m}(X), \\hat{p}(X)\\)) using ML. Compute residuals: \\(\\tilde{Y} = Y - \\hat{m}(X)\\) (outcome residual) \\(\\tilde{D} = D - \\hat{p}(X)\\) (treatment residual) Regress \\(\\tilde{Y}\\) on \\(\\tilde{D}\\). Average estimates across folds ‚Üí final DML estimator. 11.2.2 Use Case Example Education &amp; Wages: Estimating the causal effect of education on wages when there are 500+ potential confounders (family background, demographics, test scores, etc.). ML methods (e.g., random forests, LASSO, boosting) handle the high-dimensional confounding flexibly, while DML ensures valid causal inference. 11.2.3 Advantages Doubly Robust: Consistent if nuisance estimates are sufficiently good (not perfect). Asymptotically Normal: Enables valid confidence intervals and hypothesis testing. Scalable: Works with modern ML tools (lasso, random forests, neural nets). 11.2.4 Key Reference Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., &amp; Robins, J. (2018). Double/Debiased Machine Learning for Treatment and Structural Parameters. ‚úÖ This cleaned-up version should fit neatly into your methods notebook, while still being accessible and rigorous. Would you like me to add a side-by-side comparison (Doubly Robust vs.¬†Double ML), so your notebook highlights how DML generalizes DR methods? "],["ordinary-least-squares-ols.html", "Chapter 12 Ordinary Least Squares (OLS) 12.1 Ordinary Least Squares (OLS) 12.2 Simple Linear Regression 12.3 Multivariate Regression 12.4 Generalized Linear Model 12.5 Generalised Least Square 12.6 Weighted Least Squares (WLS)", " Chapter 12 Ordinary Least Squares (OLS) 12.1 Ordinary Least Squares (OLS) Ordinary Least Squares (OLS) is a fundamental technique in regression analysis used to estimate the parameters of a linear model. For OLS estimators to be the Best Linear Unbiased Estimators (BLUE), certain assumptions must hold. Violations of these assumptions can lead to biased, inconsistent, or inefficient estimates. Here are the key assumptions and the potential consequences of their violations: 12.1.1 Linearity Assumption: The relationship between the independent variables (X) and the dependent variable (Y) is linear. Violation: If the true relationship is not linear, the OLS estimates may be biased and inefficient. This can be addressed by transforming variables, adding polynomial terms, or using other non-linear models. Note: Angrist and Pischke (2009) argue that linear regression may be useful even if the underlying CEF itself is not linear, because regression is a good approximation of the CEF. So keep an open mind as I break this down a little bit more. 12.1.2 Exogeneity Assumption: The independent variables are not correlated with the error term. Formally, \\(E(\\epsilon | X) = 0\\). Violation: If this assumption is violated (endogeneity), the OLS estimates are biased and inconsistent. This can occur due to omitted variable bias, measurement error, or simultaneity. Instrumental Variables (IV) or other techniques may be used to address endogeneity. 12.1.3 Homoscedasticity Assumption: The variance of the error term is constant across all levels of the independent variables, i.e., \\(Var(\\epsilon | X) = \\sigma^2\\). Violation: If there is heteroscedasticity (non-constant variance of errors), the OLS estimates remain unbiased, but they are no longer efficient, and the standard errors are biased, leading to unreliable hypothesis tests. Heteroscedasticity-Robust standard errors or Generalized Least Squares (GLS) can be used to address heteroscedasticity. 12.1.4 No Autocorrelation Assumption: The error terms are not correlated with each other, i.e., \\(E(\\epsilon_i \\epsilon_j) = 0\\) for \\(i \\neq j\\). Violation: If there is autocorrelation (correlated errors), the OLS estimates remain unbiased, but they are inefficient, and standard errors are biased, leading to incorrect inferences. This is common in time series data. Techniques such as Newey-West standard errors or autoregressive models can be used to correct for autocorrelation. 12.1.5 No Perfect Multicollinearity Assumption: There is no perfect linear relationship among the independent variables. Violation: Perfect multicollinearity makes it impossible to estimate the coefficients uniquely. High but imperfect multicollinearity can inflate the variances of the coefficient estimates, making them unstable and sensitive to changes in the model. This can be addressed by removing or combining collinear variables, or using techniques such as Ridge Regression. 12.1.6 Normality of Errors (for inference) Assumption: The error terms are normally distributed, particularly important for small sample sizes to conduct valid hypothesis tests. Violation: If the errors are not normally distributed, the OLS estimates are still unbiased and consistent, but the hypothesis tests may be invalid. For large samples, the Central Limit Theorem mitigates this issue, but for small samples, transformations or non-parametric methods might be necessary. 12.1.7 Practical Considerations and Tests Detecting Violations: Linearity: Scatter plots, residual plots, and tests like the RESET test. Exogeneity: Hausman test for endogeneity, instrumental variable techniques. Homoscedasticity: Residual plots, Breusch-Pagan test, White test. Autocorrelation: Durbin-Watson test, Ljung-Box test. Multicollinearity: Variance Inflation Factor (VIF), condition index. Normality: Q-Q plots, Shapiro-Wilk test, Kolmogorov-Smirnov test. Understanding these assumptions and how to address their violations is crucial for robust regression analysis and accurate inference using OLS. 12.2 Simple Linear Regression Let‚Äôs derive the Ordinary Least Squares (OLS) estimators for the simple linear regression model: \\[ Y = \\alpha + \\beta X + \\epsilon \\] Here, \\(Y\\) is the dependent variable, \\(X\\) is the independent variable, \\(\\alpha\\) (alpha) is the intercept, \\(\\beta\\) (beta) is the slope, and \\(\\epsilon\\) (epsilon) is the error term. To find the OLS estimators, we need to minimize the sum of squared residuals (SSR): \\[ SSR = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 \\] \\[ \\hat{Y}_i = \\hat{\\alpha} + \\hat{\\beta} X_i \\] Substituting \\(\\hat{Y}_i\\): \\[ SSR = \\sum_{i=1}^n (Y_i - (\\hat{\\alpha} + \\hat{\\beta} X_i))^2 \\] To minimize SSR with respect to \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\), we take partial derivatives and set them to zero: Partial derivative with respect to \\(\\hat{\\alpha}\\): \\[ \\frac{\\partial SSR}{\\partial \\hat{\\alpha}} = \\sum_{i=1}^n -2(Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i) = 0 \\] Simplifying: \\[ \\sum_{i=1}^n (Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i) = 0 \\] \\[ \\sum_{i=1}^n Y_i - n\\hat{\\alpha} - \\hat{\\beta}\\sum_{i=1}^n X_i = 0 \\] Solving for \\(\\hat{\\alpha}\\): \\[ n\\hat{\\alpha} = \\sum_{i=1}^n Y_i - \\hat{\\beta} \\sum_{i=1}^n X_i \\] \\[ \\hat{\\alpha} = \\frac{1}{n} \\sum_{i=1}^n Y_i - \\hat{\\beta} \\frac{1}{n} \\sum_{i=1}^n X_i \\] \\[ \\hat{\\alpha} = \\bar{Y} - \\hat{\\beta} \\bar{X} \\] Where \\(\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^n Y_i\\) and \\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\\). Partial derivative with respect to \\(\\hat{\\beta}\\): \\[ \\frac{\\partial SSR}{\\partial \\hat{\\beta}} = \\sum_{i=1}^n -2X_i(Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i) = 0 \\] Simplifying: \\[ \\sum_{i=1}^n X_i(Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i) = 0 \\] Substitute \\(\\hat{\\alpha}\\) from the previous result: \\[ \\sum_{i=1}^n X_i Y_i - \\hat{\\alpha}\\sum_{i=1}^n X_i - \\hat{\\beta}\\sum_{i=1}^n X_i^2 = 0 \\] \\[ \\sum_{i=1}^n X_i Y_i - (\\bar{Y} - \\hat{\\beta} \\bar{X})\\sum_{i=1}^n X_i - \\hat{\\beta}\\sum_{i=1}^n X_i^2 = 0 \\] Substitute \\(\\hat{\\alpha} = \\bar{Y} - \\hat{\\beta} \\bar{X}\\): \\[ \\sum_{i=1}^n X_i Y_i - (\\bar{Y}\\sum_{i=1}^n X_i - \\hat{\\beta} \\bar{X} \\sum_{i=1}^n X_i) - \\hat{\\beta}\\sum_{i=1}^n X_i^2 = 0 \\] \\[ \\sum_{i=1}^n X_i Y_i - \\bar{Y}\\sum_{i=1}^n X_i + \\hat{\\beta} \\bar{X} \\sum_{i=1}^n X_i - \\hat{\\beta}\\sum_{i=1}^n X_i^2 = 0 \\] \\[ \\sum_{i=1}^n X_i Y_i - \\bar{Y}\\sum_{i=1}^n X_i = \\hat{\\beta}\\left(\\sum_{i=1}^n X_i^2 - \\bar{X} \\sum_{i=1}^n X_i\\right) \\] Since \\(\\sum_{i=1}^n \\bar{X} = n \\bar{X}\\): \\[ \\sum_{i=1}^n X_i Y_i - \\bar{Y}\\sum_{i=1}^n X_i = \\hat{\\beta}\\left(\\sum_{i=1}^n X_i^2 - n \\bar{X}^2\\right) \\] Simplifying: \\[ \\sum_{i=1}^n X_i Y_i - n \\bar{X} \\bar{Y} = \\hat{\\beta}\\left(\\sum_{i=1}^n X_i^2 - n \\bar{X}^2\\right) \\] Finally, solving for \\(\\hat{\\beta}\\): \\[ \\hat{\\beta} = \\frac{\\sum_{i=1}^n X_i Y_i - n \\bar{X} \\bar{Y}}{\\sum_{i=1}^n X_i^2 - n \\bar{X}^2} \\] Alternatively, this can be written in terms of deviations from the mean: \\[ \\hat{\\beta} = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2} \\] 12.2.1 Summary The OLS estimator for the slope (\\(\\beta\\)) is: \\[ \\hat{\\beta} = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2} \\] The OLS estimator for the intercept (\\(\\alpha\\)) is: \\[ \\hat{\\alpha} = \\bar{Y} - \\hat{\\beta} \\bar{X} \\] These derivations show how the OLS estimators for \\(\\beta\\) and \\(\\alpha\\) are obtained by minimizing the sum of squared residuals in a simple linear regression model. 12.2.2 Interpreting Model Coefficients in OLS Models In Ordinary Least Squares (OLS) regression, the interpretation of coefficients depends on the functional form of the model and the nature of the variables involved. Here are some common types of models and how to interpret their coefficients. 12.2.3 1. Linear Models (Linear-Linear) Model Form: \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\) Coefficients Interpretation: Continuous Variable: \\(\\beta_1\\) represents the change in \\(Y\\) for a one-unit increase in \\(X\\). Example: If \\(Y\\) is annual income in dollars and \\(X\\) is years of education, \\(\\beta_1 = 2000\\) means an additional year of education increases income by $2000. Dummy Variable: \\(\\beta_1\\) represents the difference in \\(Y\\) between the reference category (usually coded as 0) and the category represented by the dummy variable (coded as 1). Example: If \\(Y\\) is income and \\(X\\) is a dummy variable for gender (1 for male, 0 for female), \\(\\beta_1 = 5000\\) means males earn $5000 more than females, holding other factors constant. 12.2.4 2. Log-Linear Models Model Form: \\(\\log(Y) = \\beta_0 + \\beta_1 X + \\epsilon\\) Coefficients Interpretation: Continuous Variable: \\(\\beta_1\\) represents the approximate percentage change in \\(Y\\) for a one-unit increase in \\(X\\). Example: If \\(\\log(Y)\\) is the natural log of income and \\(X\\) is years of education, \\(\\beta_1 = 0.05\\) means an additional year of education increases income by approximately 5%. Dummy Variable: \\(\\beta_1\\) represents the approximate percentage difference in \\(Y\\) between the reference category and the category represented by the dummy variable. Example: If \\(\\log(Y)\\) is the natural log of income and \\(X\\) is a dummy for gender, \\(\\beta_1 = 0.10\\) means males earn approximately 10% more than females, holding other factors constant. 12.2.5 3. Linear-Log Models Model Form: \\(Y = \\beta_0 + \\beta_1 \\log(X) + \\epsilon\\) Coefficients Interpretation: Continuous Variable: \\(\\beta_1\\) represents the change in \\(Y\\) for a 1% change in \\(X\\) (since a 1% change in \\(X\\) is approximately a change of 0.01 in \\(\\log(X)\\)). Example: If \\(Y\\) is income and \\(\\log(X)\\) is the natural log of years of experience, \\(\\beta_1 = 1000\\) means a 1% increase in experience leads to an increase in income of $10 (since \\(1000 \\times 0.01 = 10\\)). Dummy Variable: Less common in linear-log models but would be interpreted similarly to linear models. 12.2.6 4. Log-Log Models Model Form: \\(\\log(Y) = \\beta_0 + \\beta_1 \\log(X) + \\epsilon\\) Coefficients Interpretation: Continuous Variable: \\(\\beta_1\\) represents the elasticity of \\(Y\\) with respect to \\(X\\), meaning the percentage change in \\(Y\\) for a 1% change in \\(X\\). Example: If \\(\\log(Y)\\) is the natural log of income and \\(\\log(X)\\) is the natural log of years of education, \\(\\beta_1 = 0.5\\) means a 1% increase in years of education results in a 0.5% increase in income. Dummy Variable: Less common in log-log models but would be interpreted as in log-linear models. 12.2.7 Examples of Dummy and Continuous Variables 12.2.7.1 Example 1: Linear Model with Dummy and Continuous Variables Model: \\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 D + \\epsilon\\) \\(Y\\): Income \\(X_1\\): Years of education (continuous) \\(D\\): Gender (dummy, 1 for male, 0 for female) Interpretation: - \\(\\beta_1\\): Change in income for an additional year of education. - \\(\\beta_2\\): Difference in income between males and females, holding education constant. 12.2.7.2 Example 2: Log-Linear Model with Continuous Variables Model: \\(\\log(Y) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\\) \\(\\log(Y)\\): Log of income \\(X_1\\): Years of education (continuous) \\(X_2\\): Years of work experience (continuous) Interpretation: - \\(\\beta_1\\): Percentage change in income for an additional year of education. - \\(\\beta_2\\): Percentage change in income for an additional year of work experience. 12.2.7.3 Example 3: Log-Log Model with Continuous Variables Model: \\(\\log(Y) = \\beta_0 + \\beta_1 \\log(X_1) + \\beta_2 \\log(X_2) + \\epsilon\\) \\(\\log(Y)\\): Log of income \\(\\log(X_1)\\): Log of years of education \\(\\log(X_2)\\): Log of years of work experience Interpretation: - \\(\\beta_1\\): Elasticity of income with respect to education. - \\(\\beta_2\\): Elasticity of income with respect to work experience. 12.2.8 Assumptions and Considerations Linearity: The relationship between the variables should be correctly specified (linear, log-linear, etc.). Homoscedasticity: The variance of the error terms should be constant across all levels of the independent variables. No Multicollinearity: Independent variables should not be too highly correlated with each other. No Autocorrelation: The residuals should not be correlated with each other (important in time series data). Normality of Errors: The error terms should be normally distributed, especially for small sample sizes. Understanding these models and their interpretations is crucial for accurately conveying the impact of variables in econometric analyses. Being prepared to explain these interpretations clearly and provide relevant examples will be beneficial for your interview. 12.3 Multivariate Regression It will eventually become second nature for you to talk about including more variables on the right side of a regression as ‚Äúcontrolling for‚Äù those variables. 12.4 Generalized Linear Model 12.5 Generalised Least Square 12.6 Weighted Least Squares (WLS) WLS is a special case of GLS where the variance-covariance matrix Œ£ of the error terms is diagonal. This means that the errors are uncorrelated but may have different variances. WLS is used specifically to address heteroscedasticity by assigning different weights to different observations based on their error variances. Weights: In WLS, each observation is assigned a weight that is inversely proportional to the variance of its error term. If an observation has a high variance, it receives a lower weight, and vice versa. "],["doubly-robust-methods.html", "Chapter 13 Doubly Robust Methods 13.1 Why They Matter 13.2 Core Ingredients 13.3 More on Outcome Models for Treated and Control 13.4 Advantages 13.5 Assumptions 13.6 Example Applications 13.7 Summary 13.8 Extension: DR in Difference-in-Differences (DRDID)", " Chapter 13 Doubly Robust Methods Doubly Robust (DR) estimators are a class of causal inference methods that combine two modeling strategies‚Äî Outcome regression models and Treatment assignment (propensity score) models. The key property is double robustness: the estimator of the causal effect remains consistent if either the outcome model or the treatment model is correctly specified (not necessarily both). 13.1 Why They Matter Problem: Using only outcome regression (risk of omitted confounders) or only propensity scores (risk of poor overlap/imbalance) can lead to bias. Solution: DR estimators merge both approaches, providing a safety net: if one model fails but the other is correct, the ATT/ATE estimate is still consistent. Applications: Widely used in observational studies, program evaluation, healthcare, education, and labor economics. 13.2 Core Ingredients Outcome Model Predicts the expected outcome given covariates and treatment: \\[ \\hat{m}(X,D) = \\hat{E}[Y \\mid X, D] \\] Example: linear regression, random forest, or other machine learning regressors. Treatment Model (Propensity Score Model) Models the probability of treatment given covariates: \\[ \\hat{p}(X) = P(D = 1 \\mid X) \\] Example: logistic regression, boosted trees. DR Estimator Combines the two: \\[ \\hat{\\theta}_{DR} = \\frac{1}{n} \\sum_{i=1}^n \\Big[ \\hat{m}(X_i, D_i) \\;+\\; \\frac{D_i \\cdot (Y_i - \\hat{m}(X_i,1))}{\\hat{p}(X_i)} \\;-\\; \\frac{(1-D_i)\\cdot (Y_i - \\hat{m}(X_i,0))}{1-\\hat{p}(X_i)} \\Big] \\] Intuition: outcome model ‚Äúpredicts‚Äù \\(Y\\); propensity weighting corrects residual differences. 13.3 More on Outcome Models for Treated and Control In the outcome regression step, the goal is to estimate what the outcome would be for each unit under both treatment states (\\(D=1\\) and \\(D=0\\)). This means we need: \\(\\hat{m}_1(X) = \\hat{E}[Y \\mid D=1, X]\\) (predicted outcome if treated) \\(\\hat{m}_0(X) = \\hat{E}[Y \\mid D=0, X]\\) (predicted outcome if untreated) There are two ways to get these: Single pooled regression (with treatment indicator): Fit one model: \\[ Y \\sim D + X + D \\times X \\] This allows different intercepts/slopes by treatment status. Separate regressions by group (common in practice): Fit one regression for treated (\\(D=1\\)) and one for controls (\\(D=0\\)). Then use each model to predict \\(\\hat{m}_1(X)\\) and \\(\\hat{m}_0(X)\\) for all units. Both approaches are trying to achieve the same thing: predict counterfactual outcomes for each unit under both treatment states. 13.3.1 Why Some Articles Emphasize Separate Models It makes the intuition cleaner: you estimate the outcome model within each group, then apply it across the sample. It avoids strong functional form assumptions (a single pooled regression assumes the same structure across treated and controls, unless you include interactions). In machine learning implementations (e.g., random forests, boosting), it‚Äôs natural to just train two models ‚Äî one per treatment status. 13.3.2 Key Point üëâ The distinction isn‚Äôt about the logic of DR, but about the implementation. Econometrics textbooks (like Wooldridge) often write it in pooled form for simplicity. Applied ML papers or software implementations often do separate regressions for treated vs.¬†controls because it‚Äôs flexible. 13.3.3 Takeaway for Your Notebook You can phrase it like this: In practice, the outcome regression can be specified in two ways: one pooled model with treatment indicators and interactions, or separate models for treated and controls. Both approaches aim to estimate the conditional expectation of outcomes under treatment and control, which are then combined with the propensity score model in the doubly robust estimator. 13.4 Advantages Robustness: Consistency as long as one model is correctly specified. Efficiency: Lower variance than using outcome regression or IPW alone. Flexibility: Can use linear models or modern ML for either part. 13.5 Assumptions Overlap: For all covariate values, both treated and untreated units exist. No unmeasured confounding: All relevant confounders are observed. Consistency/SUTVA: Treatment assignment is well-defined, no interference. 13.6 Example Applications Healthcare: Effect of new drug adoption on patient recovery, adjusting for patient characteristics. Labor Economics: Effect of training programs on wages when participants self-select. Education: Effect of tutoring on test scores, controlling for student demographics and school resources. 13.7 Summary Doubly robust estimators are widely considered a best practice in causal inference: They provide a safeguard against misspecification. They unify regression and weighting methods. They extend naturally into advanced designs (e.g., DR DiD, Double Machine Learning). They don‚Äôt remove the need for strong design and good covariates, but they improve the credibility and stability of causal effect estimates. 13.8 Extension: DR in Difference-in-Differences (DRDID) Extends DR logic to DiD setups. Estimates the ATT when parallel trends may hold only after conditioning on covariates. Uses both: Outcome regression in pre/post periods, and Propensity scores for treatment assignment. Still consistent if either the outcome regression or the propensity score model is correct. "],["resampling-methods.html", "Chapter 14 Resampling methods 14.1 Randomization-Based Methods 14.2 Bootstrapping", " Chapter 14 Resampling methods 14.1 Randomization-Based Methods Athey and Imbens are prominent economists who have contributed significantly to the development and application of randomization-based methods in econometrics. These methods are particularly valuable for inferring the probability that an estimated coefficient is not simply a result of chance. Here‚Äôs an explanation of the context and importance of their work: 14.1.1 Traditional Methods vs.¬†Randomization-Based Methods Traditional Methods: Economists often use parametric tests (like t-tests or F-tests) which rely on assumptions about the distribution of the error terms (e.g., normality, homoscedasticity). These methods use asymptotic approximations to calculate p-values, which may not always be accurate, especially in small samples or when assumptions are violated. Randomization-Based Methods: These methods, such as permutation tests, do not rely on these distributional assumptions. Instead, they use the actual data to construct the distribution of the test statistic under the null hypothesis. This approach involves reshuffling the treatment labels or data points multiple times to create a distribution of the test statistic that purely reflects random chance. 14.1.2 Why Use Randomization-Based Methods? Exact P-Values: Randomization methods can construct exact p-values, which provide a more precise measure of the probability that the observed effect could have arisen by chance. This is especially important in cases where traditional assumptions do not hold. Robustness: These methods are robust to violations of common statistical assumptions (e.g., non-normality, heteroscedasticity). They rely on the randomization process used in the experiment or observational study, ensuring that the inferences are valid regardless of the underlying data distribution. Small Samples: In small sample sizes, randomization-based methods are particularly advantageous because traditional asymptotic approximations may be unreliable. 14.1.3 How Randomization-Based Methods Work Calculate the Observed Statistic: First, calculate the test statistic (e.g., the difference in means, regression coefficient) from the observed data. Randomize Data: Randomly reassign the treatment labels or shuffle the data points many times (typically thousands). Each randomization should maintain the structure of the data but break any systematic association between treatment and outcome. Calculate Test Statistics for Randomizations: For each randomization, calculate the test statistic. This generates a distribution of the test statistic under the null hypothesis of no treatment effect. Compare Observed Statistic: Compare the observed test statistic to this distribution. The p-value is the proportion of randomized test statistics that are as extreme as or more extreme than the observed statistic. 14.1.4 Example Suppose Athey and Imbens are evaluating the effect of a training program on income: Observed Difference in Means: The difference in mean income between the treatment (received training) and control (no training) groups is calculated. Randomization: The treatment labels (received training or not) are randomly shuffled, and the difference in means is recalculated for each shuffle. This process is repeated many times. Distribution of Differences: The randomization process generates a distribution of differences in means under the null hypothesis that training has no effect. Calculate P-Value: The p-value is the proportion of differences from the randomization distribution that are as extreme or more extreme than the observed difference. 14.1.5 Contribution of Athey and Imbens Athey and Imbens‚Äô work emphasizes the use of these robust, non-parametric methods to provide more reliable and exact p-values. Their approach is part of a broader trend in econometrics and other social sciences toward more reliable inference methods that do not heavily rely on restrictive assumptions. By focusing on randomization-based methods, they help ensure that the conclusions drawn from empirical studies are more credible and less sensitive to potential violations of traditional model assumptions. This methodology is particularly valuable in experimental economics, field experiments, and observational studies where treatment effects are being evaluated. By using randomization-based methods, researchers can make stronger causal inferences and provide more robust evidence for policy and decision-making. 14.1.5.1 Jackknife The jackknife method is a resampling technique where each observation (in this case, each treated unit) is systematically omitted from the dataset, and the analysis is repeated each time to estimate the variance of the treatment effect. This provides a robust estimate of the standard errors that accounts for the potential variability across different treated units. 14.2 Bootstrapping bootstrapping is a procedure used to estimate the variance of an estimator. In the context of inverse probability weighting, we would repeatedly draw (‚Äúwith replacement‚Äù) a random sample of our original data and then use that smaller sample to calculate the sample analogs of the ATE or ATT. standard bootstrapping wild bootstrapping "],["hypotheis-testing.html", "Chapter 15 Hypotheis Testing 15.1 Concepts 15.2 Null Hypothesis 15.3 Permutation Tests 15.4 Fischer‚Äôs Exact Test", " Chapter 15 Hypotheis Testing 15.1 Concepts 15.1.1 Significance Level (Œ±) Definition: The significance level (Œ±) is the probability threshold used in hypothesis testing to determine whether to reject the null hypothesis. It represents the maximum probability of incorrectly rejecting the null hypothesis when it is actually true. Commonly Used Values: Typical Value: Œ± is commonly set at 0.05, meaning there is a 5% chance of incorrectly rejecting the null hypothesis. Other Values: Researchers may choose Œ± levels such as 0.01 or 0.10 depending on the study‚Äôs requirements and the desired balance between Type I and Type II errors. Interpretation: If the calculated p-value is less than or equal to Œ±, the results are considered statistically significant. If the p-value is greater than Œ±, the results are not statistically significant, suggesting that the null hypothesis cannot be rejected. 15.1.2 P-Value Definition: The p-value is the probability of obtaining test results at least as extreme as the observed results, under the assumption that the null hypothesis is true. It measures the strength of evidence against the null hypothesis. Key Points: Lower p-value: Indicates stronger evidence against the null hypothesis. A small p-value (typically ‚â§ Œ±) suggests that the observed results are unlikely to occur if the null hypothesis is true, leading to rejection of the null hypothesis. Higher p-value: Indicates weaker evidence against the null hypothesis. A larger p-value (typically &gt; Œ±) suggests that the observed results are reasonably likely to occur even if the null hypothesis is true, leading to failure to reject the null hypothesis. Interpretation: p ‚â§ Œ±: The results are statistically significant, suggesting that the observed effect is unlikely due to chance alone. p &gt; Œ±: The results are not statistically significant, suggesting that the observed effect could reasonably occur due to chance. Type I and Type II errors are concepts used in hypothesis testing and statistical decision-making, describing the errors that can occur when making conclusions about the population based on sample data. Here‚Äôs an explanation of each: 15.1.3 Type I Error Definition: A Type I error occurs when the null hypothesis (H‚ÇÄ) is incorrectly rejected, even though it is actually true. In other words, it represents the situation where the researcher concludes there is an effect or relationship when, in fact, there is no such effect or relationship in the population. Probability and Significance Level: - The probability of committing a Type I error is equal to the significance level (Œ±) chosen for the hypothesis test. For example, if Œ± is set at 0.05, there is a 5% chance of mistakenly rejecting the null hypothesis when it is true. Type I error This happens when we reject the null hypothesis when it should not be rejected. Type I error rate is the probability when Type I error happens, also known as significance level, or \\(\\alpha\\). A common value for alpha is 0.05. 15.1.4 Type II Error Definition: A Type II error occurs when the null hypothesis (H‚ÇÄ) is incorrectly not rejected, even though it is false. It represents the situation where the researcher fails to detect an effect or relationship that actually exists in the population. Probability and Power: - The probability of committing a Type II error is denoted as Œ≤ (beta). - Power (1 - Œ≤) represents the probability of correctly rejecting the null hypothesis when it is false. - Type II error rate is influenced by factors such as sample size, effect size, and variability in the data. Type II error This happens when we fail to reject the null hypothesis when it should be rejected. Type II error rate is also known as \\(\\beta\\). 15.1.5 Relationship Between Type I and Type II Errors Inverse Relationship: As the significance level (Œ±) decreases (making Type I errors less likely), the probability of committing a Type II error (Œ≤) tends to increase, and vice versa. Balancing Act: Researchers aim to strike a balance between Type I and Type II errors depending on the context and consequences of each error type. Power Analysis: Conducted to determine an appropriate sample size to minimize both types of errors and maximize the likelihood of detecting real effects. 15.1.6 Importance in Research and Decision Making Statistical Rigor: Understanding and controlling Type I and Type II error rates are essential for maintaining the integrity and reliability of research findings. Impact: Errors can have significant implications in fields such as medicine, psychology, economics, and policy-making, influencing decisions based on study results. In summary, Type I and Type II errors are critical concepts in hypothesis testing, highlighting the trade-offs and risks involved in drawing conclusions from sample data about the population. Proper consideration and calculation of these errors are vital for ensuring valid and meaningful research outcomes. 15.1.7 Statistical power Statistical power refers to the probability that a hypothesis test correctly rejects the null hypothesis when it should be rejected. It is denoted as \\(1 - \\beta\\), where \\(\\beta\\) is the probability of a Type II error (failing to reject the null hypothesis when it is false). A commonly accepted level of statistical power is 0.80, which corresponds to a Type II error rate (Œ≤) of 0.20. Achieving sufficient statistical power is crucial for obtaining reliable and meaningful results in research. Sample size plays a critical role in determining statistical power. For instance, when comparing two means, the calculation of statistical power can be based on factors such as the significance level (Œ±), effect size (Œ¥), and sample size (n). In summary, statistical power measures the ability of a study to detect a true effect or relationship, and it depends on various factors that should be carefully considered during the design and interpretation of experiments. The formula for calculating statistical power is given by: \\[ \\text{Power} = 1 - \\beta = 1 - P(\\text{Type II Error}) = P(\\text{Reject } H_0 | H_0 \\text{ is false}) \\] where: $ : $ $ H_0 : $ Increasing sample size or effect size generally increases statistical power, while reducing the significance level increases the probability of Type II Error. As sample size increases, the statistical power increases. Therefore, for our test to have desirable statistical power (usually 0.80), we want to estimate the minimum sample size required. 15.2 Null Hypothesis The debate between Fisher and Neyman on the null hypothesis in causal inference revolves around different interpretations and implications of the null hypothesis in the context of randomized experiments. 15.2.1 Fisher‚Äôs Sharp Null Hypothesis Sharp Null Hypothesis: Fisher‚Äôs sharp null hypothesis is the assertion that every single unit in the population has a treatment effect of zero. Implication: Under this null hypothesis, the treatment has no effect whatsoever on any unit. This is a very strong statement, implying that the treatment effect is exactly zero for all individuals. Testing: This allows for a precise and exact test of the null hypothesis because it specifies the potential outcomes for every unit. In this scenario, you can use randomization tests to calculate exact p-values by comparing observed outcomes to those expected under complete nullification of treatment effects. 15.2.2 Neyman‚Äôs Null Hypothesis Average Treatment Effect (ATE) Null Hypothesis: Neyman, in contrast, proposed a weaker form of the null hypothesis, which asserts that the average treatment effect (ATE) across all units is zero. Implication: This means that, on average, the treatment does not have an effect, but it allows for the possibility that some units could have positive treatment effects while others have negative effects, as long as they cancel out in the aggregate. Testing: Testing this hypothesis typically involves estimating the average treatment effect and assessing its statistical significance. This approach does not specify the exact potential outcomes for each unit, making it more general but less powerful for exact testing. 15.2.3 Key Differences Stringency: Fisher‚Äôs sharp null hypothesis is more stringent because it makes a precise statement about every unit‚Äôs treatment effect being zero. Neyman‚Äôs null hypothesis is less stringent because it only concerns the average effect across the population. Testing Methodologies: Under Fisher‚Äôs sharp null, one can use permutation or randomization tests to obtain exact p-values, as the null hypothesis specifies the exact distribution of outcomes under no treatment effect. Under Neyman‚Äôs null, one typically relies on estimations and asymptotic properties to test the significance of the estimated ATE. This involves confidence intervals and standard errors. Implications for Experimental Design: Fisher‚Äôs approach allows for stronger conclusions in terms of causality for each unit but requires stronger assumptions. Neyman‚Äôs approach provides a broader inference about the average effect, which is often more realistic in practical scenarios where treatment effects can vary across units. 15.2.4 Example to Illustrate the Difference Suppose we conduct an experiment to test the effect of a new drug on blood pressure. We have two groups: a treatment group and a control group. Fisher‚Äôs Sharp Null Hypothesis: The null hypothesis here would state that the new drug has no effect on blood pressure for every individual in the treatment group. This means if an individual‚Äôs blood pressure would be 120 without the drug, it remains 120 with the drug. If we observe a difference between the treatment and control groups, we can use randomization tests to see if this difference is likely to occur by chance under the sharp null. Neyman‚Äôs ATE Null Hypothesis: The null hypothesis here would state that the average change in blood pressure due to the drug across all individuals is zero. This allows for some individuals to experience a decrease in blood pressure and others an increase, as long as these changes average out to zero. Here, we would estimate the ATE and test if it is significantly different from zero using statistical inference methods. 15.2.5 Conclusion The debate between Fisher and Neyman highlights a fundamental difference in how causal effects are conceptualized and tested in statistics. Fisher‚Äôs sharp null hypothesis allows for precise testing with exact p-values but requires stronger assumptions, while Neyman‚Äôs ATE null hypothesis is more flexible and realistic but relies on estimation and inference methods that are less precise in defining individual treatment effects. Understanding both approaches provides a comprehensive view of hypothesis testing in the context of causal inference. 15.3 Permutation Tests A permutation test (also known as a randomization test or re-randomization test). This method is used in econometrics and statistics to test the null hypothesis when the assumptions required for traditional parametric tests (like the t-test) may not hold, particularly in the context of small sample sizes or when the distribution of the test statistic under the null hypothesis is complex or unknown. 15.3.1 When and Why to Use Permutation Tests Non-Parametric Nature: When: Permutation tests are useful when the data do not necessarily meet the assumptions of parametric tests, such as normality or homogeneity of variance. Why: Because they are non-parametric, permutation tests do not rely on the underlying distribution of the data, making them more robust in certain situations. Small Sample Sizes: When: They are particularly valuable when dealing with small sample sizes where the Central Limit Theory may not apply, and thus, the sampling distribution of the test statistic under the null hypothesis is not well-approximated by a normal distribution. Why: In small samples, traditional methods like the t-test may not be reliable. Permutation tests use the actual data to construct the distribution of the test statistic, which can provide a more accurate p-value. Exact Test: When: When you need an exact test rather than an approximate one. Why: Permutation tests generate the exact distribution of the test statistic under the null hypothesis by considering all possible reassignments of treatment labels, ensuring an accurate p-value. Complex Experimental Designs: When: In complex experimental designs where the structure of the data or the treatment assignment mechanism does not fit well with the assumptions of standard tests. Why: Permutation tests are flexible and can be adapted to a wide variety of experimental designs and data structures. 15.3.2 How to Perform a Permutation Test Here‚Äôs a step-by-step outline of the permutation test procedure: Calculate the Original Test Statistic: Compute the test statistic (e.g., the difference in means between treatment and control groups) for the observed data. Drop the Treatment Variable: Remove the treatment labels from the data, essentially pooling all the data together. Reassign Treatment Labels: Randomly reassign the treatment labels to the data, ensuring the same number of treatment and control units as in the original data. Calculate the Test Statistic for the New Assignment: Compute the test statistic for this new random assignment of treatment labels. Repeat the Process: Repeat the re-randomization and calculation of the test statistic many times (e.g., 1,000 or more) to build a distribution of the test statistic under the null hypothesis. Create the Empirical Distribution: Collect all the computed test statistics from the repeated random assignments to form an empirical distribution of the test statistic under the null hypothesis. Calculate the Empirical P-Value: Compare the original test statistic to this empirical distribution. The p-value is the proportion of test statistics in the empirical distribution that are as extreme as, or more extreme than, the original test statistic. 15.3.3 Example Calculation Observed Data: Suppose you have two groups, treatment (\\(Y_T\\)) and control (\\(Y_C\\)), and you calculate the observed difference in means, \\(\\Delta_{obs} = \\bar{Y}_T - \\bar{Y}_C\\). Reassign Labels: Randomly shuffle the combined data and reassign the treatment and control labels. Compute New Statistic: Calculate the new difference in means for this re-randomized data, \\(\\Delta_{rand}\\). Repeat: Perform steps 2 and 3, say 1,000 times, to get a distribution of \\(\\Delta_{rand}\\). Compare: Determine where \\(\\Delta_{obs}\\) falls within the distribution of \\(\\Delta_{rand}\\). If \\(\\Delta_{obs}\\) is in the extreme tails of this distribution, it suggests that the observed effect is unlikely to have occurred by random chance. P-Value: Calculate the p-value as the proportion of \\(\\Delta_{rand}\\) values that are as extreme or more extreme than \\(\\Delta_{obs}\\). 15.3.4 Conclusion Permutation tests are a powerful tool for testing hypotheses in situations where traditional assumptions may not hold. By using the actual data to generate the null distribution of the test statistic, permutation tests provide a robust, non-parametric method for hypothesis testing, ensuring accurate p-values even in complex or small-sample scenarios. 15.4 Fischer‚Äôs Exact Test Fisher‚Äôs Exact Test is a statistical test used to determine if there are nonrandom associations between two categorical variables. It is particularly useful when sample sizes are small, and the assumptions of the chi-square test (like the expected frequency in each cell being at least 5) are not met. The test is named after the famous statistician Ronald A. Fisher. 15.4.1 When to Use Fisher‚Äôs Exact Test Small Sample Sizes: Fisher‚Äôs Exact Test is often used when dealing with small sample sizes, where the chi-square test may not be appropriate. Categorical Data: It is used for categorical data to test for independence between two variables. 2x2 Contingency Tables: While the test can be extended to larger tables, it is most commonly applied to 2x2 contingency tables. 15.4.2 How Fisher‚Äôs Exact Test Works 15.4.2.1 Example Consider the following 2x2 contingency table: Treatment Control Total Success a b a + b Failure c d c + d Total a + c b + d n Where: - \\(a\\): Number of successes in the treatment group - \\(b\\): Number of successes in the control group - \\(c\\): Number of failures in the treatment group - \\(d\\): Number of failures in the control group - \\(n\\): Total number of observations 15.4.2.2 Calculating the P-Value Fisher‚Äôs Exact Test calculates the exact probability of obtaining a distribution of values in the contingency table given the observed marginal totals. The p-value is computed by summing the probabilities of all possible tables that have the same row and column totals as the observed table and have a test statistic as extreme as, or more extreme than, the observed one. The probability of any particular table is given by the hypergeometric distribution: \\[ P = \\frac{\\binom{a+b}{a} \\binom{c+d}{c}}{\\binom{n}{a+c}} \\] Where \\(\\binom{n}{k}\\) is the binomial coefficient, representing the number of ways to choose \\(k\\) successes out of \\(n\\) trials. 15.4.3 Step-by-Step Example Let‚Äôs say we have the following data from a clinical trial: Treatment Control Improved 8 2 Not Improved 1 5 We want to test if there is a significant association between the treatment and the improvement. Construct the Contingency Table: Treatment Control Total Improved 8 2 10 Not Improved 1 5 6 Total 9 7 16 Calculate the P-Value: The p-value for Fisher‚Äôs Exact Test is the sum of the probabilities of all tables that are as extreme as or more extreme than the observed table, given the marginal totals. For the given table: \\[ P = \\frac{\\binom{10}{8} \\binom{6}{1}}{\\binom{16}{9}} = \\frac{45 \\times 6}{11440} \\approx 0.0236 \\] Interpret the Result: If the p-value (0.0236) is less than the chosen significance level (commonly 0.05), we reject the null hypothesis and conclude that there is a significant association between the treatment and the improvement. 15.4.4 Conclusion Fisher‚Äôs Exact Test is a powerful tool for analyzing contingency tables, especially when sample sizes are small. It provides an exact p-value for the test of independence between two categorical variables, making it a preferred choice when the assumptions of the chi-square test are not satisfied. By using this test, researchers can make accurate inferences about the relationships between categorical variables even in studies with limited data. "],["maximum-likelihood-estimation.html", "Chapter 16 Maximum Likelihood Estimation", " Chapter 16 Maximum Likelihood Estimation very nice website [https://www.statlect.com/fundamentals-of-statistics/linear-regression-maximum-likelihood] "],["amazon-economist---case-study---2024.html", "Chapter 17 Amazon Economist - Case Study - 2024 17.1 Concept 17.2 üì¶ The Scenario 17.3 Step 1: Clarify the Business Question and Data 17.4 Step 2: Possible Evaluation Designs (Tech Breadth) 17.5 Step 3: Tech Depth ‚Äî Example with Difference-in-Differences 17.6 Step 3 with RCTs 17.7 Step 4: Communicate Results 17.8 More on Instrumental Variables 17.9 More Questions", " Chapter 17 Amazon Economist - Case Study - 2024 17.1 Concept Reduced Form Causal Analysis (RFCA) RFCA economists specialize in econometric methods to identify causal relationships, handling challenges like selection bias and omitted variables. They use tools such as difference-in-differences, regression discontinuity, matching, synthetic control, and DML, and may conduct surveys or RCTs when needed. They work on program/product evaluations, elasticity estimation, customer behavior analysis, long-term effect prediction, and translating results into business decisions. Communication with non-technical partners is key. In interviews, you‚Äôll be given an ambiguous business case to test your breadth (range of methods you can propose) and depth (ability to implement and explain one method in detail). I suggest asking clarifying questions on the front end before diving into a strategy. Sample Technical Questions How would you measure the effect of a training program for Fulfillment Center associates on performance? How would you help business leaders think about whether they should invest in creating tools to help AWS customers reduce their costs by optimizing the cloud services they use? There‚Äôs an intervention X that we tried or are thinking of trying. Leaders want to know if it was or will be a good idea. How would you help them answer that question? They‚Äôre testing: Tech breadth ‚Üí can you list a range of reasonable evaluation approaches? Tech depth ‚Üí can you dive into one and explain it rigorously (causal inference). 17.2 üì¶ The Scenario Amazon rolls out a new delivery program (1-day delivery) in 20‚Äì30 cities. They ask: ‚ÄúHow do you evaluate it? What outcomes would you look at?‚Äù 17.3 Step 1: Clarify the Business Question and Data Data Availability ‚ÄúDo we have pre-period data on orders, customer demographics, costs?‚Äù If they don‚Äôt specify, assume city-level panel data (treated vs.¬†untreated cities across time). But it‚Äôs good to show breadth by saying something like: ‚ÄúI‚Äôd expect the data at the city-week level: orders, revenue, Prime retention, etc. If more granular data is available, like customer-level transactions, I could run matching or micro-level causal models. But even at the city level, difference-in-differences or synthetic control would be natural.‚Äù ‚ÄúAt the city-week panel level, many characteristics like population or median income won‚Äôt change, so they‚Äôll be absorbed by city fixed effects. That way, I‚Äôm comparing each city to itself over time. For time-varying factors, like seasonality, local unemployment, or weather disruptions, I‚Äôd include them as controls in the regression. This ensures that my difference-in-differences estimate isn‚Äôt confounded by local shocks that vary over time.‚Äù Before jumping into methods, clarify: Primary outcomes: Customer satisfaction (ratings, NPS, repeat purchase). Order volume (total orders, frequency). Revenue/profit per customer. Prime signups / retention. Secondary outcomes: Operational costs (delivery costs, fulfillment costs). Spillovers (did 1-day shift demand from 2-day, or increase net new orders?). This shows you think like an economist, not just a statistician. Outcomes of Interest ‚ÄúWhat does success mean for this program ‚Äî more orders, more Prime signups, higher revenue, or customer satisfaction?‚Äù This tells them you know different outcomes may imply different models. order frequency, customer retention, order size, sign-ups, customer satisfaction - NPS, revenue per customer, profit margin, delivery cost per order, on time delivery rates. Unit of Rollout ‚ÄúWas the rollout done at the city level, ZIP code level, or customer level?‚Äù Helps you know what level of aggregation to use in analysis. Timing / Staggering ‚ÄúWere all 20‚Äì30 cities rolled out at once, or was it staggered over time?‚Äù If staggered, you can talk about staggered diff-in-diff or event studies. Selection of Cities ‚ÄúHow were the rollout cities chosen? Were they the largest markets, or chosen randomly?‚Äù Critical: if not random, you‚Äôll need to account for bias (synthetic control, matching, IV). Lets you judge whether parallel trends can be tested. ‚ÄúFirst, I‚Äôd want to clarify what outcome Amazon cares most about. For a delivery program like this, possible outcomes could be order volume, customer satisfaction, revenue per customer, or Prime retention. I‚Äôd also ask how the rollout was done ‚Äî were the 20‚Äì30 cities chosen randomly, or were they the biggest markets? And was the rollout staggered or simultaneous? Those details matter because they influence whether I can use difference-in-differences, a staggered event study, or whether I‚Äôd need something like synthetic control to address selection bias. Assuming we have pre- and post-data on treated and untreated cities, a clean way to evaluate would be a difference-in-differences design. I‚Äôd compare order volumes before and after in rollout cities relative to control cities. The key assumption is parallel trends, and I‚Äôd test that using pre-trends. The outcomes I‚Äôd look at would include both customer-side measures (orders per customer, conversion rates, Prime retention) and cost-side measures (delivery cost per order, overall fulfillment cost). The goal is to see not only if customers are ordering more, but also if it‚Äôs profitable. For robustness, I‚Äôd also try synthetic control if the rollout wasn‚Äôt random, or event study if adoption was staggered. The idea is to triangulate the effect with multiple methods to make sure the result is credible.‚Äù 17.4 Step 2: Possible Evaluation Designs (Tech Breadth) You want to list several designs, then choose one to dive into: 17.4.1 Randomized Controlled Trial (RCT) The gold standard would be a randomized controlled trial. For example, Amazon could randomly assign some cities, ZIP codes, or customers to get the 1-day delivery program, and others to stay at 2-day delivery. We‚Äôd then compare outcomes like order frequency, customer retention, order size, sign-ups, customer satisfaction - NPS, revenue per customer, profit margin, delivery cost per order, on time delivery rates. The strength of an RCT is that it removes selection bias, since treated and control groups are comparable by design. The challenge, of course, is feasibility ‚Äî Amazon may not want to hold back 1-day delivery in some places once it‚Äôs available. That‚Äôs why in practice, we might rely more on natural experiments, difference-in-differences, or synthetic controls. But if we could do it, an RCT would give the cleanest causal estimate. 17.4.2 Difference-in-Differences (DiD) üîπ What DiD Does in This Case Setup: Amazon rolls out 1-day delivery in 20‚Äì30 cities (treated group). Other cities don‚Äôt get it (control group). Idea: Compare how outcomes (orders, revenue, Prime retention, etc.) change before vs.¬†after rollout in treated cities, relative to the same change in control cities. Why it works: Controls for time trends affecting all cities (e.g., seasonality, holidays, economic shocks). üîπ The Core Equation At the city-week level, you‚Äôd estimate something like: \\[ Y_{it} = \\alpha + \\beta \\cdot Treatment_{it} + \\gamma_i + \\delta_t + \\epsilon_{it} \\] Where: \\(Y_{it}\\): outcome (e.g., orders per capita). \\(Treatment_{it}\\): 1 if city i has 1-day delivery at time t. \\(\\gamma_i\\): city fixed effects (control for time-invariant city differences). \\(\\delta_t\\): time fixed effects (control for shocks affecting all cities). \\(\\beta\\): DiD estimate = causal effect of 1-day delivery. Key Assumption Parallel Trends: In absence of treatment, treated and control cities would have had similar outcome trends. You test this using pre-trends (plot outcomes over time before rollout). If treated cities were already diverging, plain DiD won‚Äôt be valid. Variants / Extensions Event Study / Dynamic DiD: Estimate effects over time (e.g., 1 week after rollout, 4 weeks after, 12 weeks after). Lets you see whether effects grow, fade, or persist. Staggered Adoption: If cities rolled out at different times, you use modern DiD estimators (like Sun &amp; Abraham 2020 or Callaway &amp; Sant‚ÄôAnna 2021) to avoid bias from naive two-way fixed effects. Matched DiD: Pre-match cities on observables (size, income, Prime penetration) before applying DiD. üîπ Strengths Straightforward to implement. Widely used and easy to explain to business stakeholders. Naturally fits rollout experiments where not everyone is treated. üîπ Limitations Relies heavily on parallel trends assumption. Vulnerable to spillovers: if control cities benefit indirectly (e.g., nearby customers crossing into treated areas). If treatment timing is staggered, naive TWFE (two-way FE) DiD can be biased ‚Äî must use updated estimators. üîπ How You‚Äôd Say It in Interview A natural evaluation method here would be difference-in-differences. I‚Äôd compare changes in order volume and customer retention in the 20‚Äì30 rollout cities before vs.¬†after treatment, relative to similar changes in non-rollout cities. The DiD model would include city fixed effects to control for time-invariant differences and time fixed effects for national shocks. The key assumption is parallel trends, so I‚Äôd test pre-trends carefully. If rollout was staggered, I‚Äôd use a modern staggered DiD estimator to avoid bias. The nice thing about DiD is it controls for shared seasonality, like Prime Day or holiday shopping, and gives a clear causal estimate. 17.4.3 Event Study / Staggered Adoption If rollout timing differs across cities, use staggered DiD (good for program evaluation with multiple cohorts). 17.4.4 Synthetic Control What Synthetic Control Is Synthetic Control builds a counterfactual for a treated unit (e.g., a city that got one-day delivery). Instead of picking one ‚Äúcontrol city,‚Äù you create a weighted combination of multiple control cities that best mimics the treated city‚Äôs pre-treatment outcomes. After rollout, you compare the treated city to its synthetic version. Why It‚Äôs Useful Here Rollout cities are not random ‚Äî Amazon likely chose large, high-Prime, high-volume cities. Those treated cities may look nothing like the average untreated city. A synthetic control lets you construct a ‚Äúlook-alike‚Äù city from weighted controls. Example: Seattle = 0.5 Portland + 0.3 Denver + 0.2 Kansas City (weights chosen to minimize pre-treatment differences). This gives a more credible counterfactual than just comparing Seattle vs.¬†one city. Implementation Steps Choose predictors: Pre-rollout outcomes (orders, revenue, retention), demographics, Prime penetration. Fit weights: Find nonnegative weights for control cities so that the weighted average matches the treated city‚Äôs pre-treatment trajectory. Compare outcomes post-rollout: The gap between actual treated city and its synthetic twin = estimated treatment effect. Strengths Transparent: You can show stakeholders that treated and synthetic city had nearly identical trends before rollout. Handles selection bias better than plain DiD when treated units differ a lot from untreated. Useful when the number of treated units is small (like 20‚Äì30 cities). Limitations Works best with one or a few treated units ‚Äî gets tricky with many treated cities at once. Sensitive to choice of predictors and pre-treatment period. Cannot easily capture spillovers (e.g., if nearby untreated cities are indirectly affected). Extensions Multiple Synthetic Controls: Build one synthetic twin per treated city. Generalized Synthetic Control: Matrix factorization methods allow handling multiple treated units and staggered adoption. Another approach is Synthetic Control. For each rollout city, I could build a weighted combination of untreated cities that closely tracks its pre-treatment outcomes. For example, if Seattle got 1-day delivery, its synthetic twin might be 50% Portland, 30% Denver, and 20% Kansas City. If Seattle diverges from its synthetic twin after rollout, that gap gives us the treatment effect. This is powerful when rollout cities are very different from average controls, and it makes the counterfactual more credible than simple before-after or DiD. The limitation is that it works best with a small number of treated units and needs stable pre-treatment data. 17.4.5 Propensity Score Matching / Reweighting Match treated cities with comparable controls (population size, Prime penetration, order frequency). Propensity Score Matching (PSM) was originally designed for individual-level data (like patients in medical studies, or customers in marketing experiments), where you have many units to match across treatment/control. In the Amazon delivery rollout case, your ‚Äúunits‚Äù are cities ‚Äî and you only have 20‚Äì30 treated cities. That‚Äôs a small sample, which makes PSM less powerful. But PSM can still apply at the city level if: You have a large enough pool of potential control cities (e.g., 200+ U.S. cities not in the rollout). You include rich covariates (population, income, Prime penetration, baseline orders, etc.) to balance them. You treat PSM as a preprocessing step before running DiD or regression ‚Äî so it reduces imbalance but doesn‚Äôt stand alone. Propensity Score Matching is often most effective at the individual level, because you have many units to match. With cities, the sample size is small, so matching quality can be limited. That said, if we had a large enough pool of untreated cities, we could use PSM as a preprocessing step ‚Äî match treated and control cities on observables like population, income, and baseline demand, and then run a difference-in-differences. This way, we reduce observable imbalance while still accounting for time trends. But I‚Äôd be careful about relying on PSM alone here, because unobserved differences across cities could still bias the estimate. 17.4.6 Instrumental Variables (IV) (if spillovers or selection bias is a concern). Why IV Might Be Needed Rollout to 20‚Äì30 cities is not random ‚Äî Amazon may have picked biggest cities, high-Prime penetration, or operationally easier locations. This creates selection bias ‚Üí treated cities differ systematically from controls. Spillovers are also possible ‚Üí if neighboring cities benefit (e.g., customers order from a nearby one-day delivery area), it contaminates the control group. IV is useful when treatment assignment is endogenous (correlated with unobserved demand). How IV Works Here We want: the causal effect of one-day delivery availability on outcomes (orders, revenue, Prime signups). Problem: rollout cities are chosen strategically ‚Üí treatment is endogenous. Solution: find a variable (instrument) that: Is correlated with rollout (affects likelihood a city gets one-day delivery). Does not directly affect demand, except through rollout. Example Instruments Distance to Nearest Fulfillment Center (FC) Closer cities are cheaper to serve ‚Üí more likely to get one-day rollout. Conditional on city demand trends, distance itself doesn‚Äôt directly change customer ordering. Weather Disruptions / Storm Patterns If rollout timing depends on logistical feasibility (e.g., avoiding regions with winter weather bottlenecks), this variation can serve as an instrument. Logistical Capacity Constraints Cities where delivery stations had available excess capacity vs.¬†constrained ones. Historical Route Density Areas with high density of delivery routes may be more likely to get rollout ‚Äî conditional on demand, this variable proxies operational feasibility, not demand growth. Assumptions Exclusion Restriction: The instrument (Z) must affect the outcome (Y) only through its effect on the treatment (T). Relevance Condition: The instrument needs to be correlated with treatment variable, which is rollout cities. üîπ Example in the Amazon 1-Day Delivery Rollout Instrument: Distance to nearest fulfillment center. Treatment: Whether the city received 1-day delivery. Outcome: Orders per customer. ‚úÖ Valid if: Distance influences orders only because it changes the likelihood of 1-day delivery. ‚ùå Invalid if: Distance also affects delivery cost and therefore pricing, or if rural/urban proximity to FC changes customer demand for unrelated reasons. If rollout selection was endogenous, I‚Äôd consider an instrumental variables approach. For example, distance to the nearest fulfillment center is a strong predictor of whether a city can support one-day delivery. That satisfies the relevance condition ‚Äî closer cities are more likely to be rolled out first. The more challenging part is the exclusion restriction: distance should affect customer orders only through its impact on rollout, not directly. If distance is correlated with other demand drivers like urban density or competition, that could violate the assumption. I‚Äôd address this by controlling for observables and checking robustness, and if multiple instruments are available, I‚Äôd run overidentification tests. In practice, I‚Äôd test instrument strength using the first-stage regression, making sure the F-statistic is comfortably above 10 to avoid weak instruments. I‚Äôd also check whether the instrument predicts pre-treatment outcomes ‚Äî if it does, that would raise concerns about independence. Finally, IV also relies on monotonicity: the instrument should push all units in the same direction (e.g., no city becomes less likely to get rollout just because it‚Äôs closer to a fulfillment center). That‚Äôs typically a theoretical assumption but important to state. If these assumptions hold, I‚Äôd use 2SLS: the first stage predicts rollout using distance, and the second stage estimates the causal effect of rollout on outcomes like orders and revenue. This approach helps isolate a credible causal effect even when rollout isn‚Äôt random. üëâ Key is: Show you know when IV is appropriate (endogeneity, selection bias). Give realistic instruments (distance to FC, logistics capacity). Acknowledge assumptions (exclusion restriction). 17.5 Step 3: Tech Depth ‚Äî Example with Difference-in-Differences If pressed, go deep on one (Amazon loves DiD for these questions): Setup: \\[ Y_{it} = \\alpha + \\beta \\cdot Treatment_{it} + \\gamma_i + \\delta_t + \\epsilon_{it} \\] \\(Y_{it}\\): outcome for city i at time t (e.g., orders per capita). \\(Treatment_{it}\\): indicator for cities after rollout. \\(\\gamma_i\\): city fixed effects (control for time-invariant city differences). \\(\\delta_t\\): time fixed effects (control for global shocks like seasonality). \\(\\beta\\): causal effect of 1-day delivery. Parallel Trends Assumption: Treated and control cities would have followed similar trends without the program. You‚Äôd test this with pre-trends. Robustness Checks: Add covariates (city size, Prime penetration). Check for heterogeneous effects (small vs.¬†large cities). Placebo tests (pretend rollout earlier). 17.6 Step 3 with RCTs 17.6.1 üîπ Design Choices Unit of randomization: city, ZIP code, or customer. Blocking/stratification: ensure balance on baseline order volume, Prime penetration, region. Stepped-wedge design: randomize order of rollout if Amazon doesn‚Äôt want to hold back treatment indefinitely. Guarding against spillovers: buffer zones between treated and control ZIPs. 17.6.2 üîπ Assumptions SUTVA (no interference) ‚Üí one unit‚Äôs treatment shouldn‚Äôt affect another. Need cluster-level randomization if spillovers exist. Stable treatment ‚Üí customers offered ‚Äú1-day‚Äù see the same version (not free for some, paid for others). Compliance ‚Üí customers in treatment group may not use 1-day; customers in control might still access it. Handle with ITT (intent-to-treat) and IV (assignment ‚Üí actual usage). 17.6.3 üîπ Measurement Plan Primary outcomes: orders per customer, revenue, contribution margin. Secondary outcomes: Prime retention, customer satisfaction, delivery costs. Timing: define ramp-up period vs.¬†stable period for evaluation. Heterogeneity: look at effects by city size, income level, Prime vs.¬†non-Prime. 17.6.4 üîπ Estimation Difference-in-means if balanced. Regression framework to increase precision: \\[ Y_i = \\alpha + \\beta \\cdot Treatment_i + \\gamma X_i + \\epsilon_i \\] Cluster-robust SEs at the randomization level (city or ZIP). ITT: average effect of being assigned treatment. LATE: if compliance issues, estimate via IV using assignment as instrument for actual usage. 17.6.5 üîπ Power and Sample Size City-level RCT: fewer clusters, higher intra-cluster correlation ‚Üí need careful power analysis. Customer-level RCT: more units, easier to detect smaller effects. Rule of thumb: detect +3‚Äì5% lift in orders ‚Üí need enough units to get 80% power at 5% significance. 17.6.6 üîπ Threats and Mitigations Spillovers ‚Üí cluster-level assignment + buffer ZIPs. Noncompliance ‚Üí ITT + IV. Seasonality ‚Üí block by time or include time FE. Attrition ‚Üí track if customers in control ‚Äúdrop out‚Äù differently. If I could design it, the cleanest evaluation would be a randomized controlled trial. For example, we could randomize ZIP codes to get 1-day delivery, with blocking on baseline demand and Prime penetration. To avoid spillovers, I‚Äôd use buffer zones between treated and control areas. The primary outcomes would be orders per customer, revenue, and contribution margin, with Prime retention as a secondary outcome. I‚Äôd analyze intent-to-treat using a regression with cluster-robust SEs. If some customers don‚Äôt comply ‚Äî for example, they‚Äôre eligible but don‚Äôt use 1-day ‚Äî I‚Äôd also estimate the local average treatment effect using assignment as an instrument. Finally, I‚Äôd check power carefully: at the city level, the number of clusters is small, so stepped-wedge or customer-level randomization might be better. This way, the RCT gives us a credible causal estimate while accounting for real-world constraints. üëâ In short, you can go as deep as: Design ‚Üí Assumptions ‚Üí Outcomes ‚Üí Estimation ‚Üí Power ‚Üí Threats ‚Üí Mitigation. 17.7 Step 4: Communicate Results When they ask ‚Äúwhat would you look at?‚Äù, emphasize: Direct Effect: more orders, higher revenue. Customer Value: repeat purchase, Prime retention. Efficiency: did cost per order go up or down? Net Impact: did 1-day cannibalize 2-day, or add new demand? ‚úÖ Good Interview Flow: Clarify outcomes (business impact). Lay out multiple approaches (breadth). Dive into one rigorously (depth). Acknowledge assumptions + limitations. Tie back to business implications. 17.8 More on Instrumental Variables üîπ What is Overidentification? In IV, you need at least as many instruments as endogenous variables (this is ‚Äújust identified‚Äù). If you have more instruments than endogenous variables, the model is overidentified. Example: Suppose treatment = rollout of 1-day delivery. If you have two instruments ‚Äî (1) distance to fulfillment center and (2) local logistical capacity ‚Äî then you‚Äôre overidentified. üîπ Why it Matters With more instruments than needed, you can test whether the instruments are consistent with each other. If they all satisfy the exclusion restriction, they should give the same causal estimate. If not, at least one instrument is invalid. The Overidentification Test Hansen J-test (robust) or Sargan test (classic). Null hypothesis: all instruments are valid (exogenous). If p-value is low ‚Üí reject the null ‚Üí at least one instrument likely violates exclusion restriction. Example in Amazon Case Treatment: Whether a city got one-day rollout. Instruments: Distance to nearest fulfillment center. Historical route density (proxy for logistical feasibility). If both are valid instruments, the IV estimate of rollout effect should be consistent across them. Overidentification test helps check that. If I had more than one instrument ‚Äî say, distance to fulfillment center and logistical capacity ‚Äî the model would be overidentified. That allows me to run an overidentification test, like the Hansen J-test. The null is that all instruments are valid. If the test rejects, it suggests at least one instrument violates the exclusion restriction. Of course, it‚Äôs not a perfect guarantee, but it provides evidence about instrument validity.‚Äù üëâ So in short: Overidentified = more instruments than needed. Test = Hansen J or Sargan. Purpose = check whether exclusion restriction holds across instruments. Testing Assumptions 1. Relevance (testable) What it is: Instrument is correlated with treatment. How to test: Look at the first-stage regression. \\[ T_i = \\pi_0 + \\pi_1 Z_i + \\pi_2 X_i + \\nu_i \\] \\(T_i\\): treatment (e.g., rollout city). \\(Z_i\\): instrument (e.g., distance to FC). Check: F-statistic for \\(\\pi_1\\) &gt; 10 (rule of thumb). Weak instruments ‚Üí biased estimates. 2. Exclusion Restriction (not directly testable) What it is: Instrument affects outcome only through treatment. How to check credibility: Theoretical argument: Why would ‚Äúdistance to FC‚Äù affect orders only through rollout? Balance checks: See if instrument is correlated with baseline outcomes or covariates (e.g., cities closer to FC also richer?). If so, risk of violation. Overidentification tests (if &gt;1 instrument): Hansen J-test or Sargan test. Null: all instruments valid. If rejected ‚Üí at least one violates exclusion. 3. Independence (as-if random) What it is: Instrument is independent of unobserved factors affecting outcomes. How to check: Test if instrument predicts pre-treatment outcomes. If distance to FC predicts pre-rollout order volume, then it‚Äôs confounded. üîπ 4. Monotonicity (no defiers) What it is: Instrument pushes all units in the same direction (no one is less likely to get treatment because of the instrument). How to check: Can‚Äôt test directly. Rely on logic. Example: Closer distance can‚Äôt make a city less likely to get one-day delivery. 17.9 More Questions 17.9.1 In ab test, how do you communicate significance level change, 1%, 5% to non-technicals? Translate Significance into ‚ÄúRisk of False Alarm‚Äù 5% significance ‚Üí we‚Äôre okay with a 1 in 20 chance that the result looks real but isn‚Äôt. 1% significance ‚Üí we‚Äôre stricter: only 1 in 100 chance of a false alarm. üëâ Analogy: ‚ÄúIf we flip a coin, and it comes up heads five times in a row, would you believe it‚Äôs a weighted coin? At 5% significance, we‚Äôd say yes sooner. At 1%, we‚Äôd demand even more evidence.‚Äù Framing for Business Leaders At 5%, we‚Äôre balancing speed and certainty ‚Äî good for most product/marketing tests. At 1%, we‚Äôre asking for stronger proof ‚Äî good when the decision is costly or risky (e.g., changing logistics, pricing). So: ‚ÄúLowering the significance threshold reduces the chance of being fooled by random noise, but requires more data or a bigger effect size before we call something a win.‚Äù How to Phrase It in Plain English ‚ÄúAt 5%, we‚Äôre saying we‚Äôre comfortable being wrong 1 in 20 times. At 1%, we‚Äôre saying we only want to be wrong 1 in 100 times. The stricter we are, the more confident we are in the result ‚Äî but it also means we might need a larger sample or stronger effect before we see significance.‚Äù Business framing: ‚ÄúThink of it as how strict a referee is. At 5%, we allow close calls to count as fouls. At 1%, the referee only calls fouls when it‚Äôs really obvious. That reduces false alarms but sometimes misses subtle, real effects.‚Äù ‚ÄúTo a non-technical audience, I‚Äôd explain significance as the risk of a false alarm. At 5% significance, we‚Äôre okay being wrong 1 in 20 times; at 1%, we‚Äôre stricter and only accept being wrong 1 in 100 times. Lowering the threshold gives us more confidence in the result, but it means we need more evidence ‚Äî either a bigger effect or a larger sample size. I‚Äôd frame it as a trade-off between speed and certainty, so stakeholders can align the level of risk with the importance of the decision.‚Äù üëâ This way you show you can bridge statistical rigor with business communication, which is exactly what Amazon will test you on. 17.9.2 In synthetic control how do you calculate weigths? Core Idea You want to build a synthetic twin for a treated unit (e.g., Seattle after 1-day rollout) by combining multiple untreated units (e.g., Portland, Denver, Kansas City) with weights that make their pre-treatment trajectory as close as possible to Seattle‚Äôs. The weights (\\(w_j\\)) are chosen so that the synthetic city matches the treated city in: Pre-treatment outcomes (e.g., orders per capita, revenue, retention). Predictors (e.g., population, income, Prime penetration). How Weights Are Calculated Formally: Suppose treated city = \\(i^*\\). Potential controls = \\(j = 1, \\dots, J\\). We want weights \\(w_j\\) (nonnegative, sum to 1). Synthetic control outcome: \\[ Y_{it}^{SC} = \\sum_{j=1}^J w_j Y_{jt} \\] Goal: Choose \\(w_j\\) to minimize the distance between pre-treatment outcomes of treated vs.¬†synthetic: \\[ \\min_{w} \\; (X_{i^*} - \\sum_{j=1}^J w_j X_j)&#39; V (X_{i^*} - \\sum_{j=1}^J w_j X_j) \\] where: \\(X_{i^*}\\) = vector of predictors for treated city. \\(X_j\\) = predictors for control cities. \\(V\\) = weighting matrix that reflects importance of predictors (chosen via cross-validation or researcher judgment). Intuition If Seattle had pre-treatment outcomes [10, 12, 15], and Portland [11, 13, 14], Denver [9, 11, 16], Kansas City [8, 10, 15]‚Ä¶ The algorithm finds weights like 0.5 Portland + 0.3 Denver + 0.2 Kansas City = [10.1, 12.2, 15.0]. That‚Äôs almost identical to Seattle‚Äôs pre-trends ‚Üí good synthetic twin. Tools / Implementation R: Synth package. Python: SyntheticControlMethods or econml. The software automates the minimization problem and produces weights + gaps. In synthetic control, we calculate weights by finding the convex combination of control cities that best matches the treated city‚Äôs pre-treatment outcomes and characteristics. The weights are nonnegative and sum to one. The algorithm solves an optimization problem: minimize the difference between treated and synthetic in the pre-treatment period, using a weighted distance measure. After rollout, the gap between the treated city and its synthetic twin gives the causal effect. For example, Seattle might be represented as 50% Portland, 30% Denver, and 20% Kansas City if that best reproduces its pre-treatment trajectory. üëâ So: Mathematically ‚Üí optimization problem with constraints. Intuitively ‚Üí weighted average of control cities to mimic treated city. Practically ‚Üí solved by Synth or similar packages. 17.9.3 How do you make sure your analysis is good? Internal Validity Checks Parallel trends / pre-trends (for DiD): Check that treated and control cities had similar outcome trends before rollout. If not, adjust with controls, matching, or synthetic control. Placebo tests: Pretend rollout happened earlier and see if you still get an effect (shouldn‚Äôt). Balance checks: Make sure treatment and control groups are comparable on observables (population, Prime %, baseline demand). Robustness to specification: Try different model forms (levels vs.¬†logs, alternative time windows) and see if results are consistent. Statistical Validity Standard errors: Use clustering (e.g., at city level) to account for correlation. Multiple testing / false positives: If testing many outcomes, adjust or at least note risk. Power: Confirm sample size is sufficient to detect the effect you care about. External Validity Heterogeneity: Test if effect varies by city size, region, Prime penetration, customer demographics. Helps tell the business where rollout is most valuable. Scalability: Ask: does the effect in these 20‚Äì30 cities generalize to the rest of the country? If not, what adjustments might be needed? Operational / Business Sense Check Order of magnitude check: If the model says ‚Äú1-day delivery increased orders by 50%,‚Äù is that realistic? Cross-check with survey results, pilot data, or industry benchmarks. Consistency across metrics: If orders went up but contribution margin plummeted, something‚Äôs off‚Äîdig deeper. Trace through mechanism: Did faster delivery actually improve retention, basket size, Prime sign-ups? Evidence should align with the story. Communicating Reliability When talking to non-technical stakeholders: Instead of ‚Äúp &lt; 0.05,‚Äù say: ‚ÄúWe‚Äôre 95% confident this isn‚Äôt due to chance.‚Äù Emphasize consistency: ‚ÄúNo matter how we cut the data‚Äîby time period, city size, or statistical model‚Äîthe effect size is similar.‚Äù Highlight what‚Äôs directionally robust (sign and order of magnitude), even if precise estimates shift a bit. I‚Äôd make sure the analysis is valid by first checking assumptions‚Äîlike parallel trends in a DiD‚Äîand running placebo tests to see if I find effects where none should exist. I‚Äôd test robustness by trying different model forms and checking heterogeneity across cities. On the statistical side, I‚Äôd use clustered standard errors and check power. Finally, I‚Äôd do a business sanity check‚Äîdo the numbers make sense compared to historical demand and benchmarks? When I share results, I don‚Äôt just show a coefficient, I show robustness across methods and whether the effect is consistent with business logic. 17.9.4 What if leadership says implement his program in big cities? Reframe the Question Leadership says: ‚ÄúOnly big cities should get the program.‚Äù That means rollout is non-random and likely correlated with demand drivers (population, density, income, existing infrastructure). The key challenge = selection bias ‚Üí big cities are systematically different. How to Adjust Analysis Difference-in-Differences with Big City Controls Use other big cities not yet treated as control group. Still check parallel pre-trends: do big rollout cities look like big non-rollout cities before the program? Synthetic Control Build synthetic versions of rollout cities using a weighted combo of other big cities. Helps mimic ‚Äúwhat would have happened‚Äù in the absence of rollout. Matching or Weighting Match rollout cities with similar non-rollout big cities on observables (population, Prime penetration, demand history). Use balancing weights to adjust comparisons. Instrumental Variables (if feasible) If rollout timing among big cities is staggered, or depends on something like proximity to FCs, use that as an instrument. Communicating the Challenge When explaining to leadership / non-technical: ‚ÄúSince rollout is limited to large cities, we can‚Äôt just compare treated vs.¬†untreated cities directly‚Äîbigger cities naturally have higher demand. To get a fair estimate, I‚Äôd compare treated big cities to similar big cities that haven‚Äôt rolled out yet, or construct a synthetic benchmark. This helps us isolate the true effect of faster delivery, not just the fact that big cities behave differently.‚Äù If leadership only implements in big cities, selection bias becomes a concern since big cities differ from smaller ones. I‚Äôd adjust by comparing rollout cities only against similar large cities not yet treated, checking pre-trends carefully. If needed, I‚Äôd use synthetic control or matching to build a valid counterfactual. That way, even though we can‚Äôt randomize, we still get a credible estimate of the causal effect. 17.9.5 What if leadership want to do it within east and west coast? Recognize the New Challenge Rollout limited to coastal cities. East/West Coast cities differ systematically from Midwest/South (population density, urbanization, shipping routes, demographics). Direct comparisons would be biased if you don‚Äôt account for these geographic differences. Analytical Adjustments Within-region DiD Compare treated vs.¬†untreated cities within the same coast. Example: rollout city in East Coast vs.¬†similar East Coast control city. Controls for geography-specific demand drivers. Staggered Rollout Leverage If rollout timing differs within coasts, exploit that variation ‚Üí compare early adopters vs.¬†later adopters on the same coast. Synthetic Control / Matching Construct a synthetic version of New York rollout using weighted combo of other East Coast cities not rolled out. Same for San Francisco using other West Coast cities. Control Variables Explicitly control for coast-wide economic shocks (e.g., hurricanes, port strikes, regional seasonality). This addresses region-specific confounders. Communicating to Leadership ‚ÄúIf rollout is restricted to the coasts, we need to be careful not to attribute natural coastal differences to the program. I‚Äôd evaluate using only coastal cities as comparisons‚Äîtreated vs.¬†untreated within East and West. Where possible, I‚Äôd use staggered timing or synthetic controls to create credible counterfactuals. That way, even though the rollout is geographically concentrated, we still get a clean read on the program‚Äôs causal impact.‚Äù ‚ÄúIf rollout is only on the East and West Coast, I‚Äôd avoid comparing against inland cities since those differ structurally. Instead, I‚Äôd design the evaluation within each coast, comparing treated cities to untreated coastal peers or using synthetic controls. If there‚Äôs staggered rollout, I‚Äôd leverage that timing. This ensures we‚Äôre isolating the effect of 1-day delivery, not just regional demand differences.‚Äù 17.9.6 How I determine sample size, significance, and MDE Effect Size The minimum meaningful difference we care about detecting (e.g., 2% uplift in orders, $1 increase in revenue per customer). Should be business-driven: what size of change justifies program cost? How I‚Äôd phrase in an interview: ‚ÄúI‚Äôd align effect size with business impact. For example, if a 2% uplift in conversions covers the cost of rollout, that becomes the effect size I power the test around.‚Äù Power (1 ‚Äì Œ≤) Probability of detecting a true effect when it exists (commonly 80%). Balances sample size and effect size. Significance (Œ±) Probability of Type I error (false positive). Common levels: 5% (standard), 1% (stricter). Tighter significance ‚Üí need larger sample or stronger effect. Sample Size Driven by: baseline variability, expected effect size, desired confidence. Larger sample = more precise estimate. Formula-wise: depends on variance of outcome, desired power (often 80%), and significance level (Œ±). How I‚Äôd explain it simply: ‚ÄúSample size tells us how many observations we need to confidently detect an effect. The more variable the outcome, or the smaller the effect we expect, the more data we need.‚Äù Putting It Together (Interview Version) ‚ÄúWhen planning an evaluation, I‚Äôd first work with business teams to define the effect size that matters‚Äîsay, a 2% uplift that makes the program profitable. Then I‚Äôd calculate the sample size required to detect that effect with 80% power at a 5% significance level. If leadership wants stricter confidence, like 1%, we‚Äôd need either more data or accept that only larger effects will show up as significant. This way, the statistics are tied directly to the business decision.‚Äù 17.9.7 How do you communicate 10-15% significance to non-technicals Translate into everyday risk At 5% significance: ‚ÄúWe‚Äôre okay being wrong 1 in 20 times.‚Äù At 10% significance: ‚ÄúWe‚Äôre okay being wrong 1 in 10 times.‚Äù At 15% significance: ‚ÄúWe‚Äôre okay being wrong about 1 in 7 times.‚Äù üëâ So the trade-off is: more chance of a false alarm, but faster and easier to declare a result. Use a business analogy ‚ÄúIf we set the bar at 5%, we only act when we‚Äôre 95% sure it‚Äôs real. At 10‚Äì15%, we act when we‚Äôre about 85‚Äì90% sure. That means we might make a few more false calls, but it lets us move faster when time is valuable.‚Äù Frame as speed vs.¬†certainty Stricter (5%): safer, but needs more data ‚Üí longer, costlier test. Looser (10‚Äì15%): faster insights, but with more risk of being wrong. üëâ Useful when: Testing is expensive or slow. Business needs quick directional answers. You‚Äôre in an exploratory phase, not final rollout. Contextualize with decision stakes ‚ÄúIf the cost of a false positive is high‚Äîlike rolling out a bad product‚Äîwe stick to 5%.‚Äù ‚ÄúIf the cost is low‚Äîlike trying a new ad creative‚Äîwe may accept 10‚Äì15% to move faster.‚Äù ‚úÖ One-liner for interviews / non-techs: ‚ÄúAt 10‚Äì15% significance, we‚Äôre trading some confidence for speed. Instead of being 95% sure, we‚Äôre about 85‚Äì90% sure, which means quicker tests but more risk of a false positive. Whether that‚Äôs okay depends on how costly a wrong decision is.‚Äù "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

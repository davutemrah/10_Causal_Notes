[["index.html", "Notes on Causal Models Acknowledgement", " Notes on Causal Models DEA 2024-08-30 Acknowledgement I would like to acknowledge the following sources that have greatly contributed to the preparation of this notebook on causal models. I have collected and prepared this collection to keep myself up-to-date and to serve as a resource for reviewing all aspects of causal models. Please use it at your own risk and let me know if you have any comments, find any errors, or notice any typos. Here are the excellent online resources and references in no particular order: Program Evaluation Causal Inference for the Brave and True A guide on Data analysis The Mixtape The Effect What if "],["causal-models.html", "Chapter 1 Causal Models 1.1 Concepts 1.2 Directed Acyclic Graphs (DAGs) 1.3 Bad Controls 1.4 External and Internal Validity in Econometrics 1.5 Endogeneity 1.6 Reduced Form Model 1.7 Standard Errors 1.8 Types of Biases", " Chapter 1 Causal Models Hypothesis Testing Experiments Difference in Differences Synthetic Control Resampling Techniques 1.1 Concepts 1.1.1 Goodness of Fit I would encourage you not to fixate on R-squared in research projects where the aim is to estimate some causal effect, though. It’s a useful summary measure, but it does not tell us about causality. Remember, you aren’t trying to explain variation in \\(y\\) if you are trying to estimate some causal effect. The \\(R^2\\) tells us how much of the variation in \\(y\\) is explained by the explanatory variables. But if we are interested in the causal effect of a single variable, \\(R^2\\) is irrelevant 1.1.2 Robustness checks and validation methods Robustness checks and validation methods are essential aspects of evaluating the reliability and credibility of empirical research findings, including those derived from the Synthetic Control Method (SCM). Although they are related and sometimes overlap, they serve distinct purposes in the research process. Here’s a detailed explanation of the differences between them and why each is important: 1.1.2.1 Robustness Checks Definition: Robustness checks are procedures used to assess the sensitivity and stability of research findings to various assumptions, model specifications, and data perturbations. The goal is to determine whether the results hold under different conditions and to identify any potential weaknesses in the analysis. Purpose: Assess Stability: Ensure that the findings are not unduly influenced by specific choices made in the analysis (e.g., selection of control units, predictor variables). Identify Key Drivers: Determine which aspects of the model or data are most influential in driving the results. Evaluate Generalizability: Check whether the results are consistent across different sub-samples or alternative model specifications. 1.1.2.2 Validation Methods Definition: Validation methods are procedures used to confirm that the analytical approach and findings are credible and correctly specified. The goal is to ensure that the methodology accurately captures the causal relationship of interest and that the results are not artifacts of methodological flaws. Purpose: Establish Credibility: Demonstrate that the research design and methods are sound and that the findings are credible. Detect Biases: Identify and correct any biases or errors in the analysis that could distort the results. Provide Evidence for Causal Claims: Strengthen the argument that the observed effects are truly caused by the intervention rather than other factors. 1.1.2.3 Differences Between Robustness Checks and Validation Methods Scope and Focus: Robustness Checks: Focus on testing the sensitivity of the results to various assumptions and choices within the study. They address questions like “Do the results change if we tweak the model or data in specific ways?” Validation Methods: Focus on verifying the correctness and credibility of the methodology and findings. They address questions like “Is the methodology sound and are the findings believable?” When Applied: Robustness Checks: Often applied after the main analysis to ensure the findings are not artifacts of specific decisions or assumptions. Validation Methods: Applied throughout the research process to ensure that the approach is valid and the results are credible from the outset. Why Both Are Important: Robustness Checks: Credibility: Helps build confidence that the results are not fragile or overly dependent on specific conditions. Transparency: Provides a clear understanding of how various factors influence the findings. Comprehensive Insight: Identifies which components of the analysis are most crucial and robust. Validation Methods: Reliability: Ensures that the methodological approach is correct and that the findings are not due to methodological flaws. Accuracy: Confirms that the causal claims are well-founded and not spurious. Scientific Rigor: Strengthens the overall validity of the research by providing multiple lines of evidence supporting the findings. Conclusion Robustness checks and validation methods are complementary approaches that together enhance the credibility and reliability of research findings. Robustness checks focus on the sensitivity and stability of the results, while validation methods ensure the correctness and credibility of the methodology. Both are crucial for demonstrating that the findings are both reliable and valid, thereby providing a comprehensive evaluation of the research’s strength and integrity. 1.2 Directed Acyclic Graphs (DAGs) causality runs in one direction, it runs forward in time. There are no cycles in a DAG. To show reverse causality, one would need to create multiple nodes, most likely with two versions of the same node separated by a time index. To handle either simultaneity or reverse causality, it is recommended that you take a completely different approach to the problem than the one presented in this chapter. DAGs explain causality in terms of counterfactuals. That is, a causal effect is defined as a comparison between two states of the world—one state that actually happened when some intervention took on some value and another state that didn’t happen (the “counterfactual”) under some other intervention. Arrows represent a causal effect between two random variables moving in the intuitive direction of the arrow. The direction of the arrow captures the direction of causality. Causal effects can happen in two ways. They can either be direct (e.g., D -&gt; Y), or they can be mediated by a third variable (e.g., D -&gt; X -&gt; Y). When they are mediated by a third variable, we are capturing a sequence of events originating with , which may or may not be important to you depending on the question you’re asking. A complete DAG will have all direct causal effects among the variables in the graph as well as all common causes of any pair of variables in the graph. 1.2.1 Confounder Direct path is causal: D -&gt; Y; Backdoor path is not causal: X -&gt; D and X -&gt; Y Backdoor path, it is a process that creates spurious correlations between D and Y that are driven solely by fluctuations in the X random variable. Therefore, not controlling for a variable like that in a regression creates omitted variable bias, leaving a backdoor open creates bias. We therefore call X a confounder because it jointly determines D and Y, and so confounds our ability to discern the effect of D on Y in naı̈ve comparisons. Think of the backdoor path like this: Sometimes when D takes on different values, Y takes on different values because D causes Y. But sometimes D and Y take on different values because X takes on different values, and that bit of the correlation between D and Y is purely spurious. The existence of two causal pathways is contained within the correlation between D and Y. When X is observed and put in the model, then the backdoor path is closed. 1.2.2 Collider Direct path is causal: D -&gt; Y; Backdoor path is not causal: D -&gt; X and Y -&gt; X Like above, there are two ways to get to Y from D. X is a collider along this backdoor path because D and the causal effects of Y collide at X. Colliders are special in part because when they appear along a backdoor path, that backdoor path is closed simply because of their presence. 1.2.3 What to do Open backdoor paths introduce omitted variable bias and, the bias is so bad that it flips the sign entirely. Our goal is to close these backdoor paths. And if we can close all of the open backdoor paths, then we can isolate the causal effect of D on Y using one of the research designs and identification strategies discussed in this book. 1.2.4 How to do First, if you have a confounder that has created an open backdoor path, then you can close that path by conditioning on the confounder. Conditioning requires holding the variable fixed using something like subclassification, matching, regression, or another method. It is equivalent to “controlling for” the variable in a regression. The second way to close a backdoor path is the appearance of a collider along that backdoor path. Since colliders always close backdoor paths, and conditioning on a collider always opens a backdoor path, choosing to ignore the colliders is part of your overall strategy to estimate the causal effect itself. By not conditioning on a collider, you will have closed that backdoor path and that takes you closer to your larger ambition to isolate some causal effect. Backdoor Criterian: if there is a confounder, then control for it; if there is a collider, then keep it outside your model Avoid controlling for a collider in your model. To identify a collider, carefully analyze the directions and relationships between variables. If a variable acts as a collider (i.e., it is influenced by both the treatment and the outcome), including it in your model can introduce bias. Therefore, do not include colliders in your analysis. Sample Selection and collider bias lets assume that ability and beauty is independent, but both required for being an star actor. It can be shown in a simulation that the collider bias has created a negative correlation between talent and beauty in the non-movie-star sample as well. Yet we know that there is in fact no relationship between the two variables. This kind of sample selection creates spurious correlations. A random sample of the full population would be sufficient to show that there is no relationship between the two variables, but splitting the sample into movie stars only, we introduce spurious correlations between the two variables of interest. 1.3 Bad Controls Joshua Angrist, a prominent economist known for his work in econometrics, discusses “bad controls” in the context of causal inference. “Bad controls” are variables that, when included in a regression model, can introduce bias rather than help control for it. Here are the key points on how Angrist addresses “bad controls”: 1.3.1 Key Points on “Bad Controls” by Joshua Angrist: Definition of Bad Controls: Bad controls are variables that are themselves affected by the treatment or are post-treatment variables. Including these in your model can distort the causal relationship between the treatment and the outcome. They can also be variables that are endogenous, meaning they are correlated with the error term, leading to biased and inconsistent estimates. Examples of Bad Controls: Variables that are outcomes of the treatment: If a variable is influenced by the treatment, including it as a control can create spurious correlations. Colliders: Variables that are influenced by both the treatment and the outcome. Controlling for colliders can open a backdoor path, leading to biased estimates. Why Bad Controls are Problematic: Including bad controls can lead to incorrect inferences about the causal effect of the treatment. They can introduce bias by creating or amplifying spurious relationships. Identifying Good Controls: Good controls are variables that help to isolate the causal effect by accounting for confounding factors. These are typically pre-treatment variables that influence the outcome but are not influenced by the treatment. Best Practices: Focus on pre-treatment variables that are potential confounders: Variables that affect both the treatment and the outcome but are not affected by the treatment. Use robustness checks to ensure that the inclusion of controls does not unduly influence the estimates. 1.3.2 Example from Angrist and Pischke’s “Mostly Harmless Econometrics”: In “Mostly Harmless Econometrics,” Angrist and Pischke illustrate these concepts with practical examples. For instance, they explain how including a variable like “post-treatment earnings” in a model where the treatment is “education level” can be a bad control. This is because earnings are influenced by education (the treatment), and controlling for it can obscure the true effect of education on other outcomes. 1.3.3 Practical Advice: When building your regression model, carefully consider whether each control variable is a confounder, a mediator, or a collider. Avoid including variables that lie on the causal path between the treatment and the outcome (mediators). Avoid controlling for variables that are outcomes of the treatment or are influenced by both the treatment and the outcome (colliders). 1.3.4 Summary: Joshua Angrist emphasizes the importance of identifying and avoiding bad controls in regression models to ensure unbiased causal inference. By focusing on appropriate pre-treatment controls and being wary of endogenous variables and post-treatment variables, researchers can make more accurate and reliable causal claims. 1.3.5 Unobserved Variable Affecting Only the Dependent Variable If you have an unobserved variable that affects only the dependent variable and not the independent variables, the primary concern is increased variability in the error term, but it does not bias the coefficient estimates of the independent variables. Here’s a more detailed explanation: No Endogeneity Problem: Since the unobserved variable does not affect the independent variables, it does not create a correlation between the independent variables and the error term. Hence, it does not introduce endogeneity, and the OLS estimates of the coefficients remain unbiased and consistent. Increased Variance in Error Term: The presence of an unobserved variable affecting only the dependent variable will increase the variability (variance) of the error term. This leads to less precise (more variable) estimates of the coefficients, but these estimates are still unbiased. Standard Errors: Due to the increased variability in the error term, the standard errors of the estimated coefficients will be larger, resulting in wider confidence intervals and potentially less statistical power to detect significant effects. In summary, while the presence of such an unobserved variable does not introduce bias into the coefficient estimates, it affects the precision of these estimates, leading to larger standard errors. 1.4 External and Internal Validity in Econometrics 1.4.1 Internal Validity Internal validity refers to the extent to which a study accurately establishes a causal relationship between the treatment (independent variable) and the outcome (dependent variable) within the context of the study. In other words, it measures how well the study avoids biases and errors that can lead to incorrect conclusions about causal relationships. 1.4.1.1 Key Points on Internal Validity: Causal Inference: Internal validity ensures that the observed effects on the outcome can be confidently attributed to the treatment and not to other confounding factors. Elimination of Bias: It involves controlling for confounding variables, avoiding omitted variable bias, ensuring proper randomization, and addressing issues like measurement error and simultaneity bias. Common Threats: Confounding Variables: Variables that are correlated with both the treatment and the outcome. Selection Bias: Non-random assignment of subjects to treatment and control groups. Measurement Error: Inaccurate measurement of variables. Attrition: Loss of participants during the study. Reverse Causality: Difficulty in determining the direction of causality. Omitted Variable Bias: Failing to include a relevant variable that affects both the treatment and the outcome. 1.4.1.2 Examples in Econometrics: Randomized Controlled Trials (RCTs): RCTs are considered the gold standard for internal validity because random assignment of treatment ensures that confounding variables are evenly distributed across treatment and control groups. Instrumental Variables (IV): Using instruments to address endogeneity helps ensure that the treatment effect is not biased by omitted variables or reverse causality. 1.4.2 External Validity External validity refers to the extent to which the results of a study can be generalized beyond the specific context of the study to other settings, populations, times, and circumstances. It measures the applicability of the study’s findings to real-world scenarios outside the study environment. 1.4.2.1 Key Points on External Validity: Generalizability: Ensures that the conclusions drawn from the study sample can be applied to the broader population or different contexts. Population Validity: The degree to which the study sample represents the target population. Ecological Validity: The extent to which study findings can be generalized to other settings or environments. Temporal Validity: Whether the results hold over different time periods. 1.4.2.2 Common Threats to External Validity: Non-representative Samples: If the study sample is not representative of the target population, the findings may not be generalizable. Specific Contexts: Results from a specific geographic location, industry, or demographic may not apply elsewhere. Temporal Changes: Changes over time in technology, behavior, or policy can limit the generalizability of findings from past studies. 1.4.2.3 Examples in Econometrics: Field Experiments: Conducting experiments in real-world settings can enhance external validity compared to laboratory experiments. Replication Studies: Replicating studies in different contexts and with different populations helps assess the robustness and generalizability of findings. Heterogeneous Treatment Effects: Analyzing how treatment effects vary across different subgroups can provide insights into the external validity of the findings. 1.4.3 Balancing Internal and External Validity There is often a trade-off between internal and external validity: High Internal Validity: Studies with strong internal validity, such as RCTs, often have controlled environments that may limit generalizability. High External Validity: Observational studies and natural experiments might have higher external validity because they are conducted in real-world settings, but they may suffer from issues related to internal validity due to uncontrolled confounding variables. 1.4.3.1 Practical Considerations: Study Design: Carefully design studies to address both internal and external validity. For instance, use randomization to enhance internal validity while selecting a representative sample to improve external validity. Mixed Methods: Combining different methodological approaches, such as RCTs for internal validity and observational studies for external validity, can provide a more comprehensive understanding of causal relationships. Transparency and Replication: Ensure transparency in research design and analysis, and encourage replication studies to verify findings across different contexts and populations. By understanding and addressing both internal and external validity, researchers can produce more reliable and applicable econometric analyses that contribute to evidence-based decision-making. 1.5 Endogeneity Endogeneity refers to a situation in econometrics where an explanatory variable is correlated with the error term in a regression model. This correlation can lead to biased and inconsistent estimates of the coefficients, making it difficult to establish causal relationships. 1.5.1 Sources of Endogeneity: Omitted Variable Bias: When a relevant variable that affects both the dependent and independent variables is left out of the model, its effect is captured by the error term, leading to endogeneity. Measurement Error: Errors in measuring the independent variable can cause it to be correlated with the error term. Simultaneity (Reverse Causality): When the independent variable and the dependent variable mutually influence each other, leading to a two-way causation. 1.5.2 Consequences of Endogeneity: Biased Estimates: The estimated coefficients do not accurately reflect the true relationship between the variables. Inconsistent Estimates: As the sample size increases, the estimates do not converge to the true population parameters. 1.5.3 Methods to Address Endogeneity: Instrumental Variables (IV): Instrumental Variables: Use variables (instruments) that are correlated with the endogenous explanatory variable but uncorrelated with the error term. Two-Stage Least Squares (2SLS): First stage involves regressing the endogenous variable on the instruments. The second stage uses the predicted values from the first stage as the independent variable in the main regression. Fixed Effects Models: Panel Data: Use fixed effects to control for time-invariant unobserved heterogeneity. Difference-in-Differences (DiD): Control for unobserved confounding by comparing changes over time between treatment and control groups. Control Function Approach: Include the residuals from the first stage regression of the endogenous variable on instruments in the second stage regression to account for endogeneity. Natural Experiments: Utilize exogenous variations caused by external events or policies that affect the treatment variable but are unrelated to the error term. 1.6 Reduced Form Model Reduced Form Models refer to econometric models where the endogenous variables are expressed solely in terms of exogenous variables and error terms. These models simplify the relationship between variables by avoiding the need to specify the underlying structural model, focusing instead on the observed correlations. 1.6.1 Characteristics of Reduced Form Models: Simplified Representation: Reduced form models express endogenous variables directly as functions of exogenous variables and error terms. Focus on Exogeneity: They rely on exogenous variation to identify causal effects, avoiding direct specification of the structural relationships between variables. 1.6.2 Uses of Reduced Form Models: Policy Evaluation: Reduced form models are often used in policy evaluation to estimate the causal impact of policies by leveraging exogenous variation. Instrumental Variables: In IV estimation, the first stage regression (predicting the endogenous variable with instruments) is a reduced form model. Natural Experiments: Reduced form models are frequently used in natural experiments where exogenous shocks provide a source of variation. 1.6.3 Example of a Reduced Form Model: Suppose we want to estimate the impact of education (\\(E\\)) on earnings (\\(Y\\)): Structural Model: \\[ Y = \\alpha + \\beta E + \\epsilon \\] Endogeneity Problem: Education (\\(E\\)) might be endogenous due to omitted variables like ability or family background. Reduced Form Model: Use an instrument \\(Z\\) (e.g., proximity to a college) that affects education but is exogenous with respect to earnings: \\(E = \\pi_0 + \\pi_1 Z + \\nu\\) The reduced form equation for earnings in terms of the instrument: \\(Y = \\gamma_0 + \\gamma_1 Z + \\eta\\) Here, \\(\\gamma_1\\) provides an estimate of the causal effect of $ Z $ on $ Y $, which, under certain conditions, can be used to infer the effect of \\(E\\) on \\(Y\\) through \\(Z\\). In summary, understanding and addressing endogeneity is crucial for accurate causal inference in econometrics. Reduced form models provide a simplified framework to estimate relationships using exogenous variation, often serving as a preliminary step before more complex structural modeling. 1.7 Standard Errors Homoskedasticity Assumption: In linear regression, we assume that the variance of the error term is constant across all levels of the independent variables, i.e., \\(Var(\\epsilon | X) = \\sigma^2\\). Violation: If there is heteroscedasticity (non-constant variance of errors), the OLS estimates remain unbiased, but they are no longer efficient, and the standard errors are biased, leading to unreliable hypothesis tests. Heteroscedasticity-Robust standard errors or Generalized Least Squares (GLS) can be used to address heteroscedasticity. Eiker-Huber-White: Heteroscedasticity-Robust standard errors Cluster-robust standard errors (geographic units) Without homoskedasticity assumption, OLS estimator will still be unbiased but not efficient. Robust standard error usage will not change the OLS estimator but will change the standard errors. Without constant variance, mean squared errors are not minimum anymore. Estimated standard errors are biased. In real life, errors will mostly be heteroskedastic Solution for heteroskedasticity is mostly known as ‘robust’ standard errors. 1.7.1 heteroskedasticity-consistent standard errors Also known as robust standard errors or The sandwich standard error estimator, is a technique used to obtain valid standard errors in the presence of heteroskedasticity. These standard errors are “robust” because they do not assume that the error terms have constant variance (homoscedasticity), making them useful for hypothesis testing and confidence intervals when the usual OLS assumptions are violated. 1.7.2 Why Use Sandwich Standard Errors? In OLS regression, if the assumption of homoscedasticity is violated (i.e., the error variance is not constant), the usual standard errors of the estimated coefficients are biased. This bias can lead to incorrect inferences, such as invalid hypothesis tests and confidence intervals. Sandwich standard errors correct for this bias, providing more reliable inference. 1.7.2.1 How It Works The sandwich estimator adjusts the standard errors of the OLS estimates to account for heteroscedasticity. The name “sandwich” comes from the structure of the formula, where the “bread” parts are the matrices that involve the model’s design matrix, and the “meat” part is a matrix involving the residuals. 1.7.3 Clustering Standard Errors In the real world, though, you can never assume that errors are independent draws from the same distribution. You need to know how your variables were constructed in the first place in order to choose the correct error structure for calculating your standard errors. If you have aggregate variables, like class size, then you’ll need to cluster at that level. If some treatment occurred at the state level, then you’ll need to cluster at that level. When the units of analysis are clustered into groups and the researcher suspects that the errors are correlated within (but not across) groups, it may be appropriate to employ variance estimators that are robust to the clustered nature of the data. When we cluster standard errors at the state level, we allow for arbitrary serial correlation within state. multi way clustering 1.7.3.1 When Should You Adjust Standard Errors for Clustering? Abadie et al 2022 Formally, clustered standard errors adjust for the correlations induced by sampling the outcome variable from a data-generating process with unobserved cluster- level components. Source The authors argue that there are two reasons for clustering standard errors: 1- a sampling design reason, which arises because you have sampled data from a population using clustered sampling, and want to say something about the broader population; 2- and an experimental design reason, where the assignment mechanism for some causal treatment of interest is clustered. Let me go through each in turn, by way of examples, and end with some of their takeaways. A Sampling Design reason Consider running a simple Mincer earnings regression of the form: Log(wages) = a + byears of schooling + cexperience + d*experience^2 + e You present this model, and are deciding whether to cluster the standard errors. Referee 1 tells you “the wage residual is likely to be correlated within local labor markets, so you should cluster your standard errors by state or village.”. But referee 2 argues “The wage residual is likely to be correlated for people working in the same industry, so you should cluster your standard errors by industry”, and referee 3 argues that “the wage residual is likely to be correlated by age cohort, so you should cluster your standard errors by cohort”. What should you do? Under the sampling perspective, what matters for clustering is how the sample was selected and whether there are clusters in the population of interest that are not represented in the sample. So, we can imagine different scenarios here: You want to say something about the association between schooling and wages in a particular population, and are using a random sample of workers from this population. Then there is no need to adjust the standard errors for clustering at all, even if clustering would change the standard errors. The sample was selected by randomly sampling 100 towns and villages from within the country, and then randomly sampling people in each; and your goal is to say something about the return to education in the overall population. Here you should cluster standard errors by village, since there are villages in the population of interest beyond those seen in the sample. This same logic makes it clear why you generally wouldn’t cluster by age cohort (it seems unlikely that we would randomly sample some age cohorts and not others, and then try and say something about all ages); and that we would only want to cluster by industry if the sample was drawn by randomly selecting a sample of industries, and then sampling individuals from within each. Even in the second case, Abadie et al. note that both the usual robust (Eicker-Huber-White or EHW) standard errors, and the clustered standard errors (which they call Liang-Zeger or LZ standard errors) can both be correct, it is just that they are correct for different estimands. That is, if you are content on just saying something about the particular sample of individuals you have, without trying to generalize to the population, the EHW standard errors are all you need; but if you want to say something about the broader population, the LZ standard errors are necessary. The Experimental Design Reason for Clustering The second reason for clustering is the one we are probably more familiar with, which is when clusters of units, rather than individual units, are assigned to a treatment. Let’s take the same equation as above, but assume that we have a binary treatment that assigns more schooling to people. So now we have: Log(wages) = a +b*Treatment + e Then if the treatment is assigned at the individual level, there is no need to cluster (*). There has been much confusion about this, as Chris Blattman explored in two earlier posts about this issue (the fabulously titled clusterjerk and clusterjerk the sequel), and I still occasionally get referees suggesting I try clustering by industry or something similar in an individually-randomized experiment. This Abadie et al. paper is now finally a good reference to explain why this is not necessary. (*) unless you are using multiple time periods, and then you will want to cluster by individual, since the unit of randomization is individual, and not individual-time period. What about if your treatment is assigned at the village level. Then cluster by village. This is also why you want to cluster difference-in-differences at the state-level when you have a source of variation that comes from differences across states, and why a “treatment” like being on one side of a border vs the other is problematic (because you have only 2 clusters). 1.8 Types of Biases In econometrics and statistical analysis, various types of biases can affect the validity and reliability of estimates and inferences. Here are some common types of bias: 1.8.1 1. Selection Bias Definition: Occurs when the sample is not representative of the population due to non-random selection of observations. Examples: Sample Selection Bias: When individuals are selected into the sample based on criteria related to the outcome of interest (e.g., studying the impact of education on earnings but only sampling employed individuals). Survivorship Bias: When only surviving or existing subjects are included in the analysis, ignoring those that have dropped out or failed (e.g., analyzing the performance of mutual funds without including funds that have closed). 1.8.2 2. Omitted Variable Bias Definition: Arises when a relevant variable that affects both the dependent and independent variables is left out of the model, causing the included variables to capture the effect of the omitted variable. Examples: - Studying the effect of education on earnings without controlling for ability, where ability affects both education and earnings. 1.8.3 3. Measurement Bias Definition: Occurs when there are errors in measuring the variables, leading to inaccuracies in the estimated relationships. Examples: Systematic Measurement Error: Consistent, predictable errors (e.g., a scale that always adds 2 pounds). Random Measurement Error: Errors that vary without pattern (e.g., human errors in data entry). 1.8.4 4. Response Bias Definition: Happens when respondents give inaccurate or false answers, often due to social desirability, recall issues, or misunderstanding the questions. Examples: - Survey participants underreporting their alcohol consumption due to social desirability. 1.8.5 5. Attrition Bias Definition: Results from systematic differences between those who drop out of a study and those who remain, leading to a non-representative sample over time. Examples: - A long-term study on the effects of a diet where participants who do not see results drop out, leaving only those who benefit. 1.8.6 6. Publication Bias Definition: Arises when studies with significant or positive results are more likely to be published than studies with non-significant or negative results. Examples: - Meta-analyses showing inflated effects due to the exclusion of unpublished studies with null results. 1.8.7 7. Survivorship Bias Definition: Occurs when only successful subjects or cases are considered, ignoring those that failed or were excluded from the sample. Examples: Analyzing the performance of companies that are currently listed on the stock exchange, ignoring those that went bankrupt. 1.8.8 8. Recall Bias Definition: Happens when participants do not remember past events accurately, leading to inaccurate data. Examples: - Patients in a medical study failing to accurately recall past health behaviors. 1.8.9 9. Confirmation Bias Definition: The tendency to search for, interpret, and remember information that confirms pre-existing beliefs, leading to biased outcomes. Examples: - Researchers focusing on data that supports their hypothesis while disregarding data that contradicts it. 1.8.10 10. Confounding Bias Definition: Occurs when the effect of the primary explanatory variable on the outcome is mixed with the effect of another variable (confounder) that is related to both the explanatory variable and the outcome. Examples: - Studying the effect of smoking on lung cancer without controlling for age, where age is a confounder. 1.8.11 11. Endogeneity Bias Definition: Arises when an explanatory variable is correlated with the error term in the model, often due to omitted variables, measurement error, or reverse causality. Examples: - Estimating the impact of education on earnings without accounting for the fact that higher ability individuals are more likely to obtain more education and earn higher wages. 1.8.12 12. Non-Response Bias Definition: Occurs when individuals who do not respond to a survey differ in meaningful ways from those who do respond. Examples: - A survey on household income where higher-income households are less likely to respond, skewing results towards lower-income households. 1.8.13 13. Observer Bias Definition: Happens when researchers’ expectations or knowledge influence the outcome of the study or the interpretation of results. Examples: - A researcher subtly influencing participants’ responses in a study on therapy effectiveness due to their belief in the therapy’s efficacy. 1.8.14 14. Overfitting Bias Definition: In predictive modeling, overfitting occurs when the model is too complex and captures noise rather than the underlying relationship, leading to poor generalization to new data. Examples: - A regression model with too many parameters that fits the training data very well but performs poorly on validation data. 1.8.15 Addressing Biases To mitigate these biases, researchers can use various strategies, including: - Randomization: Randomly assigning subjects to treatment and control groups to avoid selection bias and confounding. - Control Groups: Including control groups to help identify causal effects. - Instrumental Variables: Using instruments to address endogeneity. - Robustness Checks: Performing sensitivity analyses to check the stability of results under different assumptions. - Data Cleaning and Validation: Ensuring accurate measurement and data entry. - Blinding: Keeping participants and researchers unaware of group assignments to reduce observer and response biases. Understanding and addressing these biases is crucial for improving the validity and reliability of econometric analyses. 1.8.16 Self-Selection Bias Self-Selection Bias occurs when individuals select themselves into a group, causing a non-random sample that may not be representative of the population. This type of bias can severely impact the validity of causal inferences because the differences in outcomes between groups may be driven by the self-selection process rather than the treatment or intervention itself. 1.8.16.1 Examples of Self-Selection Bias: Online Surveys: If participation in an online survey is voluntary, those who choose to respond may have different characteristics or opinions compared to those who do not participate. Program Participation: When studying the impact of a job training program, individuals who opt into the program might be more motivated or have better baseline skills than those who do not, leading to biased estimates of the program’s effectiveness. Product Reviews: Customers who leave reviews for a product might have extreme opinions (either very positive or very negative), while those with moderate opinions are less likely to leave a review, skewing the perception of the product’s quality. Health Studies: People who enroll in health studies or clinical trials might be more health-conscious or have a particular interest in their health, which could lead to differences in health outcomes compared to the general population. 1.8.16.2 Addressing Self-Selection Bias Randomized Controlled Trials (RCTs): The gold standard for addressing self-selection bias is to conduct an RCT where participants are randomly assigned to treatment and control groups, ensuring any differences in outcomes are due to the intervention. Propensity Score Matching (PSM): This method involves matching participants in the treatment group with similar participants in the control group based on a set of observed characteristics, aiming to mimic randomization. Instrumental Variables (IV): Using instruments that affect participation in the treatment but do not directly affect the outcome can help isolate the causal effect of the treatment by accounting for the self-selection. Heckman Correction (Selection Models): The Heckman two-step correction involves modeling the selection process and then incorporating this model into the outcome equation to correct for self-selection bias. Difference-in-Differences (DiD): This approach compares changes in outcomes over time between a treatment group and a control group, assuming that any differences in trends can be attributed to the intervention. Control Variables: Including control variables in the regression model that capture the factors influencing self-selection can help mitigate bias, although it relies on the assumption that all relevant factors are observed and included. 1.8.16.3 Conclusion Self-selection bias is a common challenge in observational studies where individuals choose their own treatment or participation status. It can lead to biased and inconsistent estimates if not properly addressed. Methods such as propensity score matching, instrumental variables, and randomized controlled trials can help mitigate self-selection bias and provide more reliable causal inferences. "],["potential-outcomes.html", "Chapter 2 Potential Outcomes 2.1 Key Concepts 2.2 Assumptions for Identifying Causal Effects 2.3 Methods for Estimating Causal Effects 2.4 Example 2.5 On how parameters are calculated", " Chapter 2 Potential Outcomes The potential outcomes framework, often associated with the Rubin Causal Model (RCM), is a powerful method for defining and estimating causal effects. In this framework, a causal effect is understood through the comparison of potential outcomes under different treatment conditions. 2.1 Key Concepts Potential Outcomes: For each unit (e.g., individual, group, or entity), there are two potential outcomes: \\(Y{_i}^1\\): The outcome if the unit receives the treatment. \\(Y{_i}^0\\): The outcome if the unit does not receive the treatment. These outcomes are also known as counterfactual outcomes because they represent hypothetical scenarios that cannot be simultaneously observed. Observed Outcome: For each unit, we can only observe one of these potential outcomes depending on the treatment assignment. This is expressed using the switching equation: \\[ Y_{i} = D_i \\cdot Y{_i}^1 + (1 - D_i) \\cdot Y{_i}^0 \\] where \\(Y\\) is the observed outcome and \\(D\\) is the treatment indicator (\\(D = 1\\) if the unit receives the treatment and \\(D = 0\\) if the unit does not). 2.1.1 Causal Effect The individual causal effect for a unit \\(i\\) is defined as the difference between its two potential outcomes: \\[ \\text{Causal Effect}_i = Y_i ^1 - Y_i ^0 \\] However, because we can only observe one of these outcomes for each unit, we typically focus on average causal effects across a population. 2.1.2 Average Treatment Effect (ATE) The Average Treatment Effect (ATE) is the expected difference in outcomes if all units were treated versus if none were treated: \\[ \\text{ATE} = E[Y_i ^1] - E[Y_i ^0] \\] 2.1.3 Average Treatment Effect on the Treated (ATT) The Average Treatment Effect on the Treated (ATT) is the average causal effect for those units that actually received the treatment: \\[ \\text{ATT} = E[Y(1) | D = 1] - E[Y(0) | D = 1] \\] 2.1.4 The Fundamental Problem of Causal Inference A major challenge in causal inference is that we can never observe both potential outcomes for the same unit simultaneously. This is known as the fundamental problem of causal inference. Therefore, we rely on assumptions and statistical methods to estimate causal effects. 2.2 Assumptions for Identifying Causal Effects Several assumptions can help identify causal effects: 2.2.1 Independence The independence assumption, also known as the unconfoundedness or ignorability assumption, is crucial in causal inference: \\[[Y^0, Y^1] \\perp D\\] This notation means that the potential outcomes \\((Y^0, Y^1)\\) are independent of the treatment assignment \\(D\\). In other words, the treatment is assigned randomly with respect to the potential outcomes. This ensures that any difference in outcomes between treated and control groups can be attributed to the treatment itself, rather than other factors. It means there are no unobserved confounders. However, in real-world scenarios, human-based sorting and decision-making processes often violate this assumption. People self-select into treatments based on various observed and unobserved characteristics, leading to non-random assignment. As a result, naïve observational comparisons—which do not account for this non-randomness—are almost always incapable of accurately recovering causal effects. To address this issue, researchers use various methods such as randomized controlled trials (RCTs), matching techniques, instrumental variables, and regression adjustment to attempt to approximate random assignment and thus make valid causal inferences. 2.2.1.1 Conditional Independence \\[[Y^0, Y^1] \\perp D \\mid X \\] This assumption implies that conditional on covariates \\(X\\), the treatment assignment \\(D\\) is independent of the potential outcomes. Treatment can be assigned conditionally on covariates. For example state assign student to three classes randomly but schools chosen first, then students are assigned randomly later. The treatment assignment was only conditionally random. When treatment assignment had been conditional on observable variables, it is a situation of selection on observables. 2.2.2 Stable Unit Treatment Value Assumption (SUTVA) SUTVA is a critical assumption in causal inference and has two main components: No Interference: The potential outcomes for any unit are unaffected by the treatment status of other units. This means the treatment effect on one unit does not spill over to affect another unit. Consistency: The observed outcome for a unit under the treatment received is the same as the potential outcome under that treatment. This means that if a unit receives the treatment, its observed outcome should match the potential outcome we would expect if it had received that treatment. Implications of SUTVA Homogeneous Treatment: SUTVA implies that the treatment is administered uniformly across all units. In practice, this assumption can be violated if, for instance, the effectiveness of a treatment varies due to differences in how it is delivered. For example, if some doctors are better surgeons than others, the “dose” of the treatment (surgery) is not homogeneous. No Externalities (No Spillovers): SUTVA assumes there are no externalities, meaning that the treatment of one unit does not affect the outcomes of other units. If unit 1 receives the treatment and this somehow affects unit 2’s outcome, this would be a violation of SUTVA. We are assuming away such spillover effects to ensure that the treatment effect can be isolated and accurately measured. No general equilibrium effects Violations of SUTVA can lead to biased estimates of causal effects, so it is essential to consider these assumptions carefully and take appropriate steps to address potential violations when conducting causal inference. 2.3 Methods for Estimating Causal Effects Several methods can be used to estimate causal effects under the potential outcomes framework: Randomized Controlled Trials (RCTs): Random assignment ensures that the treatment and control groups are comparable, allowing for unbiased estimation of the Average Treatment Effect (ATE). This is often considered the gold standard for causal inference. Matching: Pairing treated and control units with similar covariates to estimate the treatment effect. This method attempts to simulate a randomized experiment by creating a sample of units that received the treatment and a comparable sample that did not. Regression Adjustment: Using regression models to adjust for differences in covariates between treated and control groups. This method helps control for confounding variables by including them in the regression model to isolate the treatment effect. Instrumental Variables (IV): Using instruments that affect the treatment assignment but are not related to the potential outcomes, except through the treatment. This method is useful when there is concern about endogeneity or unobserved confounding variables. Difference-in-Differences (DiD): Comparing the changes in outcomes over time between treated and control groups to account for time-invariant unobserved heterogeneity. This method is useful for evaluating the effect of a treatment or intervention that is implemented at a specific point in time. Regression Discontinuity (RD): Exploiting a cutoff or threshold in the assignment of treatment to estimate the causal effect. Units just above and below the cutoff are assumed to be comparable, allowing for a local estimation of the treatment effect. Synthetic Control Method: Constructing a weighted combination of control units to create a synthetic control group that approximates the characteristics of the treated group. This method is particularly useful for case studies and evaluating the impact of interventions in a single treated unit. These methods provide a robust toolkit for estimating causal effects and addressing various challenges in observational data analysis. 2.4 Example Let’s consider an example to illustrate the potential outcomes framework: Scenario: We want to estimate the effect of a job training program (treatment) on participants’ earnings. Potential Outcomes: \\(Y_i(1)\\): Earnings of individual \\(i\\) if they participate in the job training program. \\(Y_i(0)\\): Earnings of individual \\(i\\) if they do not participate in the job training program. Observed Outcome: If individual \\(i\\) participates in the program (\\(D_i = 1\\)), we observe \\(Y_i = Y_i(1)\\). If individual \\(i\\) does not participate (\\(D_i = 0\\)), we observe \\(Y_i = Y_i(0)\\). Objective: Estimate the ATE of the job training program on earnings: \\[ \\text{ATE} = E[Y(1)] - E[Y(0)] \\] In practice, we might use matching or regression adjustment to control for covariates that affect both participation in the program and earnings, helping us to estimate the causal effect more accurately. 2.4.1 Simple Difference Method The simple difference method is one of the basic approaches to estimating causal effects in observational studies. It compares the average outcomes of a treatment group and a control group. This method is straightforward but relies on the assumption that the two groups are comparable in all respects except for the treatment. 2.4.1.1 Key Concepts Treatment Group: The group that receives the treatment or intervention. Control Group: The group that does not receive the treatment or intervention. 2.4.1.2 Steps to Implement the Simple Difference Method Identify Treatment and Control Groups: Define the groups that have received the treatment (treatment group) and those that have not (control group). Calculate Average Outcomes: Compute the average outcome for the treatment group (\\(\\bar{Y}_T\\)). Compute the average outcome for the control group (\\(\\bar{Y}_C\\)). Compute the Difference: The estimated treatment effect is the difference between the average outcomes of the treatment and control groups: \\[\\hat{\\delta} = \\bar{Y}_T - \\bar{Y}_C \\] 2.4.1.3 Assumptions The simple difference method assumes that the treatment and control groups are comparable, meaning that any difference in outcomes is solely due to the treatment. This assumption is often referred to as the strong ignorability assumption. No Confounding Variables: There are no unobserved factors that influence both the treatment assignment and the outcome. Homogeneity: The treatment effect is constant across all individuals in the population. 2.4.1.4 Limitations Selection Bias: If individuals self-select into the treatment group based on characteristics that also affect the outcome, the estimate will be biased. Confounding Variables: If there are unobserved confounders that affect both the treatment and the outcome, the simple difference method will not provide a valid estimate of the causal effect. 2.4.1.5 Example Let’s illustrate the simple difference method with an example. Scenario: We want to estimate the effect of a job training program on participants’ earnings. Data: Treatment group: Participants of the job training program. Control group: Non-participants of the job training program. Outcome: Earnings after the program. Average Outcomes: Average earnings for the treatment group (\\(\\bar{Y}_T\\)): $50,000 Average earnings for the control group (\\(\\bar{Y}_C\\)): $45,000 Compute the Difference: The estimated treatment effect: $ = {Y}_T - {Y}_C = 50,000 - 45,000 = 5,000 $ Interpretation: The job training program is estimated to increase earnings by $5,000 on average. 2.4.1.6 Addressing Limitations To address the limitations of the simple difference method, researchers can use more sophisticated techniques that control for confounding variables and selection bias: Randomized Controlled Trials (RCTs): Random assignment of treatment can ensure comparability between treatment and control groups. Matching Methods: Match treatment and control units based on observed covariates to create comparable groups. Regression Adjustment: Use regression models to control for observed covariates that may confound the relationship between treatment and outcome. Instrumental Variables (IV): Use instruments that are correlated with the treatment but not directly with the outcome to account for unobserved confounders. Difference-in-Differences (DiD): Compare changes in outcomes over time between treatment and control groups to account fortime-invariant unobserved heterogeneity. 2.4.2 Conclusion The simple difference method provides an intuitive way to estimate causal effects by comparing the average outcomes of treatment and control groups. However, its validity relies on the strong assumption that the groups are comparable in all respects except for the treatment. In practice, researchers often need to use more advanced techniques to address potential biases and confounding factors. 2.5 On how parameters are calculated 2.5.1 Propensity Score Matching (PSM) and Maximum Likelihood Estimation (MLE) Does PSM use Maximum Likelihood Estimation (MLE)? Yes, PSM typically uses logistic regression (or probit regression) to estimate propensity scores, and logistic regression uses MLE to estimate coefficients. Does every logistic regression use MLE? Yes, logistic regression commonly uses MLE to estimate the model parameters. 2.5.2 Logistic Regression 2.5.2.1 Objective Function and Loss Function Objective Function: The objective in logistic regression is to maximize the likelihood function, i.e., the probability of observing the given sample. Loss Function: The log-likelihood function is used as the loss function in logistic regression, which is minimized (or equivalently, the negative log-likelihood is maximized). 2.5.2.2 Calculating Coefficients Coefficients in logistic regression are estimated using MLE. The likelihood function for logistic regression is: \\[ L(\\beta) = \\prod_{i=1}^n P(y_i|\\mathbf{x}_i;\\beta)^{y_i}(1 - P(y_i|\\mathbf{x}_i;\\beta))^{1 - y_i} \\] where \\(P(y_i|\\mathbf{x}_i;\\beta) = \\frac{1}{1 + \\exp(-\\mathbf{x}_i^T \\beta)}\\). The log-likelihood function is: \\[ \\log L(\\beta) = \\sum_{i=1}^n \\left[ y_i \\log P(y_i|\\mathbf{x}_i;\\beta) + (1 - y_i) \\log (1 - P(y_i|\\mathbf{x}_i;\\beta)) \\right] \\] The parameters \\(\\beta\\) are estimated by maximizing this log-likelihood function. 2.5.2.3 Hypothesis Testing Z-test: Logistic regression typically uses z-tests to test hypotheses about the coefficients. Null hypothesis: The coefficient is equal to zero. The z-statistic is calculated as the coefficient estimate divided by its standard error: \\(z = \\frac{\\hat{\\beta}}{\\text{SE}(\\hat{\\beta})}\\) The p-value is derived from the standard normal distribution. 2.5.3 Ordinary Least Squares (OLS) Regression 2.5.3.1 Objective Function and Loss Function Objective Function: The objective in OLS regression is to minimize the sum of squared residuals. Loss Function: The loss function in OLS is the residual sum of squares (RSS): \\(RSS = \\sum_{i=1}^n (y_i - \\mathbf{x}_i^T \\beta)^2\\) 2.5.3.2 Calculating Coefficients Coefficients in OLS regression are estimated by minimizing the RSS. The normal equations derived from setting the gradient of RSS to zero are: \\(\\mathbf{\\hat{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\\) where \\(\\mathbf{X}\\) is the design matrix of predictors and \\(\\mathbf{y}\\) is the vector of observed outcomes. 2.5.3.3 Hypothesis Testing T-test: OLS regression typically uses t-tests to test hypotheses about the coefficients. Null hypothesis: The coefficient is equal to zero. The t-statistic is calculated as the coefficient estimate divided by its standard error: \\(t = \\frac{\\hat{\\beta}}{\\text{SE}(\\hat{\\beta})}\\) The p-value is derived from the t-distribution with \\(n - p - 1\\) degrees of freedom (where \\(p\\) is the number of predictors). "],["matching.html", "Chapter 3 Matching 3.1 Subclassification 3.2 Exact Matching 3.3 Approximate Matching Methods 3.4 Propensity Score Methods 3.5 Inverse Probability Weighting (Weighting on the propensity score) 3.6 Nearest-neighbor matching 3.7 Coarsened Exact Matching 3.8 A/B Test article from Medium 3.9 Task 1: Load the data 3.10 Task 2: Set up Hypothesis 3.11 Task 3: Compute the difference in the click-through rate", " Chapter 3 Matching 3.1 Subclassification Subclassification is a method used to satisfy the backdoor criterion by adjusting differences in means with strata-specific weights. These weights ensure that the distribution of means by strata matches the counterfactual’s strata distribution. This method addresses the problem when treatment assignment is random, conditional on observables. The assumption in mathematical notation is: \\[ [Y^0, Y^1] \\perp D \\mid X \\] This implies that, given covariates \\(X\\), the treatment assignment \\(D\\) is independent of the potential outcomes (random). Treatment can be assigned conditionally on covariates. For example, a state might assign students to three classes randomly, but first, schools are chosen, and then students are assigned randomly within those schools. 3.1.1 Example Consider a study investigating the impact of cigar type on mortality. Without considering age, it appears that cigar or pipe users have higher mortality rates, which is controversial. However, age is a crucial factor that influences both cigar type selection (treatment) and mortality rate (outcome), making it a confounder. In this method, my strategy for addressing covariate imbalance is to condition on age, ensuring that the age distribution is comparable between the treatment and control groups. 3.1.2 Step-by-Step Example Bin Age into Groups: Create age groups (e.g., 18-30, 31-45, 46-60, 61+). Calculate Percent Distribution: Determine the percentage of individuals in each age group for both treatment and control groups. Weighted Mortality Rate: Calculate the mortality rate for each age group within each treatment group. Use the age group percentages to calculate a weighted average mortality rate for each treatment group. 3.1.2.1 Implementation Suppose we have the following data: Age Group Treatment Control Treatment Deaths Control Deaths Treatment Total Control Total 18-30 20% 25% 10 15 100 150 31-45 30% 20% 20 10 150 100 46-60 25% 30% 30 20 125 150 61+ 25% 25% 40 30 125 125 Calculate Age-Specific Mortality Rates: \\[ \\text{Mortality Rate (18-30)} = \\frac{10}{100} = 10\\%, \\quad \\text{Control Mortality Rate (18-30)} = \\frac{15}{150} = 10\\% \\] (Repeat for other age groups.) Calculate Weighted Mortality Rates: \\[ \\text{Weighted Mortality Rate (Treatment)} = 0.20 \\times 10\\% + 0.30 \\times 13.3\\% + 0.25 \\times 24\\% + 0.25 \\times 32\\% \\] \\[ \\text{Weighted Mortality Rate (Control)} = 0.25 \\times 10\\% + 0.20 \\times 10\\% + 0.30 \\times 13.3\\% + 0.25 \\times 24\\% \\] 3.1.3 Considerations Choice of Variables: Deciding which variables to use for adjustment can be challenging. Including too many variables can lead to the “curse of dimensionality,” where the data becomes too sparse in higher dimensions. Common Support Assumption: This assumption requires that for each stratum, there exist observations in both the treatment and control groups. If the sample size is small, this assumption may be violated, making it difficult to compare groups effectively. By carefully choosing variables and ensuring sufficient sample sizes within strata, subclassification can effectively adjust for covariate imbalances and yield more accurate estimates of treatment effects. 3.2 Exact Matching Exact matching is a method used to estimate causal effects by pairing units in the treatment group with units in the control group that have identical values for certain covariates. This method helps in comparing the outcomes of similar units under different treatments to infer causal effects. Why? Because independence assumption is violated, and treatment assignment is not random. 3.2.1 Explanation Suppose we have a treatment group and a control group, and we want to estimate the treatment effect by finding exact matches based on a covariate. Matching Process: If a unit in the treatment group has a covariate value of 2, we look for a unit in the control group with the same covariate value of 2. If we find such a match, we use the outcome of the control unit to impute the counterfactual outcome for the treatment unit. Handling Multiple Matches: If there are multiple control units with the same covariate value as the treatment unit, we take the average of those control units’ outcomes to impute the counterfactual for the treatment unit. Calculating the Average Treatment Effect (ATE): By imputing counterfactual outcomes for each unit in both the control and treatment groups based on matching covariates, we can calculate the ATE. Calculating the Average Treatment Effect on the Treated (ATT): Typically, we find exact matching control units for treatment units and calculate the ATT. This involves comparing only the matched pairs. In a typical example, control group is much larger than treatment group and it is much easier to find similar treated units within a larger control unit. 3.2.2 Example Let’s consider an example where we are studying the effect of a new teaching method on student performance. We have two groups: students who received the new teaching method (treatment group) and students who received the traditional method (control group). We will use the exact matching method based on a covariate, such as prior test scores. 3.2.2.1 Step-by-Step Process Identify Covariate: Prior test scores are used as the matching covariate. Exact Matching: Suppose a student in the treatment group has a prior test score of 85. We look for students in the control group with a prior test score of 85. If we find multiple students in the control group with a prior test score of 85, we average their outcomes. Impute Counterfactuals: For the treatment student with a prior test score of 85, we use the average outcome of the matched control students to impute the counterfactual outcome. Create Matched Sample: The matched sample consists of pairs of treatment and control units with the same covariate value. For example, if we have another treatment student with a prior test score of 90, we find control students with a prior test score of 90 and repeat the process. Calculate ATT: For each matched pair, we calculate the difference in outcomes. Average these differences to obtain the ATT. 3.2.2.2 Example Data Student Group Prior Test Score Outcome (Final Score) A Treatment 85 90 B Treatment 90 88 C Control 85 85 D Control 90 86 E Control 85 87 For Student A (Treatment, 85): Match with Students C and E (Control, 85). Average outcome: (85 + 87) / 2 = 86. Imputed counterfactual for A: 86. For Student B (Treatment, 90): Match with Student D (Control, 90). Imputed counterfactual for B: 86. 3.2.2.3 ATT Calculation Difference for Student A: 90 - 86 = 4 Difference for Student B: 88 - 86 = 2 \\[ \\text{ATT} = \\frac{4 + 2}{2} = 3 \\] 3.2.3 Conclusion Exact matching helps in creating a comparable control group for each treatment unit based on covariates. By doing so, we can more accurately estimate the causal effect of the treatment. However, this method requires sufficient overlap between the covariate distributions of the treatment and control groups, and the common support assumption must be satisfied. 3.3 Approximate Matching Methods When you have multiple covariates to match or do not have exact matches, you can use approximate matching methods to find the best possible matches. 3.3.1 Nearest Neighbor Covariate Matching When the number of matching covariates exceeds one, we need a new definition of distance to measure closeness between units. Multiple covariates not only introduce the curse-of-dimensionality problem but also complicate the measurement of distance. This poses challenges for finding a good match in the data and demands a large sample size for the matching discrepancies to be trivially small. 3.3.1.1 Euclidean Distance Definition: Euclidean distance is a common measure of distance between two points in a multi-dimensional space. Problem: The distance measure depends on the scale of the variables, which can distort the true closeness between points. 3.3.1.2 Normalized Euclidean Distance Definition: This is the Euclidean distance normalized by the variance of the variables. Advantage: Normalizing by variance adjusts for differences in scale among the covariates, making the distance measure more accurate. 3.3.1.3 Mahalanobis Distance Definition: Mahalanobis distance is a scale-invariant distance metric that takes into account the correlations between variables. Advantage: It adjusts for the scale and correlations of the covariates, providing a more accurate measure of distance. 3.3.2 Example Suppose we are studying the impact of a job training program on employment outcomes. We have multiple covariates, such as age, education level, and prior work experience. We want to use approximate matching to find control units that are similar to the treatment units based on these covariates. 3.3.2.1 Step-by-Step Process Identify Covariates: Age Education Level Prior Work Experience Calculate Distances: Euclidean Distance: \\[\\text{Distance} = \\sqrt{(X_1 - Y_1)^2 + (X_2 - Y_2)^2 + \\cdots + (X_n - Y_n)^2}\\] Normalized Euclidean Distance: \\[\\text{Distance} = \\sqrt{\\left(\\frac{X_1 - Y_1}{\\sigma_1}\\right)^2 + \\left(\\frac{X_2 - Y_2}{\\sigma_2}\\right)^2 + \\cdots + \\left(\\frac{X_n - Y_n}{\\sigma_n}\\right)^2}\\] Mahalanobis Distance: \\[\\text{Distance} = \\sqrt{(X - Y)^T S^{-1} (X - Y)}\\] where \\(S\\) is the covariance matrix of the covariates. Find Nearest Neighbors: For each treatment unit, calculate the distance to all control units using the chosen distance metric. Select the control unit with the smallest distance as the match for the treatment unit. Calculate Treatment Effect: Compare the outcomes of matched pairs to estimate the treatment effect. 3.3.3 Hypothetical Data Unit Group Age Education Level Prior Work Experience Outcome 1 Treatment 25 Bachelor’s 2 years Employed 2 Treatment 30 Master’s 5 years Employed 3 Control 26 Bachelor’s 1 year Unemployed 4 Control 29 Master’s 6 years Employed 3.3.3.1 Matching Process Calculate Normalized Euclidean Distances: Normalize the covariates by their variances. Compute distances between each treatment unit and all control units. Match Units: Match Unit 1 (Treatment) with Unit 3 (Control) based on the smallest normalized Euclidean distance. Match Unit 2 (Treatment) with Unit 4 (Control) based on the smallest normalized Euclidean distance. Estimate Treatment Effect: Compare outcomes of matched pairs: Unit 1 (Treatment) vs. Unit 3 (Control): Employed vs. Unemployed Unit 2 (Treatment) vs. Unit 4 (Control): Employed vs. Employed By using approximate matching methods like nearest neighbor matching with normalized Euclidean or Mahalanobis distances, we can more accurately estimate the treatment effect even when dealing with multiple covariates and the lack of exact matches. 3.4 Propensity Score Methods Propensity score methods are approximate matching techniques that use propensity scores as distance metrics. These methods offer several ways to perform matching based on propensity scores. Propensity score matching (PSM) is a widely used method, particularly in medical sciences, for addressing selection on observables. However, it has not been as widely adopted among economists as other non-experimental methods like regression discontinuity or difference-in-differences. This reluctance is largely due to skepticism about the conditional independence assumption (CIA) being achievable in any dataset. Economists are often more concerned about selection on unobservables than selection on observables, making them less likely to use matching methods. 3.4.1 Concept Propensity score matching is used when a conditioning strategy can satisfy the backdoor criterion. The method involves estimating a model (usually logit or probit) to predict the conditional probability of treatment based on covariates. The predicted values from this estimation, called propensity scores, collapse the covariates into a single scalar. Comparisons between the treatment and control groups are then based on these propensity scores. 3.4.2 Steps Estimate Propensity Scores: Use a logit or probit model to estimate the probability of receiving the treatment based on observed covariates. Match Units: Match treatment units with control units that have similar propensity scores. Assess Overlap: Ensure that there is common support, meaning there are units in both treatment and control groups across the range of propensity scores. Calculate Treatment Effect: Compare outcomes between matched treatment and control units to estimate the treatment effect. 3.4.3 Example Suppose we are studying the effect of a job training program on employment outcomes. We have the following covariates: age, education level, and prior work experience. We will use propensity score matching to estimate the effect of the program. 3.4.3.1 Step-by-Step Process Estimate Propensity Scores: Fit a logit model to predict the probability of receiving the job training based on age, education level, and prior work experience. Example logit model: \\[P(Treatment) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\cdot Age + \\beta_2 \\cdot Education + \\beta_3 \\cdot Experience)}} \\] Calculate Propensity Scores: Use the fitted logit model to calculate the propensity score for each unit. Match Units: Match each treatment unit with one or more control units that have similar propensity scores. Example matching method: Nearest neighbor matching. NN matching is greedy in the sense that each pairing occurs without reference to how other units will be or have been paired, and therefore does not aim to optimize any criterion. Nearest neighbor matching is the most common form of matching used. For large datasets (i.e., in 10,000s to millions), some matching methods will be too slow to be used at scale. Instead, users should consider generalized full matching, subclassification, or coarsened exact matching, which are all very fast and designed to work with large datasets. (Nice article on MatchIt)[https://cran.r-project.org/web/packages/MatchIt/vignettes/matching-methods.html] Assess Overlap: Check for common support to ensure there is overlap in propensity scores between the treatment and control groups. If there is no overlap, adjust the model or consider other methods. Calculate Treatment Effect: Compare employment outcomes between matched treatment and control units. Calculate the average treatment effect on the treated (ATT). 3.4.3.2 Example Data Unit Group Age Education Level Prior Work Experience Outcome Propensity Score 1 Treatment 25 Bachelor’s 2 years Employed 0.70 2 Treatment 30 Master’s 5 years Employed 0.85 3 Control 26 Bachelor’s 1 year Unemployed 0.65 4 Control 29 Master’s 6 years Employed 0.80 Match Units: Match Unit 1 (Treatment) with Unit 3 (Control) based on similar propensity scores (0.70 vs. 0.65). Match Unit 2 (Treatment) with Unit 4 (Control) based on similar propensity scores (0.85 vs. 0.80). Not very intuitive or even confusing: There are many ways to use PS for matching Calculate ATT: Compare outcomes of matched pairs: - Unit 1 (Treatment) vs. Unit 3 (Control): Employed vs. Unemployed - Unit 2 (Treatment) vs. Unit 4 (Control): Employed vs. Employed 3.4.4 Assumptions and Considerations Conditional Independence Assumption (CIA): Treatment assignment is independent of potential outcomes given the observed covariates. This assumption is crucial for PSM to provide unbiased estimates. Common Support: There must be overlap in the distribution of propensity scores between the treatment and control groups. Lack of common support can lead to biased estimates as some treatment units may have no comparable control units. Model Specification: The logit or probit model must be correctly specified to accurately estimate propensity scores. Including relevant covariates and interactions is important for achieving balance between groups. Sample Size: PSM requires a sufficiently large sample size to find good matches for each treatment unit. Smaller samples may lead to poor matches and biased estimates. Propensity score matching is a powerful tool for estimating causal effects in observational studies, but it relies heavily on the assumptions of CIA and common support. Proper model specification and adequate sample size are essential for obtaining reliable estimates. 3.5 Inverse Probability Weighting (Weighting on the propensity score) It is weighting treatment and control units according to, which is causing units with very small values of the propensity score to blow up and become unusually influential in the calculation of ATT. Thus, we will need to trim the data. A good rule of thumb, they note, is to keep only observations on the interval [0.1,0.9], which was performed at the end of the program. We still need to calculate standard errors, such as based on a bootstrapping method, The sensitivity of inverse probability weighting to extreme values of the propensity score has led some researchers to propose an alternative that can handle extremes a bit better. Most software packages have programs that will estimate the sample analog of these inverse probability weighted parameters that use the second method with normalized weights. 3.6 Nearest-neighbor matching An alternative, very popular approach to inverse probability weighting is matching on the propensity score. This is often done by finding a couple of units with comparable propensity scores from the control unit donor pool within some ad hoc chosen radius distance of the treated unit’s own propensity score. The researcher then averages the outcomes and then assigns that average as an imputation to the original treated unit as a proxy for the potential outcome under counterfactual control. Then effort is made to enforce common support through trimming. Nevertheless, nearest-neighbor matching, along with inverse probability weighting, is perhaps the most common method for estimating a propensity score model. Nearest-neighbor matching using the propensity score pairs each treatment unit with one or more comparable control group units , where comparability is measured in terms of distance to the nearest propensity score. This control group unit’s outcome is then plugged into a matched sample. Once we have the matched sample, we can calculate the ATT as We will focus on the ATT because of the problems with overlap that we discussed earlier. We can chose to match using K nearest neighbors. Nearest neighbors, in other words, will find the K nearest units in the control group, where “nearest” is measured as closest on the propensity score itself. Unlike covariate matching, distance here is straightforward because of the dimension reduction afforded by the propensity score. We then average actual outcome, and match that average outcome to each treatment unit. Once we have that, we subtract each unit’s matched control from its treatment value, and then divide by N, the number of treatment units. 3.6.1 Example in R library(MatchIt) library(Zelig) m_out &lt;- matchit(treat ~ age + agesq + agecube + educ + educsq + marr + nodegree + black + hisp + re74 + re75 + u74 + u75 + interaction1, data = nsw_dw_cpscontrol, method = &quot;nearest&quot;, distance = &quot;logit&quot;, ratio =5) m_data &lt;- match.data(m_out) z_out &lt;- zelig(re78 ~ treat + age + agesq + agecube + educ + educsq + marr + nodegree + black + hisp + re74 + re75 + u74 + u75 + interaction1, model = &quot;ls&quot;, data = m_data) x_out &lt;- setx(z_out, treat = 0) x1_out &lt;- setx(z_out, treat = 1) s_out &lt;- sim(z_out, x = x_out, x1 = x1_out) summary(s_out) 3.7 Coarsened Exact Matching Coarsened Exact Matching (CEM) is a method based on the idea that exact matching can often be achieved by coarsening the data. Coarsening involves creating categorical variables (e.g., 0- to 10-year-olds, 11- to 20-year-olds), making it easier to find exact matches. Once exact matches are found, weights are calculated based on where a person fits within certain strata, and these weights are used in a simple weighted regression. This method can be implemented using the MatchIt library in R. 3.7.1 Example Consider the variable “schooling,” which can be categorized as: - Less than high school - High school only - Some college - College graduate - Post-college Each observation is placed into one of these categories, creating strata for each unique combination of observations. Assign these strata to the original (uncoarsened) data, and drop any observation whose stratum doesn’t contain at least one treated and one control unit. Then, add weights based on stratum size and analyze the data without further matching. 3.7.2 Steps in Coarsened Exact Matching Coarsen the Data: Transform continuous or detailed categorical variables into coarser categories. Example: Age groups (0-10, 11-20, etc.) or education levels (less than high school, high school, etc.). Create Strata: For each unique combination of the coarsened variables, create a stratum. Each observation is assigned to a stratum based on its coarsened characteristics. Assign Weights: Calculate weights for each observation based on the stratum size. Observations in larger strata receive smaller weights and vice versa. Drop Unmatched Observations: Remove any strata that do not contain both treated and control units. Weighted Regression: Use the weights in a regression analysis to account for the matching process. 3.7.3 Considerations Balance: Coarsening can improve balance between treated and control groups but may result in some loss of information. Weight Calculation: Weights should reflect the relative sizes of the strata to ensure accurate representation in the analysis. Implementation: Ensure the coarsening process does not oversimplify the data, potentially masking important variations. By carefully coarsening the data and using appropriate weights, CEM allows for more accurate and reliable estimation of treatment effects, even when exact matching on the original variables is not feasible. 3.8 A/B Test article from Medium 3.8.1 Example: Conversion Rate of an E-Commerce Website Article Source Suppose an e-commerce website wants to test if implementing a new feature (e.g., layout or button) will significantly improve conversion rate. conversion rate: number of purchases divided by number of sessions/visits We can randomly show the new webpage to 50% of the users. Then, we have a test group and a control group. Once we have enough data points, we can test if the conversion rate in the treatment group is significantly higher (one side test) than that in the control group. The null hypothesis is that conversion rates are not significantly different in the two group. Sample Size for Comparing Two Means. One way to perform the test is to calculate daily conversion rates for both the treatment and the control groups. Since the conversion rate in a group on a certain day represents a single data point, the sample size is actually the number of days. Thus, we will be testing the difference between the mean of daily conversion rates in each group across the testing period. The formula for estimate minimum sample size is as follows: Sample Size Estimate for A/B Test In an A/B test, the sample size (\\(n\\)) required for each group can be estimated using the formula: \\[n = \\frac{{2 \\cdot (Z_{\\alpha/2} + Z_{\\beta})^2 \\cdot \\sigma^2}}{{\\delta^2}}\\] where: $ n : $ $ Z_{/2} : $ $ Z_{} : $ $ ^2 : $ $ : $ This formula helps in determining the sample size needed to achieve desired levels of significance and power in an A/B test. For our example, let’s assume that the mean daily conversion rate for the past 6 months is 0.15 and the sample standard deviation is 0.05. With the new feature, we expect to see a 3% absolute increase in conversion rate. Thus, for the conversion rate for the treatment group will be 0.18. We also assume that the sample standard deviations are the same for the two group. Our parameters are as follows. \\(\\mu_1 = 0.15\\) \\(\\mu_2 = 0.18\\) \\(\\sigma_1 = \\sigma_2 = 0.05\\) Assuming \\(\\alpha = 0.05\\) and \\(\\beta = 0.20\\) (\\(power = 0.80\\)), applying the formula, the required minimum sample size is 35 days. This is consistent with the result from this web calculator. Sample Size for Comparing Two Proportions The two-means approach considers each day+group as a data point. But what if we focus on individual users and visits? What if we want to know how many visits/sessions are required for the testing? In this case, the conversion rate for a group is basically all purchases divided by all sessions in that group. If each session is a Bernoulli trial (convert or not), each group follows a binomial distribution. To test the difference in conversion rate between the treatment and control groups, we need a test of two proportions. The formula for estimating the minimum required sample size is as follows. Summary: Sample Size Estimate for Comparing Proportions When comparing proportions in two independent groups, the sample size (\\(n\\)) required for each group can be estimated using the formula: \\[n = \\frac{{2 \\cdot (Z_{\\alpha/2} + Z_{\\beta})^2 \\cdot (p(1-p))}}{{\\delta^2}}\\] where: \\(n : \\text{ Sample size per group}\\) \\(Z_{\\alpha/2} : \\text{ Critical value for significance level}\\) \\(Z_{\\beta} : \\text{ Critical value for desired power}\\) \\(p : \\text{ Expected proportion in one group}\\) \\(\\delta : \\text{ Minimum detectable difference in proportions}\\) This formula helps in determining the sample size needed to detect a specified difference in proportions between two groups with desired levels of significance and power. Assuming 50–50 split, we have the following parameters: \\(p_1 = 0.15\\) \\(p_2 = 0.18\\) \\(k = 1\\) Using \\(\\alpha = 0.05\\) and \\(\\beta = 0.20\\), applying the formula, the required sample size is \\(1,892\\) sessions per group. 3.8.2 Example: A/B Test A/B testing is an experiment where two or more variants are evaluated using statistical analysis to determine which variation performs better for a given conversion goal. A/B testing is widely used by digital marketing agencies as it is the most effective method to determine the best content for converting visits into sign-ups and purchases. In this scenario, you will set up hypothesis testing to advise a digital marketing agency on whether to adopt a new ad they designed for their client. Assume you are hired by a digital marketing agency to conduct an A/B test on a new ad hosted on a website. Your task is to determine whether the new ad outperforms the existing one. The agency has run an experiment where one group of users was shown the new ad design, while another group was shown the old ad design. The users’ interactions with the ads were observed, specifically whether they clicked on the ad or not. 3.9 Task 1: Load the data In this task, we will import our libraries and then load our dataset library(tidyverse) ## ── Attaching core tidyverse packages ────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.3 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.1 ## ── Conflicts ──────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors library(readxl) df &lt;- read_excel(&#39;/Users/deayan/Desktop/GITHUB/10_Causal_Notes/__repo/data/AB_Test.xlsx&#39;) glimpse(df) ## Rows: 3,757 ## Columns: 2 ## $ group &lt;chr&gt; &quot;experiment&quot;, &quot;control&quot;, &quot;control&quot;, &quot;control&quot;, &quot;control&quot;, &quot;cont… ## $ action &lt;chr&gt; &quot;view&quot;, &quot;view&quot;, &quot;view and click&quot;, &quot;view and click&quot;, &quot;view&quot;, &quot;vi… df%&gt;% group_by(group, action)%&gt;% summarise(n = n(), .groups = &quot;drop&quot;) ## # A tibble: 4 × 3 ## group action n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 control view 1513 ## 2 control view and click 363 ## 3 experiment view 1569 ## 4 experiment view and click 312 3.10 Task 2: Set up Hypothesis experiment group: the group that is involved in the new experiment . i.e the group that received the new ad . Control group: the 2nd group that didn’t receive the new ad Click-through rate (CTR): the number of clicks advertisers receive on their ads per number of impressions. table(df$group) ## ## control experiment ## 1876 1881 df%&gt;%count(group) ## # A tibble: 2 × 2 ## group n ## &lt;chr&gt; &lt;int&gt; ## 1 control 1876 ## 2 experiment 1881 table(df) ## action ## group view view and click ## control 1513 363 ## experiment 1569 312 prop.table(table(df), 1) ## action ## group view view and click ## control 0.8065032 0.1934968 ## experiment 0.8341308 0.1658692 df %&gt;% count(group) %&gt;% mutate(prop = n / sum(n)) ## # A tibble: 2 × 3 ## group n prop ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 control 1876 0.499 ## 2 experiment 1881 0.501 x &lt;- df %&gt;% group_by(group, action)%&gt;% summarise(n = n(), .groups = &#39;drop&#39;)%&gt;% pivot_wider(names_from = action, values_from = n, values_fill = list(n = 0)) names(x) &lt;- c(&quot;group&quot;, &quot;view&quot;, &quot;view_click&quot;) x%&gt;% group_by(group)%&gt;% transmute(view1 = view/(view+view_click), view_click1 = view_click/(view+view_click)) ## # A tibble: 2 × 3 ## # Groups: group [2] ## group view1 view_click1 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 control 0.807 0.193 ## 2 experiment 0.834 0.166 The null hypothesis is what we assume to be true before we collect the data, and the alternative hypothesis is usually what we want to try and prove to be true. So in our experiment than null hypothesis is assuming that the old ad is better than than new one. Then we set the significance level \\(\\alpha\\). The significance level is the probability of rejecting the null hypothesis when it’s true. (Type I error rate) For example, a significance level of 0.05 indicates a 5% risk of concluding that a difference exists when there is no actual difference. Lower significance levels indicate that you require stronger evidence before you reject the null hypothesis. So we will set our significance level to be 0.05. And if we reject the null hypothesis as a result of our experiment, then by having significant level of 0.05 then we are 95% confident that we can reject the null hypothesis. So setting the significance level is about how confident you are while you reject the null hypothesis, the fourth step is calculating the corresponding P value. The definition of P value is the probability of observing your statistic, if the null hypothesis is true. And then we will draw a conclusion whether to go for the new ad or not. Hypothesis Testing steps: 1) Specify the Null Hypothesis. 2) Specify the Alternative Hypothesis. 3) Set the Significance Level (a) 4) Calculate the Corresponding P-Value. 5) Drawing a Conclusion Our Hypothesis Hypothesis is that the click through rate associated with the new ad is less than that associated with the old ad, which means that the old ad is better than than new one. And the alternative hypothesis will be the opposite. 3.11 Task 3: Compute the difference in the click-through rate This task we will compute the difference in the click through rate between the control and experiment groups. control_df &lt;- df[df$group == &quot;control&quot;, ] experiment_df &lt;- df[df$group == &quot;experiment&quot;, ] control_ctr &lt;- mean(ifelse(control_df$action==&quot;view and click&quot;, 1, 0)) experiment_ctr &lt;- mean(ifelse(experiment_df$action==&quot;view and click&quot;, 1, 0)) diff &lt;- experiment_ctr - control_ctr diff ## [1] -0.02762758 "],["task-four-create-sample-distribution-using-bootsrapping.html", "Chapter 4 Task four : create sample distribution using bootsrapping 4.1 Task five : Evaluate the null hypothesis and draw conclustions. 4.2 alternative random sampling code", " Chapter 4 Task four : create sample distribution using bootsrapping 4.0.1 Bootstrapping : The bootstrap method is a statistical technique for estimating quantities about a population by averaging estimates from multiple small data samples. Importantly, samples are constructed by drawing observations from a large data sample one at a time and returning them to the data sample after they have been chosen. This allows a given observation to be included in a given small sample more than once. This approach to sampling is called sampling with replacement. 4.0.2 Example : Bootstrapping in statistics, means sampling with replacement. So, if we have a group of individuals and , and want to bootstrap sample of ten individuals from this group , we could randomly sample any ten individuals but with bootsrapping, we are sampling with replacement so we could actually end up sampling 7 out of the ten individuals and three of the previously selected individuals might end up being sampled again. set.seed(1234) difference &lt;- numeric() size &lt;- dim(df)[1] for (i in 1:10000){ sample_index &lt;- sample(1:nrow(df), size = size, replace = TRUE) sample_df &lt;- df[sample_index, ] controls_df &lt;- sample_df[sample_df$group==&quot;control&quot;,] experiments_df &lt;- sample_df[sample_df$group==&quot;experiment&quot;,] controls_ctr &lt;- mean(ifelse(controls_df$action==&quot;view and click&quot;, 1, 0)) experiments_ctr &lt;- mean(ifelse(experiments_df$action==&quot;view and click&quot;, 1, 0)) difference &lt;- append(difference, experiments_ctr - controls_ctr) } 4.1 Task five : Evaluate the null hypothesis and draw conclustions. The central limit theorem states that if you have a population with mean μ and standard deviation σ and take sufficiently large random samples from the population with replacement , then the distribution of the sample means will be approximately normally distributed. hist(difference) simulate the distribution under the null hypothesis (difference = 0) null_hypothesis &lt;- rnorm(n = length(difference), mean=0, sd=sd(difference)) hist(null_hypothesis) abline(v= diff, col = &quot;red&quot;) The definition of a p-value is the probability of observing your statistic (or one more extreme in favor of the alternative) if the null hypothesis is true. The confidence level is equivalent to 1 – the alpha level. So, if your significance level is 0.05, the corresponding confidence level is 95%. i.e for P Value less than 0.05 we are 95% percent confident that we can reject the null hypothesis compute p-value mean((null_hypothesis &gt; diff)) ## [1] 0.9864 It says that we dont reject the null hypothesis. We can find more extreme values than our test statistics 98% of the time if the null hypothesis true. 4.2 alternative random sampling code df %&gt;% slice_sample(n = size, replace = T) ## # A tibble: 3,757 × 2 ## group action ## &lt;chr&gt; &lt;chr&gt; ## 1 experiment view ## 2 experiment view ## 3 experiment view ## 4 control view ## 5 control view ## 6 control view ## 7 experiment view ## 8 control view ## 9 experiment view ## 10 control view ## # ℹ 3,747 more rows "],["ab-testing.html", "Chapter 5 AB Testing 5.1 Sources 5.2 Concepts 5.3 AI Summary 5.4 Size of the Control Group", " Chapter 5 AB Testing It is now widely accepted that the gold standard technique to compute the causal effect of a treatment (a drug, ad, product, …) on an outcome of interest (a disease, firm revenue, customer satisfaction, …) is AB testing, a.k.a. randomized experiments. We randomly split a set of subjects (patients, users, customers, …) into a treatment and a control group and give the treatment to the treatment group. This procedure ensures that, ex-ante, the only expected difference between the two groups is caused by the treatment. 5.1 Sources Sample Size Article Sample Size Calculator Sample Size Calculator in Excel 5.2 Concepts Contamination: One of the key assumptions in AB testing is that there is no contamination between treatment and control group. Giving a drug to one patient in the treatment group does not affect the health of patients in the control group. This might not be the case for example if we are trying to cure a contageous disease and the two groups are not isolated. In the industry, frequent violations of the contamination assumption are network effects - my utility of using a social network increases as the number of friends on the network increases - and general equilibrium effects - if I improve one product, it might decrease the sales of another similar product. Because of this reason, often experiments are carried out at a sufficiently large scale so that there is no contamination across groups, such as cities, states or even countries. Then another problem arises because of the larger scale: the treatment becomes more expensive. Giving a drug to 50% of patients in a hospital is much less expensive than giving a drug to 50% of cities in a country. Therefore, often only few units are treated but often over a longer period of time. In these settings, a very powerful method emerged around 10 years age: Synthetic Control. The idea of synthetic control is to exploit the temporal variation in the data instead of the cross-sectional one (across time instead of across units). This method is extremely popular in the industry - e.g. in companies like Google, Uber, Facebook, Microsoft, Amazon - because it is easy to interpret and deals with a setting that emerges often at large scales. 5.3 AI Summary Studying A/B testing (also known as randomized controlled trials or RCTs) is a great way to understand experimental design in causal inference, particularly in contexts where random assignment is feasible and ethical considerations allow for manipulation of variables to observe their effects. Here’s a detailed overview to get you started: 5.3.1 A/B Testing (Randomized Controlled Trials) A/B testing, or randomized controlled trials (RCTs), is an experimental design where participants or subjects are randomly assigned to different groups (treatment and control) to test the effectiveness of a particular intervention or treatment. 5.3.1.1 Key Concepts Random Assignment: Participants are assigned to either the treatment group (exposed to the intervention) or the control group (not exposed) through randomization. This helps ensure that any differences observed between the groups are due to the intervention rather than pre-existing differences. Controlled Environment: The experiment is conducted in a controlled environment where researchers can manipulate variables and minimize external influences that could affect the outcomes. Causal Inference: A/B testing allows researchers to make causal inferences about the effect of the intervention on the outcome variable. By comparing outcomes between the treatment and control groups, researchers can estimate the causal impact of the intervention. 5.3.1.2 Steps in A/B Testing Hypothesis Formulation: Define a clear hypothesis about the expected effect of the intervention on the outcome variable. Random Assignment: Randomly assign participants or subjects to the treatment and control groups. Randomization helps ensure that the groups are comparable on average, reducing the risk of bias. Implementation of Intervention: Implement the intervention or treatment with the treatment group while keeping conditions unchanged for the control group. Outcome Measurement: Measure the outcome of interest for both the treatment and control groups after the intervention. This could be metrics like conversion rates, satisfaction scores, or health outcomes. Statistical Analysis: Compare the outcomes between the treatment and control groups using statistical methods (e.g., t-tests, regression analysis) to determine if there is a significant difference attributable to the intervention. Interpretation of Results: Interpret the results to determine whether the intervention had a causal effect on the outcome variable. Consider factors such as statistical significance, effect size, and practical significance. 5.3.1.3 Advantages of A/B Testing Causal Inference: Allows for strong causal claims about the impact of interventions. Control Over Variables: Researchers have control over experimental conditions, minimizing confounding factors. Versatility: Applicable across various fields including marketing, healthcare, education, and technology. 5.3.1.4 Example Application Imagine a company wants to test the effectiveness of two different website layouts (A and B) on user engagement: Hypothesis: Layout B will lead to higher user engagement compared to Layout A. Random Assignment: Users visiting the website are randomly assigned to either see Layout A or Layout B. Outcome Measurement: Engagement metrics such as click-through rates or time spent on the website are measured for both groups. Analysis: Statistical tests are conducted to compare engagement metrics between Layout A and Layout B. Conclusion: If Layout B shows significantly higher engagement metrics, the company may decide to implement Layout B on their website. 5.3.2 Concepts: 5.3.2.1 Effect Size Definition: Effect size quantifies the magnitude of the difference or relationship between variables in a study. It provides a standardized measure of the strength of an effect or phenomenon being studied, independent of sample size. Key Points: Standardized Measure: Effect size is expressed in standard deviation units or other standardized metrics, making it comparable across different studies. Interpretation: A larger effect size indicates a stronger relationship or more substantial difference between groups or conditions. Example: In an A/B test measuring website conversion rates: If Group A (control) has a conversion rate of 5% and Group B (treatment) has a conversion rate of 7%, the effect size could be quantified using metrics like Cohen’s d or relative risk increase to indicate the practical significance of the difference. Certainly! Cohen’s d and relative risk are commonly used effect size measures in different contexts, providing standardized ways to quantify and compare the magnitude of effects between groups or conditions in research studies. Here’s an explanation of each: 5.3.2.2 Cohen’s d Definition: Cohen’s d is a standardized measure of effect size that indicates the difference between two means (e.g., treatment group mean and control group mean) in terms of standard deviation units. Formula: $ d = $ where: - $ {X}_1$ and ${X}_2 $ are the means of the two groups, - $ s $ is the pooled standard deviation of the two groups. Interpretation: - Effect Size Magnitude: Cohen’s d values are interpreted as follows: - Small effect: d = 0.2 - Medium effect: d = 0.5 - Large effect: d = 0.8 Example: In a study comparing the effectiveness of two teaching methods on exam scores: - If the mean exam score in Method A (treatment) is 80 and in Method B (control) is 75, and the pooled standard deviation is 10, then Cohen’s d would be \\(\\frac{80 - 75}{10} = 0.5\\), indicating a medium effect size. 5.3.2.3 Relative Risk Definition: Relative risk (RR) is a measure of the strength of association between a risk factor (or exposure) and an outcome in epidemiology and medical research. It compares the risk of an outcome occurring in the exposed group versus the unexposed (or control) group. Formula: \\[ RR = \\frac{\\text{Risk in exposed group}}{\\text{Risk in unexposed group}} \\] Interpretation: - RR = 1: Indicates no association between exposure and outcome. - RR &gt; 1: Indicates higher risk in the exposed group compared to the unexposed group. - RR &lt; 1: Indicates lower risk in the exposed group compared to the unexposed group. Example: In a clinical trial evaluating a new drug for heart disease: - If the risk of heart attack among patients taking the new drug is 10% and among patients not taking the drug (control group) is 20%, then the relative risk would be \\(\\frac{0.10}{0.20} = 0.5\\). - This means patients taking the drug have half the risk of experiencing a heart attack compared to those not taking the drug. 5.3.3 Comparison and Usage Cohen’s d: Typically used in studies comparing means of continuous variables (e.g., exam scores, reaction times) between two groups. Relative Risk: Primarily used in studies of binary outcomes (e.g., disease incidence, event occurrence) to compare the risk of an outcome between exposed and unexposed groups. Both measures provide valuable insights into the strength and direction of effects in research studies. The choice between Cohen’s d and relative risk depends on the nature of the data (continuous or binary) and the specific research question being addressed. Researchers often use these effect size measures alongside significance testing to provide a comprehensive assessment of the findings’ practical and statistical significance. 5.3.4 Significance Definition: Statistical significance determines whether the observed results in a study are likely to be due to the intervention (or other factors being studied) rather than random chance. It is typically assessed through hypothesis testing. Key Points: - Hypothesis Testing: Statistical tests (e.g., t-tests, ANOVA, chi-square tests) evaluate whether the observed differences between groups are statistically significant. Threshold: Results are deemed statistically significant if the probability (p-value) of observing such differences due to chance alone is below a predefined significance level (commonly set at 0.05). Does Not Equal Importance: Statistical significance does not necessarily equate to practical or clinical significance; it only indicates the reliability of the observed effect. Example: In a clinical trial evaluating a new drug: - If the treatment group shows a significantly lower incidence of adverse effects compared to the control group (p &lt; 0.05), it suggests that the drug may have a beneficial effect on reducing adverse reactions. 5.3.5 Group Size Definition: Group size refers to the number of participants or subjects included in each experimental group or condition in a study. It directly influences the statistical power and precision of the study’s results. Key Points: - Statistical Power: Larger group sizes generally increase the statistical power of a study, making it more likely to detect a true effect if one exists. - Precision: Larger group sizes reduce sampling variability and increase the precision of estimates (e.g., mean values, effect sizes). - Resource Allocation: Group size is often determined by practical considerations such as budget, time constraints, ethical considerations, and expected effect size. Example: In an A/B test comparing two marketing strategies: - If Group A consists of 1000 customers and Group B consists of 500 customers, the study’s power to detect differences between the groups will be influenced by the unequal group sizes. 5.3.6 Relationship Between Effect Size, Significance, and Group Size Effect Size and Significance: A larger effect size increases the likelihood of achieving statistical significance with smaller group sizes. Conversely, smaller effect sizes may require larger group sizes to achieve statistical significance. Group Size and Precision: Larger group sizes generally provide more precise estimates of effects and reduce the impact of random variability in the data. Balancing Factors: Researchers often balance effect size, significance level, and group size to achieve meaningful and reliable results within practical constraints. 5.3.7 Conclusion Understanding effect size, significance, and group size is crucial for interpreting and evaluating research findings accurately. Effect size measures the magnitude of effects, significance assesses the likelihood of results being due to chance, and group size influences the study’s statistical power and precision. Together, these concepts help researchers draw meaningful conclusions and inform decision-making based on empirical evidence. 5.3.7.1 Baseline conversion rate The baseline conversion rate is the current conversion rate for the page you are testing. Conversion rate is the number of conversions divided by the total number of visitors. 5.3.7.2 Minimum detectable effect (MDE) After baseline conversion rate, you need to decide how much change from the baseline (how big or small a lift) you want to detect. You wil need less traffic to detect big changes and more traffic to detect small changes. To demonstrate, let us use an example with a 20% baseline conversion rate and a 5% MDE. Based on these values, your experiment will be able to detect 80% of the time when a variation’s underlying conversion rate is actually 19% or 21% (20%, +/- 5% × 20%). If you try to detect differences smaller than 5%, your test is considered underpowered. Power is a measure of how well you can distinguish the difference you are detecting from no difference at all. So running an underpowered test is the equivalent of not being able to strongly declare whether your variations are winning or losing. 5.4 Size of the Control Group Calculating the appropriate size of the control group in an experiment involves several important factors to ensure the study has sufficient power to detect a true effect if one exists. Here are the key factors that influence control group size calculation: Effect Size (δ) Effect size is a measure of the magnitude of the difference between groups or the strength of the relationship between variables. It quantifies the practical significance of the treatment effect. Influence on Sample Size: A larger effect size requires a smaller sample size to detect a significant difference. A smaller effect size requires a larger sample size to achieve the same level of power. Significance Level (α) The significance level (α) is the threshold for determining whether the observed effect is statistically significant. It represents the probability of committing a Type I error (rejecting a true null hypothesis). Common Values: - α is typically set at 0.05, meaning there is a 5% chance of rejecting the null hypothesis when it is true. Influence on Sample Size: A lower α (e.g., 0.01) requires a larger sample size to maintain the same power, as the test becomes more stringent. A higher α (e.g., 0.10) allows for a smaller sample size but increases the risk of Type I errors. Power (1 - β) Power is the probability of correctly rejecting a false null hypothesis. It reflects the study’s ability to detect an effect if one exists. Common Values: - A common target for power is 0.80, indicating an 80% chance of detecting a true effect. Influence on Sample Size: - Higher power (e.g., 0.90) requires a larger sample size. - Lower power (e.g., 0.70) allows for a smaller sample size but increases the risk of Type II errors (failing to detect a true effect). Variability (σ) Variability refers to the spread or dispersion of data points within a population, often measured by the standard deviation (σ). Influence on Sample Size: - Higher variability (greater standard deviation) requires a larger sample size to detect a significant difference. - Lower variability allows for a smaller sample size as the effect is easier to detect against a less noisy background. Allocation Ratio Definition: The allocation ratio determines the proportion of participants assigned to the treatment group versus the control group. An equal allocation ratio (1:1) means equal numbers in both groups. Influence on Sample Size: - Unequal allocation ratios (e.g., 2:1 or 3:1) may be used based on study design or practical considerations but can affect the total sample size required to achieve the desired power. - An equal allocation ratio generally provides the most statistically efficient design, minimizing the total sample size required. Dropout Rate Definition: The dropout rate accounts for participants who may leave the study before its completion, affecting the effective sample size. Influence on Sample Size: - Anticipated dropouts should be factored into the initial sample size calculation to ensure the study retains adequate power despite participant loss. 5.4.1 Sample Size Calculation Formula For comparing two means, the sample size for each group can be calculated using: \\[ n = \\left( \\frac{(Z_{\\alpha/2} + Z_{\\beta}) \\cdot \\sigma}{\\delta} \\right)^2 \\] where: - \\(n\\) is the sample size per group, - \\(Z_{\\alpha/2}\\) is the critical value for the desired significance level, - \\(Z_{\\beta}\\) is the critical value for the desired power, - \\(\\sigma\\) is the standard deviation, - \\(\\delta\\) is the effect size. 5.4.2 Conclusion Determining the control group size involves considering the desired effect size, significance level, power, variability in the data, allocation ratio, and potential dropout rates. Properly calculating the control group size ensures that the study is adequately powered to detect meaningful effects, thereby enhancing the validity and reliability of the research findings. 5.4.3 Statistical Assumptions for Randomized Controlled Trials (RCTs) Randomized Controlled Trials (RCTs) are considered the gold standard in experimental research due to their ability to minimize bias and establish causality. However, the validity of RCT results depends on several key statistical assumptions: Randomization: Assumption: Participants are randomly assigned to treatment and control groups. Purpose: Ensures that the groups are comparable on average, reducing selection bias and balancing both known and unknown confounders. Independence: Assumption: Observations are independent of each other. Purpose: Ensures that the outcome of one participant does not influence the outcome of another, which is crucial for valid statistical inference. Consistency: Assumption: The treatment effect is consistent across all participants. Purpose: Ensures that the treatment effect observed in the sample can be generalized to the broader population. Exclusion of Confounders: Assumption: No confounding variables influence the treatment-outcome relationship. Purpose: Ensures that the observed effect is due to the treatment and not due to other external factors. Stable Unit Treatment Value Assumption (SUTVA): Assumption: The potential outcomes for any participant are not affected by the treatment assignment of other participants. Purpose: Prevents interference between participants, ensuring that each participant’s outcome is solely a result of their treatment assignment. No Systematic Differences in Measurement: Assumption: Measurement of outcomes is consistent and unbiased across treatment and control groups. Purpose: Ensures that outcome measures are not systematically biased by the treatment assignment. 5.4.4 Robustness Checks Robustness checks involve testing the stability and reliability of the study’s findings under various assumptions and conditions. They help to confirm that the results are not sensitive to specific assumptions or potential biases. Key robustness checks for RCTs include: Sensitivity Analysis: Purpose: Evaluates how the results change with different assumptions or parameters. Method: Adjusting key assumptions or parameters (e.g., different definitions of the outcome variable) to see if the results remain consistent. Subgroup Analysis: Purpose: Examines the effect of the treatment within different subgroups of the sample. Method: Dividing the sample into subgroups (e.g., by age, gender, or baseline risk) and checking if the treatment effect is consistent across these groups. Placebo Tests: Purpose: Tests whether the results hold when using a placebo treatment. Method: Using a placebo group to confirm that the observed effects are specifically due to the treatment and not to other factors. Alternative Specifications: Purpose: Tests the robustness of the results to different model specifications. Method: Using alternative statistical models or different functional forms to ensure results are not model-dependent. Attrition Analysis: Purpose: Examines the impact of participant dropout on the study results. Method: Analyzing the characteristics of dropouts and conducting analyses to understand if and how attrition might bias the results. 5.4.5 Validation Methods Validation methods are used to confirm the internal and external validity of the study’s findings. These methods help to ensure that the results are credible and can be generalized to other settings or populations. Internal Validity Checks: Balance Checks: Purpose: Ensures that randomization created comparable groups. Method: Comparing baseline characteristics between treatment and control groups to check for balance. Compliance Checks: Purpose: Ensures participants adhere to the assigned treatment. Method: Analyzing adherence rates and conducting per-protocol analyses if necessary. External Validity Checks: Population Representativeness: Purpose: Ensures that the study sample is representative of the broader population. Method: Comparing sample characteristics to the target population and discussing potential generalizability limitations. Replication Studies: Purpose: Confirms the findings by replicating the study in different settings or with different populations. Method: Conducting similar studies in various contexts to see if the results hold. Model Validation: Cross-Validation: Purpose: Assesses the predictive accuracy of the statistical model. Method: Using techniques like k-fold cross-validation to test the model’s performance on different subsets of the data. Out-of-Sample Validation: Purpose: Ensures the model performs well on new, unseen data. Method: Validating the model on a separate dataset that was not used for model training. 5.4.6 Conclusion The validity of RCT findings hinges on several key assumptions, and the credibility of these results is reinforced through robustness checks and validation methods. Robustness checks test the stability of findings under different conditions, while validation methods confirm the internal and external validity of the results, ensuring they are generalizable and reliable. Understanding and addressing these factors is crucial for conducting and interpreting high-quality research. "],["difference-in-differences-did-methods.html", "Chapter 6 Difference-in-Differences (DiD) Methods 6.1 Simple Difference-in-Differences (DiD) 6.2 Controversial Note 6.3 Placebo tests for parallel trends 6.4 Two-Way Fixed Effects Model 6.5 Event Study Methods 6.6 Importance of Placebos in DD 6.7 Compositional Changes 6.8 Key Assumptions 6.9 Notes 6.10 Extra Considerations 6.11 Synthetic Difference-in-Differences (SynthDiD) method 6.12 Doubly Robust Models in Econometrics 6.13 Twoway Fixed Effects with Differential Timing 6.14 Bacon Decomposition", " Chapter 6 Difference-in-Differences (DiD) Methods Difference-in-Differences (DiD) is a quasi-experimental technique used in econometrics to estimate causal relationships. It compares the changes in outcomes over time between a treatment group and a control group. Some resource links Comprehensive resource Extra reading 2 Mixtape youtube series Books The Effect What if? [Mathaeus - personal](Extra reading - python](https://matheusfacure.github.io/python-causality-handbook/13-Difference-in-Differences.htm) 6.1 Simple Difference-in-Differences (DiD) Basic Idea: Difference-in-Differences (DiD) is a quasi-experimental design used in econometrics to estimate causal relationships. It compares the changes in outcomes over time between a treatment group and a control group. Treatment assignment is not random, but we observe both treated and untreated units before and after treatment. -Under certain structural assumptions, especially parallel outcome trends in the absence of treatment, we can recover the average treatment effect. Formula: The basic DiD estimator is: \\[ \\text{DiD} = (\\text{Y}_{\\text{post-treatment, treatment group}} - \\text{Y}_{\\text{pre-treatment, treatment group}}) - (\\text{Y}_{\\text{post-treatment, control group}} - \\text{Y}_{\\text{pre-treatment, control group}}) \\] Concept: DiD is used when we have data from before and after a treatment is applied to a treatment group, and we also have a control group that does not receive the treatment. The key assumption is that in the absence of treatment, the difference between the treatment and control groups would have remained constant over time (parallel trends assumption). Simple 2x2 DD collapses to true ATT when parallel trend holds true. ATT can be calculated through differencing outcomes but regression can be used instead if we want to control for some more covariates. if you need to avoid omitted variable bias through controlling for endogenous covariates that vary over time, then you may want to use regression. Such strategies are another way of saying that you will need to close some known critical backdoor. Another reason for the equation is that by controlling for more appropriate covariates, you can reduce residual variance and improve the precision of your DD estimate. Model: \\[ Y_{it} = \\alpha + \\beta_1 \\text{Post}_t + \\beta_2 \\text{Treated}_i + \\beta_3 (\\text{Post}_t \\times \\text{Treated}_i) + \\epsilon_{it} \\] where: \\(Y_{it}\\) is the outcome variable for entity \\(i\\) at time \\(t\\). \\(\\text{Post}_t\\) is a dummy variable equal to 1 for periods after the treatment and 0 otherwise. \\(\\text{Treated}_i\\) is a dummy variable equal to 1 for the treatment group and 0 for the control group. \\(\\beta_3\\) is the DiD estimator, representing the treatment effect (ATT). 6.2 Controversial Note The variables of interest in many of these setups only vary at a group level, such as the state, and outcome variables are often serially correlated. In Card and Krueger (1994), it is very likely for instance that employment in each state is not only correlated within the state but also serially correlated. Bertrand, Duflo, and Mullainathan (2004) point out that the conventional standard errors often severely understate the standard deviation of the estimators, and so standard errors are biased downward, “too small,” and therefore overreject the null hypothesis. Bertrand, Duflo, and Mullainathan (2004) propose the following solutions: Block bootstrapping standard errors. Aggregating the data into one pre and one post period. This approach ignores the time-series dimensions altogether, and if there is only one pre and post period and one untreated group, it’s as simple as it sounds. Clustering standard errors at the group level. You simply adjust standard errors by clustering at the group level, as we discussed in the earlier chapter, or the level of treatment. For state-level panels, that would mean clustering at the state level, which allows for arbitrary serial correlation in errors within a state over time. This is the most common solution employed. If number of groups is small, then you may use wild bootstrap technique, or randomization inference. 6.3 Placebo tests for parallel trends We can test palcebo effects in the pre-treatment years to show in the pretreatment years both groups have similar trends. However, this may not prove that those groups will behave similarly after the treatment in the absence of treatment. Just because they were similar before does not logically require they be the same after. Likewise, we are not obligated to believe that that counterfactual trends would be the same post-treatment because they had been similar pre-treatment without further assumptions about the predictive power of pre-treatment trends. But this is a nice attempt anyway. While the test is important, technically pre-treatment similarities are neither necessary nor sufficient to guarantee parallel counterfactual trends (Kahn-Lang and Lang 2019). Any DD is a combination of a comparison between the treatment and the never treated, an early treated compared to a late treated, and a late treated compared to an early treated. Thus only showing the comparison with the never treated is actually a misleading presentation of the underlying mechanization of identification using an twoway fixed-effects model with differential timing. 6.4 Two-Way Fixed Effects Model Concept: The two-way fixed effects model extends the simple DiD approach by controlling for time-invariant characteristics of the entities and common shocks over time. It adds fixed effects for both entities and time periods to control for unobserved heterogeneity. Model: \\[ Y_{it} = \\alpha_0 + \\beta_1\\text{Treat}_i + \\beta_2\\text{Post}_t + \\beta_3 (\\text{Post}_t \\times \\text{Treat}_i) + \\epsilon_{it} \\] where: \\(\\beta_1\\) represents entity fixed effects. \\(\\beta_2\\) represents time fixed effects. \\(\\beta_3\\) remains the DiD estimator. Example: Using the job training program example, this model would account for fixed characteristics of individuals (such as inherent employability) and time-specific effects (such as economic conditions). \\[ Y_{it} = \\alpha_i + \\gamma_t + \\beta_3 (\\text{Post}_t \\times \\text{Treated}_i) + \\epsilon_{it} \\] This controls for both individual-specific and time-specific unobserved heterogeneity, providing a more robust estimate of the treatment effect. 6.5 Event Study Methods Concept: Event studies extend DiD by examining the dynamics of the treatment effect over multiple periods before and after the treatment. They allow for the estimation of treatment effects at different time points relative to the treatment event. As with many contemporary DD designs, Miller et al. (2019) evaluate the pre-treatment leads instead of plotting the raw data by treatment and control. Post-estimation, they plotted regression coefficients with 95% confidence intervals on their treatment leads and lags. Including leads and lags into the DD model allowed the reader to check both the degree to which the post-treatment treatment effects were dynamic, and whether the two groups were comparable on outcome dynamics pre-treatment. Typical Model: \\[ Y_{ist} = \\alpha_s + \\gamma_t + \\sum_{x=-q}^{-1} \\beta_x D_{sx} + \\sum_{x=0}^{m} \\delta_x D_{sx} + X_{ist} + \\epsilon_{ist} \\] You include \\(q\\) leads or anticipatory effects and \\(m\\) lags or post-treatment effects. 6.6 Importance of Placebos in DD It is a simple idea. For the minimum wage sttaudy, one candidate placebo falsification might simply be to use data for an alternative type of worker whose wages would not be affected by the binding minimum wage. This reasoning might lead us to consider the possibility that higher wage workers might function as a placebo. Many people like to be straightforward and simply fit the same DD design using high wage employment as the outcome. If the coefficient on minimum wages is zero when using high wage worker employment as the outcome, but the coefficient on minimum wages for low wage workers is negative, then we have provided stronger evidence that complements the earlier analysis we did when on the low wage workers. Another way to show placebo falsification. Triple DDD. 6.6.1 Triple Differences \\[ Y_{ijt} = \\alpha + \\beta_0X_{ist} + \\beta_1\\gamma_t + \\beta_2\\delta_j + \\beta_3 D_i + \\beta_4 (\\delta . \\gamma)_{jt} + \\beta_5 (\\gamma . D)_{ti} + \\beta_6 (\\delta . D)_{ij} + \\beta_7 (\\delta . \\gamma . D)_{ijt} + \\epsilon_{ijt} \\] where the parameter of interest is \\(\\beta_7\\). This requires a stacking of the data into a panel structure by group, as well as state. Second, the DDD model requires that you include all possible interactions across the group dummy \\(\\delta_j\\), post-treatment dummy \\(\\gamma_t\\) and treatment state dummy \\(D_i\\). The regression must include each dummy independently, each individual interaction, and the triple differences interaction. One of these will be dropped due to multicollinearity, but I include them in the equation so that you can visualize all the factors used in the product of these terms. 6.7 Compositional Changes DD can be applied to repeated cross-sections, as well as panel data. But one of the risks of working with the repeated cross-sections is that unlike panel data (e.g., individual-level panel data), repeated cross-sections run the risk of compositional changes. This kind of compositional change is a like an omitted variable bias built into the sample itself caused by time-variant unobservables. Diffusion of the Internet appears to be related to changing samples as younger music fans are early adopters. Identification of causal effects would need for the treatment itself to be exogenous to such changes in the composition. 6.8 Key Assumptions Parallel Trends Assumption: The treatment and control groups would have followed the same trend over time in the absence of the treatment. This is the most critical assumption. Common Shocks: Both groups are assumed to be subject to the same external factors over time. 6.8.1 Implementation Steps Identify Treatment and Control Groups: Clearly define which units are exposed to the treatment and which are not. Collect Data: Obtain data on the outcome of interest for both groups before and after the treatment. Check Parallel Trends: Visualize and statistically test if the pre-treatment trends of the groups are parallel. Estimate the Model: Use regression analysis to estimate the DiD effect. The basic regression model is: \\[ Y_{it} = \\alpha + \\beta_1 \\text{Post}_t + \\beta_2 \\text{Treatment}_i + \\beta_3 (\\text{Post}_t \\times \\text{Treatment}_i) + \\epsilon_{it} \\] where \\(\\beta_3\\) is the DiD estimator. 6.8.2 Advantages Controls for Time-Invariant Differences: Differences between the treatment and control groups that do not change over time are accounted for. Simple and Intuitive: The method is straightforward to understand and implement. 6.8.3 Limitations Violation of Parallel Trends: If the parallel trends assumption is violated, the DiD estimate can be biased. External Validity: The results are only valid for the sample and period studied. Simultaneous Interventions: Other changes occurring simultaneously with the treatment can confound the results. 6.8.4 Q: How would you test the parallel trends assumption? Visual Inspection Plot the outcome variable over time for both the treatment and control groups. If the trends are parallel before the intervention, it suggests that the parallel trends assumption holds. Statistical Tests Conduct a regression test to formally check for parallel trends. This involves using only the pre-treatment data and checking if the interaction between time and treatment is statistically significant. Steps: Restrict your data to pre-treatment periods. Regress the outcome on time, treatment, and their interaction. Check if the coefficient of the interaction term is statistically significant. Placebo Tests Conduct a placebo test by pretending that the treatment happened at a different time and check if you find a significant effect where none should exist. Steps: Choose a time period before the actual treatment period as the “placebo treatment period.” Perform a DiD analysis as if the treatment happened during this placebo period. Check for significant effects; finding none supports the parallel trends assumption. Event Study Analysis An event study involves plotting the estimated treatment effects at different time periods before and after the treatment to visually inspect if pre-treatment effects are close to zero. Steps: Create a series of dummy variables for each time period relative to the treatment. Regress the outcome on these time dummies and the interaction terms. Plot the coefficients of these interaction terms. 6.8.5 Q: How would you address potential violations of the parallel trends assumption? Pre-Treatment Trends Analysis Before conducting the DiD analysis, carefully examine the pre-treatment trends. If the trends are not parallel, you might need to reconsider your groups or the methodology. Visual Inspection: Plot the pre-treatment trends for the treatment and control groups. If they are not parallel, consider this a red flag. Statistical Testing: Perform a formal test by regressing the outcome on a time indicator, treatment indicator, and their interaction using only pre-treatment data. A significant interaction term suggests non-parallel trends. Control for Covariates Include control variables in your regression model to account for differences between the treatment and control groups that might affect the outcome variable. Collect relevant covariates that could influence the outcome. Include these covariates in your regression model: \\[ Y_{it} = \\alpha + \\beta_1 \\text{Post}_t + \\beta_2 \\text{Treatment}_i + \\beta_3 (\\text{Post}_t \\times \\text{Treatment}_i) + \\gamma X_{it} + \\epsilon_{it} \\] where \\(X_{it}\\) represents the covariates. Matching Use matching techniques to create a more comparable control group. Matching ensures that the treatment and control groups are similar in observed characteristics. Propensity Score Matching (PSM): Match treatment units with control units based on the propensity score, which is the probability of receiving treatment given covariates. Coarsened Exact Matching (CEM): Match units exactly on certain covariates. Synthetic Control Method Construct a synthetic control group that closely resembles the treatment group in the pre-treatment period. This method is particularly useful when you have one treatment unit and many potential control units. Select control units to construct a weighted combination (synthetic control) that mirrors the treatment unit’s pre-treatment characteristics. Compare the post-treatment outcomes of the treatment unit with the synthetic control. Difference-in-Differences-in-Differences (DiDiD) If you have an additional control group or variable, you can use DiDiD to control for potential violations. This method adds another layer of difference to control for unobserved confounders. Include a third group or dimension to add another difference. For example: \\[ Y_{it} = \\alpha + \\beta_1 \\text{Post}_t + \\beta_2 \\text{Treatment}_i + \\beta_3 (\\text{Post}_t \\times \\text{Treatment}_i) + \\beta_4 \\text{Group}_i + \\beta_5 (\\text{Group}_i \\times \\text{Post}_t) + \\beta_6 (\\text{Group}_i \\times \\text{Treatment}_i) + \\beta_7 (\\text{Group}_i \\times \\text{Post}_t \\times \\text{Treatment}_i) + \\epsilon_{it} \\] where \\(\\text{Group}_i\\) represents the additional dimension. Sensitivity Analysis Conduct sensitivity analyses to check how robust your results are to potential violations of the parallel trends assumption. Placebo Tests: Perform DiD analysis using periods before the actual treatment to ensure no significant effects are detected. Alternative Specifications: Use different model specifications or subsets of data to check the consistency of your results. Instrumental Variables (IV) If you have a valid instrument, use it to address endogeneity issues that might violate the parallel trends assumption. Identify an instrument that affects the treatment but not directly the outcome. Use Two-Stage Least Squares (2SLS) to estimate the treatment effect. By applying these strategies, you can address potential violations of the parallel trends assumption, ensuring more robust and credible results from your DiD analysis. 6.9 Notes Bertrand, Duflo, and Mullainathan (2004) point out that conventional robust standard errors usually overestimate the actual standard deviation of the estimator. The authors recommend clustering the standard errors at the level of randomization (e.g. classes, counties, villages, …). Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania (1994) by Card and Krueger. 6.9.1 Example: Business A firm wants to test the impact of a TV advertisement campaign on revenue. The firm releases the ad on a random sample of municipalities and track the revenue over time, before and after the ad campaign. 6.10 Extra Considerations Two-way Fixed Effects (TWFE) model can give wrong estimates. This is very likely especially if treatments are heterogeneous (differential treatment timings, different treatment sizes, different treatment statuses over time) that can contaminate the treatment effects. This can result from “bad” treatment combinations biased the average treatment estimation to the point of even reversing the sign. The new DiD methods “correct” for these TWFE biases by combining various estimation techniques, such as bootstrapping, inverse probability weights, matching, influence functions, and imputations, to handle parallel trends, negative weights, covariates, and controls. 6.11 Synthetic Difference-in-Differences (SynthDiD) method Reading SynthDiD is a generalized version of Synthetic Control Method (SCM) and DiD that combines the strengths of both methods. It enables causal inference with large panels, even with a short pretreatment period. On the other hand, synthetic DiD combines the synthetic control method with the difference-in-differences approach [1]. In this method, a synthetic control group is constructed using the same approach as in the synthetic control method. However, the treatment effect is estimated by comparing the change in outcomes between the treated unit and the synthetic control group before and after the treatment is introduced. This approach allows for a more robust estimation of the treatment effect by accounting for pre-existing differences between the treatment and control groups. In summary, while both methods use a synthetic control group, the synthetic control method estimates treatment effects by comparing the post-treatment outcomes of the treated unit to those of the synthetic control group, while synthetic DiD estimates treatment effects by comparing the change in outcomes between the treated unit and the synthetic control group before and after the treatment is introduced. It constructs a counterfactual for the treated group by optimally weighting the control group units to minimize the difference between the treated and control groups in the pretreatment period as in SCM. Then, the treatment effect is estimated by comparing the outcome changes in the treated unit and synthetic control group pre- and post-intervention as in DiD. 6.11.0.1 An Example: Suppose that we are a company that sells plant-based food products, such as soy milk or soy yogurt, and we operate in multiple countries. Some countries implement new legislation that prohibits us from marketing our plant-based products as ‘milk’ or ‘yogurt’ because it is claimed that only animal products can be marketed as ‘milk’ or ‘yogurt’. Thus, due to this new regulation in some countries, we have to market soy milk as soy drink instead of soy milk, etc. We want to know the impact of this legislation on our revenue as this might help guide our lobbying efforts and marketing activities in different countries. I simulated a balanced panel dataset that shows the revenue of our company in 30 different countries for 30 periods. Three of the countries implement this legislation in period 20. In the figure below, you can see a snapshot of the data. treat is a dummy variable indicating whether a country has implemented the legislation in a given period. revenueis the revenue in millions of EUR. You can find the simulation and estimation code in this Gist. # Install and load the required packages # devtools::install_github(&quot;synth-inference/synthdid&quot;) library(synthdid) library(ggplot2) library(fixest) # Fixed-effects regression library(data.table) # Set seed for reproducibility set.seed(12345) source(&#39;sim_data.R&#39;) # Import simulation function and some utilities dt &lt;- sim_data() head(dt) In Data, there are 30 units (3 units treated), 30 periods (10 periods treated), all units are treated at the same time. Next, we convert our panel data into a matrix required by the synthdid package. Given the outcome, treatment and control units and pretreatment periods, a synthetic control is created and treatment effect is estimated with synthdid_estimate function. # Convert the data into a matrix setup = panel.matrices(dt, unit = &#39;country&#39;, time = &#39;period&#39;, outcome = &#39;revenue&#39;, treatment = &#39;treat&#39;) # Estimate treatment effect using SynthDiD tau.hat = synthdid_estimate(setup$Y, setup$N0, setup$T0) print(summary(tau.hat)) To make inference, we also need to calculate the standard errors. I use jacknife method as I have more than one treated units. Placebo method is the only option if you have one treatment unit. Given the standard errors, I also calculate the 95% confidence interval for the treatment effect. I will report these in the figure below. When there are multiple treated units (more than one unit that received the treatment or intervention), one common approach to estimating standard errors is using the jackknife method. The jackknife method is a resampling technique where each observation (in this case, each treated unit) is systematically omitted from the dataset, and the analysis is repeated each time to estimate the variance of the treatment effect. This provides a robust estimate of the standard errors that accounts for the potential variability across different treated units. On the other hand, if there is only one treated unit (a single unit that received the treatment), using the jackknife method becomes impractical because there are not enough units to systematically leave out and still perform meaningful resampling. In such cases, the placebo method becomes a viable option. The placebo method involves creating placebo or synthetic treated units that mimic the characteristics of the treated unit but did not actually receive the treatment. By comparing the outcomes of the actual treated unit with those of the synthetic placebo units, researchers can estimate the variability and potential impact of the treatment effect more accurately. Therefore, the choice between the jackknife method and the placebo method depends on the number of treated units available for analysis within the synthetic control framework. Multiple treated units allow for the application of the jackknife method, whereas a single treated unit necessitates the use of the placebo method to estimate standard errors and make reliable inferences about the treatment effect. # Calculate standard errors se = sqrt(vcov(tau.hat, method=&#39;jackknife&#39;)) te_est &lt;- sprintf(&#39;Point estimate for the treatment effect: %1.2f&#39;, tau.hat) CI &lt;- sprintf(&#39;95%% CI (%1.2f, %1.2f)&#39;, tau.hat - 1.96 * se, tau.hat + 1.96 * se) # Plot treatment effect estimates plot(tau.hat) plot(tau.hat, se.method=&#39;jackknife&#39;) In the image below, the estimation results are displayed. Observe how the treated countries and the synthetic control exhibit fairly parallel trends on average (it might not look like a perfect parallel trends but that is not necessary for the sake of this example). The average for treated countries is more variable, primarily due to the presence of only three such countries, resulting in less smooth trends. Transparent gray lines represent different control countries. Following the treatment in period 20, a decline in revenue is observed in the treated countries, estimated to be 0.51 million EUR as indicated in the graph. This means that the new regulation has a negative impact on our company’s revenues and necessary actions should be taken to prevent further declines. # Check the number of treatment and control countries to report num_treated &lt;- length(unique(dt[treat==1]$country)) num_control &lt;- length(unique(dt$country))-num_treated # Create spaghetti plot with top 10 control units top.controls = synthdid_controls(tau.hat)[1:10, , drop=FALSE] plot(tau.hat, spaghetti.units=rownames(top.controls), trajectory.linetype = 1, line.width=.75, trajectory.alpha=.9, effect.alpha=.9, diagram.alpha=1, onset.alpha=.9, ci.alpha = .3, spaghetti.line.alpha =.2, spaghetti.label.alpha = .1, overlay = 1) + labs(x = &#39;Period&#39;, y = &#39;Revenue&#39;, title = &#39;Estimation Results&#39;, subtitle = paste0(te_est, &#39;, &#39;, CI, &#39;.&#39;), caption = paste0(&#39;The number of treatment and control units: &#39;, num_treated, &#39; and &#39;, num_control, &#39;.&#39;)) Let’s plot the weights use to estimate the synthetic control. # Plot control unit contributions synthdid_units_plot(tau.hat, se.method=&#39;jackknife&#39;) + labs(x = &#39;Country&#39;, y = &#39;Treatment effect&#39;, caption = &#39;The black horizontal line shows the actual effect; the gray ones show the endpoints of a 95% confidence interval.&#39;) ggsave(&#39;../figures/unit_weights.png&#39;) In the image below, you can observe how each country is weighted to construct the synthetic control. The treatment effects differ based on the untreated country selected as the control unit. # Check for pre-treatment parallel trends plot(tau.hat, overlay=1, se.method=&#39;jackknife&#39;) ggsave(&#39;../figures/results_simple.png&#39;) # Check the number of treatment and control countries to report num_treated &lt;- length(unique(dt[treat==1]$country)) num_control &lt;- length(unique(dt$country))-num_treated # Create spaghetti plot with top 10 control units top.controls = synthdid_controls(tau.hat)[1:10, , drop=FALSE] plot(tau.hat, spaghetti.units=rownames(top.controls), trajectory.linetype = 1, line.width=.75, trajectory.alpha=.9, effect.alpha=.9, diagram.alpha=1, onset.alpha=.9, ci.alpha = .3, spaghetti.line.alpha =.2, spaghetti.label.alpha = .1, overlay = 1) + labs(x = &#39;Period&#39;, y = &#39;Revenue&#39;, title = &#39;Estimation Results&#39;, subtitle = paste0(te_est, &#39;, &#39;, CI, &#39;.&#39;), caption = paste0(&#39;The number of treatment and control units: &#39;, num_treated, &#39; and &#39;, num_control, &#39;.&#39;)) ggsave(&#39;../figures/results.png&#39;) fe &lt;- feols(revenue~treat, dt, cluster = &#39;country&#39;, panel.id = &#39;country&#39;, fixef = c(&#39;country&#39;, &#39;period&#39;)) summary(fe) Now that we understand more about SynthDiD let’s talk about pros and cons of this method. There are some advantages and disadvantages to SynthDiD like every method. Here are some pros and cons to keep in mind when getting started with this method. Advantages of SynthDiD method: The synthetic control method is usually used for a few treated and control units and needs long, balanced data before treatment. SynthDiD, on the other hand, works well even with a short data period before treatment, unlike the synthetic control method [4]. This method is being preferred especially because it doesn’t have a strict parallel trends assumption (PTA) requirement like DiD. SynthDiD guarantees a suitable quantity of control units, considers possible pre-intervention patterns, and may accommodate a degree of endogenous treatment timing [4]. Disadvantages of SynthDiD method: Can be computationally expensive (even with only one treated group/block). Requires a balanced panel (i.e., you can only use units observed for all time periods) and that the treatment timing is identical for all treated units. Requires enough pre-treatment periods for good estimation, so, if you don’t have enough pre-treatment period might be better to use just the regular DiD. Computing and comparing the average treatment effects for subgroups is tricky. One option is to split the sample into subgroups and compute the average treatment effects for each subgroup. Implementing SynthDiD where the treatment timing varies might be tricky. In the case of staggered treatment timing, as one solution, one can estimate the average treatment effect for each treatment cohort and then aggregate cohort-specific average treatment effects to an overall average treatment effects. Here are also some other points that you might want to know when getting started. Things to note: SynthDiD employs regularized ridge regression (L2) while ensuring that the resulting weights have a sum of one. In the process of pretreatment matching, SynthDiD tries to determine the average treatment effect across the entire sample. This approach might cause individual time period estimates to be less precise. Nonetheless, the overall average yields an unbiased evaluation. The standard errors for the treatment effects are estimated with jacknife or if a cohort has only one treated unit with placebo method. The estimator is considered consistent and asymptotically normal, given that the combination of the number of control units and pretreatment periods is sufficiently large relative to the combination of the number of treated units and posttreatment periods. In practice, pre-treatment variables play a minor role in Synthetic DiD, as lagged outcomes hold more predictive power, making the treatment of these variables less critical. Conclusion In this blog post, I introduce the SynthDiD method and discuss its relationship with traditional DiD and SCM. SynthDiD combines the strengths of both SCM and DiD, allowing for causal inference with large panels even when the pretreatment period is short. I demonstrate the method using the synthdid package in R. Although it has several advantages, such as not requiring a strict parallel trends assumption, it also has drawbacks, like being computationally expensive and requiring a balanced panel. Overall, SynthDiD is a valuable tool for researchers interested in estimating causal effects using observational data, providing an alternative to traditional DiD and SCM methods. 6.12 Doubly Robust Models in Econometrics Doubly Robust (DR) Models are a class of estimators used to estimate causal effects, providing robustness against model misspecification. The key feature of DR models is that they combine elements of both outcome regression and propensity score methods. This dual approach ensures that the estimator remains consistent if at least one of the two models (outcome or treatment model) is correctly specified. DRDID website DRDID Average Treatment Effect on the Treated (ATT) in Difference-in-Differences (DiD) setups where the parallel trends assumption holds after conditioning on a vector of pre-treatment covariates. 6.12.1 Key Concepts Outcome Model: This involves modeling the outcome \\(Y\\) as a function of covariates \\(X\\) and treatment \\(D\\). Example: Using a regression model \\(E[Y | X, D]\\). Treatment Model (Propensity Score Model): This involves modeling the treatment assignment \\(D\\) as a function of covariates \\(X\\). Example: Using logistic regression to estimate the propensity score \\(P(D = 1 | X)\\). Doubly Robust Estimator: Combines the predictions from both the outcome and treatment models to estimate the average treatment effect (ATE). The estimator is “doubly robust” because it remains unbiased if either the outcome model or the treatment model is correctly specified, but not necessarily both. 6.12.2 Steps in Doubly Robust Estimation Estimate the Propensity Score: Use a logistic regression (or other suitable model) to estimate the probability of treatment given the covariates \\(X\\): \\[ \\hat{p}(X) = P(D = 1 | X) \\] Estimate the Outcome Model: Fit a regression model to estimate the expected outcome given covariates \\(X\\) and treatment \\(D\\): \\[ \\hat{E}[Y | X, D] \\] Compute the Inverse Probability Weights (IPW): Calculate the weights based on the estimated propensity scores: \\[ W = \\frac{D}{\\hat{p}(X)} + \\frac{1 - D}{1 - \\hat{p}(X)} \\] Calculate the Doubly Robust Estimator: Combine the outcome model and the inverse probability weights to adjust the outcomes: \\[ \\hat{\\theta}_{DR} = \\frac{1}{n} \\sum_{i=1}^n \\left( \\hat{E}[Y | X_i, D_i] + \\frac{D_i (Y_i - \\hat{E}[Y | X_i, D_i])}{\\hat{p}(X_i)} - \\frac{(1 - D_i) (Y_i - \\hat{E}[Y | X_i, D_i])}{1 - \\hat{p}(X_i)} \\right) \\] 6.12.3 Advantages Robustness: The estimator is consistent if either the outcome model or the propensity score model is correctly specified. Efficiency: It often has lower variance compared to using either the outcome model or propensity score model alone. Flexibility: Can be applied in various settings, including observational studies and randomized experiments with imperfect compliance. 6.12.4 Examples and Applications Healthcare: Estimating the effect of a new treatment on patient outcomes, where treatment assignment may depend on patient characteristics. Economics: Evaluating the impact of job training programs on employment, accounting for non-random selection into the program. Education: Assessing the effect of educational interventions, such as after-school tutoring programs, on student performance, considering potential confounding factors. 6.12.5 Assumptions and Considerations Consistency: Assumes that the treatment and outcome models are correctly specified for the estimator to be unbiased. Overlap: Requires that for every value of covariates \\(X\\), there is a positive probability of receiving both treatment and control (common support assumption). No Unmeasured Confounding: Assumes that all confounders affecting both treatment and outcome are observed and correctly included in the models. 6.12.6 Conclusion Doubly Robust models provide a powerful and flexible approach for causal inference in econometrics, offering robustness against model misspecification and improving efficiency. They are particularly useful in observational studies where the treatment assignment is not random, ensuring more reliable and credible estimates of causal effects. 6.13 Twoway Fixed Effects with Differential Timing \\(y_{it} = \\alpha_0 + \\delta D_{it} + X_{it} + \\alpha_i + \\alpha_t + \\epsilon_{it}\\) When researchers estimate this regression these days, they usually use the linear fixed-effects model. These linear panel models have gotten the nickname “twoway fixed effects” because they include both time fixed effects and unit fixed effects. 6.14 Bacon Decomposition The punchline of the Bacon decomposition theorem is that the twoway fixed effects estimator is a weighted average of all potential 2 x 2 DD estimates where weights are both based on group sizes and variance in treatment. 6.14.1 Overview Bacon Decomposition is a method introduced by Goodman-Bacon (2018) for decomposing the overall treatment effect estimated by a Two-Way Fixed Effects (TWFE) regression model in the context of Difference-in-Differences (DiD) settings with variation in treatment timing. The key insight from this decomposition is that the TWFE estimate in such settings can be understood as a weighted average of all possible 2x2 DiD estimates that can be constructed from the data. This decomposition helps identify the sources of bias, especially when treatment effects are heterogeneous or when there are differential pre-treatment trends. 6.14.2 Key Concepts Two-Way Fixed Effects (TWFE) Models: TWFE models are commonly used in DiD analyses to account for time-invariant differences between units and common shocks over time by including unit and time fixed effects. The model typically looks like: \\[ Y_{it} = \\alpha_i + \\lambda_t + \\beta D_{it} + \\epsilon_{it} \\] where \\(Y_{it}\\) is the outcome for unit \\(i\\) at time \\(t\\), \\(\\alpha_i\\) are unit fixed effects, \\(\\lambda_t\\) are time fixed effects, \\(D_{it}\\) is the treatment indicator, and \\(\\beta\\) is the treatment effect. Variation in Treatment Timing: In many DiD applications, units receive treatment at different times rather than simultaneously. This leads to multiple possible comparisons between treated and control units at different points in time. Bacon Decomposition: The decomposition breaks down the overall TWFE estimate into a weighted average of all possible 2x2 DiD estimates. Each of these estimates compares treated and untreated units in specific periods. The decomposition reveals that the overall estimate is influenced by: Comparisons between early-treated and late-treated units. Comparisons between treated and untreated units at different times. Comparisons within treated units (pre- and post-treatment). 6.14.3 Components of Bacon Decomposition Early vs. Late Treated Units: Comparing units treated early with those treated later. This can introduce bias if there are differential trends among these groups. Treated vs. Untreated Units: Standard DiD comparison where treated units are compared to untreated ones, assuming common trends between them. Within-Unit Comparisons: Comparing outcomes within the same unit before and after treatment. 6.14.4 Formula for Decomposition The overall TWFE estimate \\(\\hat{\\beta}_{TWFE}\\) can be decomposed as: \\[ \\hat{\\beta}_{TWFE} = \\sum_{k} w_k \\hat{\\beta}_k \\] where \\(\\hat{\\beta}_k\\) are the 2x2 DiD estimates, and \\(w_k\\) are the weights that depend on the relative timing of treatment and the distribution of the treated and control units over time. 6.14.5 Implications and Interpretation Heterogeneous Treatment Effects: When treatment effects vary over time or across units, the TWFE estimate can be biased. Bacon decomposition helps identify how much of the TWFE estimate is driven by comparisons that might be invalid due to treatment effect heterogeneity. Differential Pre-treatment Trends: If treated and control units follow different pre-treatment trends, this can also bias the TWFE estimate. Bacon decomposition highlights which comparisons are most affected by such trends. Policy Implications: Understanding the sources of bias through Bacon decomposition can inform better policy evaluations by revealing the need for more appropriate methods or robustness checks in the presence of staggered treatment adoption. 6.14.6 Example Consider a study evaluating the impact of a new education policy implemented in different schools at different times. Using a TWFE model, the overall treatment effect might be estimated as: \\[ \\hat{\\beta}_{TWFE} = 0.5 \\] Applying Bacon decomposition, we might find that: - Comparisons between schools treated in 2018 and those treated in 2020 contribute \\(0.3\\) to the estimate. - Comparisons between treated schools and untreated schools contribute \\(0.1\\). - Comparisons within schools before and after treatment contribute \\(0.1\\). If early-treated schools experienced a different trend in outcomes compared to late-treated schools, this could explain the significant contribution from early vs. late comparisons, highlighting potential bias in the overall estimate. 6.14.7 Conclusion Bacon decomposition provides a nuanced understanding of the TWFE estimates in DiD settings with staggered treatment adoption. By breaking down the overall estimate into its constituent comparisons, researchers can identify and address potential biases due to heterogeneous treatment effects and differential trends, leading to more accurate and reliable causal inferences. 6.14.7.1 Self Driving Cars Experiment (Source)[https://matteocourthoud.github.io/post/synth/] Suppose you were a ride-sharing platform and you wanted to test the effect of self-driving cars in your fleet. As you can imagine, there are many limitations to running an AB/test for this type of feature. First of all, it’s complicated to randomize individual rides. Second, it’s a very expensive intervention. Third, and statistically most important, you cannot run this intervention at the ride level. The problem is that there are spillover effects from treated to control units: if indeed self-driving cars are more efficient, it means that they can serve more customers in the same amount of time, reducing the customers available to normal drivers (the control group). This spillover contaminates the experiment and prevents a causal interpretation of the results. For all these reasons, we select only one city. Given the synthetic vibe of the article we cannot but select… (drum roll)… Miami! We have information on the largest 46 U.S. cities for the period 2002-2019. The panel is balanced, which means that we observe all cities for all time periods. Self-driving cars were introduced in 2013. As expected, the groups are not balanced: Miami is more densely populated, poorer, larger and has lower employment rate than the other cities in the US in our sample. We are interested in understanding the impact of the introduction of self-driving cars on revenue. One initial idea could be to analyze the data as we would in an A/B test, comparing control and treatment group. We can estimate the treatment effect as a difference in means in revenue between the treatment and control group, after the introduction of self-driving cars. "],["synthetic-control-method-scm.html", "Chapter 7 Synthetic Control Method (SCM) 7.1 AI Summary", " Chapter 7 Synthetic Control Method (SCM) The synthetic control method is a statistical technique that creates a “synthetic” control group by combining multiple control units that are similar to the treatment unit in all relevant characteristics. The synthetic control group is constructed to match the pre-treatment outcomes of the treated unit as closely as possible. The treatment effect is then estimated by comparing the post-treatment outcomes of the treated unit to those of the synthetic control group. Synthetic control allow us to do causal inference when we have as few as one treated unit and many control units and we observe them over time. The idea is simple: combine untreated units so that they mimic the behavior of the treated unit as closely as possible, without the treatment. Then use this “synthetic unit” as a control. The method first introduced by Abadie, Diamond and Hainmueller (2010) and has been called “the most important innovation in the policy evaluation literature in the last few years”. Moreover, it is widely used in the industry because of its simplicity and interpretability. 7.1 AI Summary 7.1.1 Synthetic Control Method (SCM) What is the Synthetic Control Method? SCM is a statistical method used to evaluate the effect of an intervention or treatment when a randomized control trial is not feasible. It constructs a synthetic version of the treated unit using a weighted combination of control units. 7.1.1.1 Real-World Examples Can you provide a real-world example where SCM has been used? SCM was famously used to evaluate the economic impact of California’s tobacco control program in 1988. By constructing a synthetic California from other states with similar pre-intervention characteristics, researchers estimated the program’s impact on cigarette sales. 7.1.1.2 Assumptions What are the key assumptions of SCM? No hidden confounders: Assumes that all relevant variables are included in the model. No anticipation effect: Assumes that the intervention did not affect the units before its implementation. Convex hull: Assumes that the treated unit lies within the convex hull of the donor pool. 7.1.1.3 Validation Methods How can you validate the results of an SCM? Placebo Tests: Apply the method to units that were not exposed to the intervention to check if significant effects are falsely detected. Leave-One-Out Cross-Validation: Assess the impact by leaving one control unit out of the synthetic control and observing if the results significantly change. Pre-Intervention Fit: Ensure the synthetic control closely tracks the treated unit before the intervention. 7.1.1.4 Robustness Checks What robustness checks can you perform for SCM? Sensitivity Analysis: Test the robustness of results to changes in the weights of control units. In-Sample Prediction: Check if the synthetic control accurately predicts the outcome for the treated unit in the pre-intervention period. Alternative Specifications: Use different sets of predictor variables to construct the synthetic control and see if the results hold. 7.1.1.5 Violating Situations What situations could violate the assumptions of SCM? Unobserved Confounders: If there are important variables that influence both the intervention and the outcome but are not included in the model. Anticipation Effects: If the units change their behavior in anticipation of the intervention. Poor Fit Pre-Intervention: If the synthetic control does not closely match the treated unit’s pre-intervention trends. 7.1.1.6 Detailed Explanation of a Key Concept Constructing a Synthetic Control: Choosing Donor Pool: Select units that were not exposed to the intervention but are similar to the treated unit. Variable Selection: Identify predictor variables that are strongly related to the outcome variable. Weighting Process: Assign weights to control units such that the weighted combination closely resembles the treated unit in terms of pre-intervention characteristics. 7.1.1.7 Example Implementation Case Study: Impact of a New Policy on Unemployment Rates Objective: Assess the impact of a new job training program on unemployment rates in a particular state. Steps: Select Donor Pool: Choose similar states without the job training program. Identify Predictors: Variables such as GDP growth, historical unemployment rates, industrial composition. Construct Synthetic Control: Calculate weights for each control state to create a synthetic state that mirrors the treated state’s pre-intervention unemployment rate. Analyze Results: Compare post-intervention unemployment rates between the treated state and its synthetic counterpart. 7.1.2 Placebo tests Placebo tests are a crucial part of validating the results of a Synthetic Control Method (SCM) analysis. They help ensure that the observed effect of an intervention is not due to random chance but rather to the intervention itself. Here’s a detailed explanation of how placebo tests are performed and interpreted: 7.1.2.1 Concept A placebo test involves applying the synthetic control methodology to units that were not subjected to the intervention. The idea is to check whether similar treatment effects are observed in these non-treated units, which should not exhibit a significant effect if the synthetic control method is working correctly. 7.1.2.2 Steps for Performing Placebo Tests Identify Non-Treated Units: Select units from the donor pool that were not exposed to the intervention. Create Placebo Interventions: Assign a placebo intervention to each non-treated unit. This means arbitrarily selecting a point in time as the “intervention” date for these units. Construct Synthetic Controls for Placebo Units: For each non-treated unit with a placebo intervention, construct a synthetic control using the same pre-intervention period and predictor variables as for the actual treated unit. Compare Outcomes: Calculate the difference between the outcomes of the non-treated units and their synthetic controls in the post-intervention period. 7.1.2.3 Interpreting Placebo Tests Distribution of Placebo Effects: Examine the distribution of the placebo effects across all non-treated units. This helps determine if the observed effect for the treated unit is unusually large compared to the placebo effects. Significance Testing: If the effect observed in the actual treated unit is significantly larger than the effects observed in the placebo tests, this suggests that the effect is likely due to the intervention rather than random variation. Graphical Analysis: Plot the post-intervention gaps (difference between actual and synthetic outcomes) for both the treated unit and the placebo units. If the treated unit’s gap stands out from the placebo gaps, it strengthens the evidence for a causal effect of the intervention. 7.1.2.4 Example Imagine you are evaluating the impact of a new job training program introduced in State A in 2010. Select Donor Pool: Choose similar states (B, C, D, etc.) that did not introduce the job training program. Placebo Interventions: Assign placebo intervention years for states B, C, and D (e.g., 2010 for all states). Construct Synthetic Controls: For each state, construct a synthetic control using the same method applied to State A. Analyze Results: Compare the post-2010 unemployment rates of each state with their respective synthetic controls. If State A shows a significant drop in unemployment rates compared to its synthetic control, and the placebo tests for states B, C, and D show no significant changes, this strengthens the case that the job training program had a real impact on State A. Graphical Evidence: Plot the unemployment rate differences (treated - synthetic) for State A and placebo states. The graph should show a distinct deviation for State A after 2010, while the placebo states’ lines should remain relatively flat. 7.1.2.5 Robustness By showing that non-treated units (placebo units) do not exhibit significant changes in the outcome, while the treated unit does, you provide strong evidence that the intervention (job training program) is responsible for the observed effect. 7.1.2.6 Conclusion Placebo tests are an essential validation tool in SCM. They help rule out the possibility that the observed effect is due to random variation or other factors unrelated to the intervention. By carefully constructing and analyzing placebo tests, you can enhance the credibility of your SCM analysis and provide robust evidence for causal inference. 7.1.3 Leave-One-Out Cross-Validation Leave-One-Out Cross-Validation (LOO-CV) is a robustness check used in the context of the Synthetic Control Method (SCM) to assess the sensitivity and stability of the results. This method systematically removes one unit from the donor pool at a time to examine how the exclusion affects the construction of the synthetic control and the estimated treatment effect. 7.1.3.1 Steps for Leave-One-Out Cross-Validation in SCM Initial Synthetic Control Construction: First, construct the synthetic control for the treated unit using the full donor pool of control units. Iterative Exclusion: Iteratively exclude one control unit at a time from the donor pool and reconstruct the synthetic control without the excluded unit. For a donor pool with \\(n\\) control units, this results in \\(n\\) different synthetic controls, each missing one different control unit. Calculate Treatment Effect: For each iteration, calculate the treatment effect as the difference between the outcome of the treated unit and the outcome of the synthetic control constructed without the excluded unit. Compare and Analyze: Compare the treatment effects from all iterations to assess the stability and robustness of the original treatment effect. 7.1.3.2 Interpreting Leave-One-Out Cross-Validation Consistency of Results: If the estimated treatment effects from all iterations are similar to the original estimate (constructed with the full donor pool), it indicates that the result is robust and not overly dependent on any single control unit. Sensitivity to Exclusion: If excluding certain control units significantly changes the estimated treatment effect, this suggests that the result may be sensitive to the composition of the donor pool. Identifying such units can provide insights into which control units are driving the results. Graphical Analysis: Plot the treatment effects from each iteration to visually inspect the variability. Ideally, the effects should cluster closely around the original estimate. 7.1.3.3 Example Consider an evaluation of the impact of a smoking ban on public health outcomes in City A. The donor pool includes Cities B, C, D, E, and F. Construct Full Synthetic Control: Using Cities B, C, D, E, and F, create a synthetic control for City A and calculate the treatment effect on public health outcomes. Iteratively Exclude Cities: Exclude City B, reconstruct the synthetic control using Cities C, D, E, and F, and calculate the new treatment effect. Repeat this process for Cities C, D, E, and F. Compare Results: Compare the treatment effect estimates obtained from each exclusion to the original estimate. Analysis: If the treatment effect estimates are similar regardless of which city is excluded, the result is robust. If excluding a particular city (e.g., City D) results in a significantly different treatment effect, this indicates that City D has a substantial influence on the synthetic control. 7.1.4 Detailed Example 7.1.4.1 Original Estimate: Full Donor Pool (Cities B, C, D, E, F): Treatment effect is a 5% reduction in public health issues. 7.1.4.2 Leave-One-Out Estimates: Excluding City B (Cities C, D, E, F): Treatment effect is a 4.8% reduction. Excluding City C (Cities B, D, E, F): Treatment effect is a 5.2% reduction. Excluding City D (Cities B, C, E, F): Treatment effect is a 3.0% reduction. Excluding City E (Cities B, C, D, F): Treatment effect is a 5.1% reduction. Excluding City F (Cities B, C, D, E): Treatment effect is a 4.9% reduction. 7.1.4.3 Analysis: The estimates are generally close to the original 5% reduction, except when City D is excluded, which yields a 3% reduction. This suggests that City D has a significant influence on the synthetic control and should be further examined to understand why its exclusion changes the result. 7.1.5 Conclusion Leave-One-Out Cross-Validation is a powerful tool to test the robustness and reliability of the synthetic control method’s results. By systematically excluding each control unit, it helps identify how dependent the estimated treatment effect is on individual control units. This ensures that the results are not driven by any single unit and provides a deeper understanding of the underlying data and the robustness of the findings. 7.1.6 Data The Synthetic Control Method (SCM) is a sophisticated tool used in causal inference to estimate the effect of an intervention or treatment when a randomized control trial is not feasible. For SCM to be effectively implemented, certain data settings and requirements need to be met. Here’s a detailed explanation of these requirements: 7.1.7 Data Setting Treated Unit: The unit (e.g., a region, city, country, organization) that is exposed to the intervention or treatment. Control Units (Donor Pool): A set of similar units that were not exposed to the intervention. These units will be used to construct the synthetic control. Outcome Variable: The variable of interest that the intervention is expected to affect. This could be economic indicators, health outcomes, crime rates, etc. Predictor Variables: A set of variables that are believed to influence the outcome variable. These predictors are used to create the synthetic control and should be available for both the treated and control units. Time Period: Data should cover a sufficiently long period before and after the intervention. The pre-intervention period is used to match the treated unit with the synthetic control, and the post-intervention period is used to evaluate the treatment effect. 7.1.8 Requirements for Synthetic Control Method Similarity in Pre-Intervention Period: The treated unit and the control units should have similar trends in the outcome variable and predictor variables during the pre-intervention period. This similarity ensures that the synthetic control can accurately replicate the treated unit’s trajectory had the intervention not occurred. Sufficient Number of Control Units: A reasonably large donor pool is necessary to construct a reliable synthetic control. The control units should be comparable to the treated unit in terms of the pre-intervention characteristics. Availability of Data: Detailed and reliable data for both the treated unit and the control units over the entire time period (both pre- and post-intervention) is crucial. Missing data can bias the results. No Anticipation Effect: It is assumed that the intervention does not affect the outcome variable before its actual implementation. This ensures that any observed changes in the outcome variable after the intervention can be attributed to the intervention itself. Convex Hull Condition: The treated unit should lie within the convex hull of the control units in the space of the predictor variables. This condition ensures that a weighted combination of control units can adequately represent the treated unit. Stationarity of Relationship: The relationship between predictor variables and the outcome variable should remain stable over time. If this relationship changes significantly, the synthetic control constructed from pre-intervention data might not be valid for post-intervention analysis. 7.1.9 Data Requirements Summary Outcome Data: Continuous data on the outcome variable for both the treated and control units over the entire study period. Predictor Data: Data on several predictor variables that influence the outcome variable. These should be available for both the treated and control units. Pre-Intervention and Post-Intervention Data: Sufficient data points before the intervention to accurately match the treated unit with the synthetic control, and enough data points after the intervention to assess its impact. 7.1.10 Practical Considerations Data Quality: High-quality, consistent data is crucial. Any inaccuracies or inconsistencies in the data can lead to unreliable results. Selection of Control Units: The control units should be selected based on their similarity to the treated unit and their relevance to the study. Including irrelevant or highly dissimilar units can distort the synthetic control. Choice of Predictors: The predictor variables should be carefully chosen based on theoretical and empirical understanding of what drives the outcome variable. Including irrelevant predictors can reduce the accuracy of the synthetic control. 7.1.11 Example Consider evaluating the impact of a new traffic policy in City A on reducing traffic accidents. Treated Unit: City A where the traffic policy was implemented. Control Units: A set of similar cities (Cities B, C, D, etc.) where the policy was not implemented. Outcome Variable: Number of traffic accidents. Predictor Variables: Variables such as population density, average income, road infrastructure quality, historical traffic accident rates, and other socio-economic factors. Time Period: Data covering several years before and after the implementation of the traffic policy. 7.1.12 Conclusion The success of the Synthetic Control Method relies heavily on the quality and appropriateness of the data used. Ensuring that the treated and control units are similar, having a sufficient number of control units, and having detailed and accurate data for the relevant time periods are crucial for producing reliable and valid estimates of the treatment effect. By carefully selecting the outcome and predictor variables and ensuring robust data quality, SCM can provide valuable insights into the causal effects of interventions. 7.1.12.1 Setting We assume that for a panel of i.i.d. subjects over time. we observed a set of variables that includes: a treatment assignment (treated) a response (revenue) a feature vector (population, density, employment and GDP) Moreover, one unit (Miami in our case) is treated at time (2013 in our case). We distinguish time periods before treatment and time periods after treatment. Crucially, treatment is not randomly assigned, therefore a difference in means between the treated unit(s) and the control group is not an unbiased estimator of the average treatment effect. Some Resources: (Synthetic Difference-in-Differences)[https://matheusfacure.github.io/python-causality-handbook/25-Synthetic-Diff-in-Diff.html] 7.1.13 Synthetic DID Advantages of SynthDiD method: The synthetic control method is usually used for a few treated and control units and needs long, balanced data before treatment. SynthDiD, on the other hand, works well even with a short data period before treatment, unlike the synthetic control method [4]. This method is being preferred especially because it doesn’t have a strict parallel trends assumption (PTA) requirement like DiD. SynthDiD guarantees a suitable quantity of control units, considers possible pre-intervention patterns, and may accommodate a degree of endogenous treatment timing [4]. Disadvantages of SynthDiD method: Can be computationally expensive (even with only one treated group/block). Requires a balanced panel (i.e., you can only use units observed for all time periods) and that the treatment timing is identical for all treated units. Requires enough pre-treatment periods for good estimation, so, if you don’t have enough pre-treatment period might be better to use just the regular DiD. Computing and comparing the average treatment effects for subgroups is tricky. One option is to split the sample into subgroups and compute the average treatment effects for each subgroup. Implementing SynthDiD where the treatment timing varies might be tricky. In the case of staggered treatment timing, as one solution, one can estimate the average treatment effect for each treatment cohort and then aggregate cohort-specific average treatment effects to an overall average treatment effects. Here are also some other points that you might want to know when getting started. Things to note: SynthDiD employs regularized ridge regression (L2) while ensuring that the resulting weights have a sum of one. In the process of pretreatment matching, SynthDiD tries to determine the average treatment effect across the entire sample. This approach might cause individual time period estimates to be less precise. Nonetheless, the overall average yields an unbiased evaluation. The standard errors for the treatment effects are estimated with jacknife or if a cohort has only one treated unit with placebo method. The estimator is considered consistent and asymptotically normal, given that the combination of the number of control units and pretreatment periods is sufficiently large relative to the combination of the number of treated units and posttreatment periods. In practice, pre-treatment variables play a minor role in Synthetic DiD, as lagged outcomes hold more predictive power, making the treatment of these variables less critical. Conclusion In this blog post, I introduce the SynthDiD method and discuss its relationship with traditional DiD and SCM. SynthDiD combines the strengths of both SCM and DiD, allowing for causal inference with large panels even when the pretreatment period is short. I demonstrate the method using the synthdid package in R. Although it has several advantages, such as not requiring a strict parallel trends assumption, it also has drawbacks, like being computationally expensive and requiring a balanced panel. Overall, SynthDiD is a valuable tool for researchers interested in estimating causal effects using observational data, providing an alternative to traditional DiD and SCM methods. Robustness checks and validation methods are essential aspects of evaluating the reliability and credibility of empirical research findings, including those derived from the Synthetic Control Method (SCM). Although they are related and sometimes overlap, they serve distinct purposes in the research process. Here’s a detailed explanation of the differences between them and why each is important: 7.1.14 Robustness Checks Definition: Robustness checks are procedures used to assess the sensitivity and stability of research findings to various assumptions, model specifications, and data perturbations. The goal is to determine whether the results hold under different conditions and to identify any potential weaknesses in the analysis. Purpose: - Assess Stability: Ensure that the findings are not unduly influenced by specific choices made in the analysis (e.g., selection of control units, predictor variables). - Identify Key Drivers: Determine which aspects of the model or data are most influential in driving the results. - Evaluate Generalizability: Check whether the results are consistent across different sub-samples or alternative model specifications. Examples of Robustness Checks: 1. Leave-One-Out Cross-Validation: Assess how the results change when each control unit is excluded one at a time. 2. Alternative Model Specifications: Test different sets of predictor variables or alternative functional forms of the model. 3. Sensitivity Analysis: Evaluate the impact of small changes in the data or assumptions on the estimated treatment effect. 4. In-Sample Prediction: Check the model’s performance in predicting the outcome variable within the pre-intervention period. 7.1.15 Validation Methods Definition: Validation methods are procedures used to confirm that the analytical approach and findings are credible and correctly specified. The goal is to ensure that the methodology accurately captures the causal relationship of interest and that the results are not artifacts of methodological flaws. Purpose: - Establish Credibility: Demonstrate that the research design and methods are sound and that the findings are credible. - Detect Biases: Identify and correct any biases or errors in the analysis that could distort the results. - Provide Evidence for Causal Claims: Strengthen the argument that the observed effects are truly caused by the intervention rather than other factors. Examples of Validation Methods: 1. Placebo Tests: Apply the methodology to units or time periods where no intervention occurred to ensure that no significant effects are detected. 2. Pre-Intervention Fit: Ensure that the synthetic control closely matches the treated unit’s trajectory in the pre-intervention period. 3. Falsification Tests: Use outcomes that should not be affected by the intervention to check for false positives. 4. External Validation: Compare the findings with results from other studies or alternative methodologies to check for consistency. "],["instrumental-variables-iv.html", "Chapter 8 Instrumental Variables (IV) 8.1 Key Concepts 8.2 Difference Between Instrumental Variable (IV) Method and Two-Stage Least Squares (2SLS) Method 8.3 Homogenous Treatment Effect 8.4 Heterogenous Treatment Effect", " Chapter 8 Instrumental Variables (IV) Instrumental Variables (IV) is a method used in econometrics and statistics to estimate causal relationships when controlled experiments are not feasible and there is an endogeneity problem—typically due to omitted variable bias, measurement error, or simultaneity. 8.1 Key Concepts Endogeneity: Occurs when an explanatory variable is correlated with the error term. This correlation violates the OLS assumption that the regressors are exogenous, leading to biased and inconsistent estimates. Instrumental Variable: A variable (or set of variables) that is correlated with the endogenous explanatory variable but uncorrelated with the error term. It helps to isolate the exogenous variation in the endogenous explanatory variable. 8.1.1 Requirements for a Valid Instrument A valid instrument must satisfy two key conditions: 1. Relevance: The instrument must be correlated with the endogenous explanatory variable. - Mathematically: $ (Z, X) $ - This ensures that the instrument can explain some of the variations in the endogenous regressor. Exogeneity: The instrument must be uncorrelated with the error term in the structural equation. Mathematically: $ (Z, u) = 0 $ This ensures that the instrument does not suffer from the same endogeneity problem as the endogenous regressor. 8.1.2 The IV Estimation Process The IV estimation is typically carried out in two stages, known as Two-Stage Least Squares (2SLS): First Stage: Regress the endogenous variable on the instrument(s) to obtain the predicted values of the endogenous variable. Equation: \\(X = \\pi_0 + \\pi_1 Z + v\\) Obtain the fitted values \\(\\hat{X}\\) from this regression. Second Stage: Regress the dependent variable on the predicted values from the first stage. Equation: \\(Y = \\beta_0 + \\beta_1 \\hat{X} + \\epsilon\\) The coefficient \\(\\beta_1\\) from this regression is the IV estimate of the effect of \\(X\\) on \\(Y\\). 8.1.3 Example 8.1.3.1 Context: Education and Earnings Endogeneity Problem: In estimating the effect of education on earnings, the variable “years of education” might be endogenous due to omitted variables like ability or family background. Instrument: Distance to the nearest college could be used as an instrument for education. Relevance: Distance to college affects the likelihood of obtaining more education. Exogeneity: Distance to college is unlikely to be directly related to individual earnings, apart from its effect on education. 8.1.3.2 Steps: First Stage: Regress “years of education” on “distance to college”. Equation: \\(\\text{Education} = \\pi_0 + \\pi_1 (\\text{Distance to College}) + v\\) Obtain the fitted values \\(\\widehat{\\text{Education}}\\). Second Stage: Regress “earnings” on the fitted values from the first stage. Equation: \\(\\text{Earnings} = \\beta_0 + \\beta_1 (\\widehat{\\text{Education}}) + \\epsilon\\) The coefficient \\(\\beta_1\\) is the IV estimate of the effect of education on earnings. 8.1.4 Assumptions and Considerations Instrument Strength: Weak instruments can lead to biased estimates even in large samples. The relevance condition must be strong enough. Test: First stage F-statistic (rule of thumb: F &gt; 10). As F gets larger, then bias goes 0. Over-Identification Test: When there are multiple instruments, it’s possible to test for the exogeneity of instruments using over-identification tests like the Sargan or Hansen J test. Single vs. Multiple Instruments: Using multiple instruments can improve the efficiency of the IV estimator, provided all instruments are valid. 8.1.5 Advantages of IV Provides consistent estimates in the presence of endogeneity. Can be used when randomized experiments are not feasible. 8.1.6 Disadvantages of IV Finding valid instruments can be difficult. Weak instruments can lead to biased and imprecise estimates. Interpretation of the IV estimate can be less straightforward, often reflecting a Local Average Treatment Effect (LATE) rather than the Average Treatment Effect (ATE). 8.1.7 Conclusion Instrumental Variables (IV) is a powerful method for addressing endogeneity in observational studies, allowing researchers to estimate causal effects more accurately. Understanding the conditions for valid instruments and the proper application of the 2SLS method is crucial for leveraging this technique effectively. For your interview, be prepared to discuss the theory, assumptions, applications, and potential pitfalls of IV estimation. 8.2 Difference Between Instrumental Variable (IV) Method and Two-Stage Least Squares (2SLS) Method The terms “Instrumental Variable (IV) method” and “Two-Stage Least Squares (2SLS) method” are closely related but not identical. Here’s a detailed explanation of the differences: 8.2.1 Instrumental Variable (IV) Method Instrumental Variable (IV) method is a general approach used to address endogeneity in regression models. It involves using instruments—variables that are correlated with the endogenous explanatory variable but uncorrelated with the error term. The IV method can be implemented in various ways, one of which is the 2SLS method. Purpose: To obtain consistent estimates when there is endogeneity due to omitted variable bias, measurement error, or simultaneity. Instruments: The choice of instruments is crucial. Valid instruments must satisfy two key conditions: Relevance: The instrument must be correlated with the endogenous explanatory variable. Exogeneity: The instrument must be uncorrelated with the error term in the regression model. 8.2.2 Two-Stage Least Squares (2SLS) Method Two-Stage Least Squares (2SLS) method is a specific implementation of the IV method. It involves a two-step estimation process to address endogeneity. Step 1 (First Stage): Regress the endogenous explanatory variable on the instruments to obtain the predicted values. \\[X = \\pi_0 + \\pi_1 Z + \\nu\\] Here, \\(X\\) is the endogenous variable, \\(Z\\) is the instrument, and \\(\\nu\\) is the error term. This regression yields the predicted values \\(\\hat{X}\\). Step 2 (Second Stage): Regress the dependent variable on the predicted values of the endogenous variable obtained from the first stage. \\[Y = \\alpha + \\beta \\hat{X} + \\epsilon\\] The 2SLS method ensures that the endogenous variable \\(X\\) is replaced by its predicted value \\(\\hat{X}\\), which is purged of the endogenous component, thus providing consistent estimates of \\(\\beta\\). 8.2.3 Summary of Differences: General Approach vs. Specific Method: The IV method is a general approach for dealing with endogeneity, while 2SLS is a specific implementation of the IV method. Implementation: 2SLS involves a two-step process to obtain consistent estimates using instruments, whereas the IV method can be implemented in various ways, not just through 2SLS. Application: In practice, 2SLS is the most commonly used method for implementing the IV approach because it is straightforward and provides clear steps for estimation. 8.3 Homogenous Treatment Effect 8.4 Heterogenous Treatment Effect Note that the treatment effect differes by individual i. \\[Y^{1}_i - Y^{0}_i = \\delta_i\\] The main questions we have now are: what is IV estimating when we have heterogeneous treatment effects, and under what assumptions will IV identify a causal effect with heterogeneous treatment effects? we introduce a distinction between the internal validity of a study and its external validity. Internal validity means our strategy identified a causal effect for the population we studied. But external validity means the study’s finding applied to different populations (not in the study). There are considerably more assumptions necessary for identification once we introduce heterogeneous treatment effects—specifically five assumptions. A stable unit treatment value assumption (SUTVA) that states that the potential outcomes for each person are unrelated to the treatment status of other individuals. 2.The independence assumption. The independence assumption is also sometimes called the “as good as random assignment” assumption. It states that the IV is independent of the potential outcomes and potential treatment assignments. we can check pretreatment covariate balance. There is the exclusion restriction. The exclusion restriction states that any effect of Z on Y must be via the effect of Z on D. First stage. The monotonicity assumption. Monotonicity requires that the instrumental variable (weakly) operate in the same direction on all individual units. In other words, while the instrument may have no effect on some people, all those who are affected are affected in the same direction (i.e., positively or negatively, but not both). If all 5 assumptions hold truem then valid IV strategy estimates the local average treatment effect (LATE) of on D on Y. The LATE framework partitions the population of units with an instrument into potentially four mutually exclusive groups. Those groups are: 1. Compliers: This is the subpopulation whose treatment status is affected by the instrument in the correct direction. That is, \\(D^{1}_i=1\\) and \\(D^{0}_i=0\\). 2. Defiers: This is the subpopulation whose treatment status is affected by the instrument in the wrong direction. That is, \\(D^{1}_i=0\\) and \\(D^{0}_i=1\\). 3. Never takers: This is the subpopulation of units that never take the treatment regardless of the value of the instrument. So, \\(D^{1}_i=D^{0}_i=0\\). They simply never take the treatment 4. Always takers: This is the subpopulation of units that always take the treatment regardless of the value of the instrument. So, \\(D^{1}_i=D^{0}_i=1\\). They simply always take the instrument. CAVEAT With all five assumptions satisfied, IV estimates the average treatment effect for compliers, which is the parameter we’ve called the local average treatment effect. Without further assumptions, LATE is not informative about effects on never-takers or always-takers because the instrument does not affect their treatment status. It matters because in most applications, we would be mostly interested in estimating the average treatment effect on the whole population, but that’s not usually possible with IV. "],["regression-discontinuity-designs-rdd.html", "Chapter 9 Regression Discontinuity Designs (RDD) 9.1 Regression Discontinuity Designs (RDD) 9.2 Key Concepts 9.3 Sharp RDD 9.4 Regression Kink Design", " Chapter 9 Regression Discontinuity Designs (RDD) (Comprehensive Source)[https://rdpackages.github.io] In the absence of randomized treatment assignment, research designs that allow for the rigorous study of non-experimental interventions are particularly promising. One of these is the Regression Discontinuity (RD) design In the RD design, all units have a score, and a treatment is assigned to those units whose value of the score exceeds a known cutoff or threshold, and not assigned to those units whose value of the score is below the cutoff. The key feature of the design is that the probability of receiving the treatment changes abruptly at the known threshold. I units are unable to sort arount the cutoff point, units with scores barely below the cutoff can be used as a comparison group for units with scores barely above it. In order to study causal effects with an RD design, the score, treatment, and cutoff must exist types of approaches A. Continuity based approach: this ensures the smoothness of the regression functions. Use least squares and polynomials, global or local to cutoff. The reason is that global polynomial approximations tend to deliver a good approximation overall, but a poor approximation at boundary points— Local polynomial methods are much better The idea that the treatment assignment is “as good as” randomly assigned in a neighborhood of the cutoff has been often invoked in the continuity-based framework to describe the required identification assumptions in an intuitive way, and it has also been used to develop formal results. However, within the continuity-based framework, the formal derivation of identification and esti- mation results always ultimately relies on continuity and differentiability of regression functions, and the idea of local randomization is used as a heuristic device only. In contrast, the local randomization approach to RD analysis formalizes the idea that the RD design behaves like a randomized experiment near the cutoff by imposing explicit randomization-type assumptions that are stronger than the continuity-based conditions. less sensitive to outliers or other extreme features of the data Local polynomial methods implement linear regression fits using only observations near the cutoff point, separately for control and treatment units. h: bandwidth that determines the size of the neighborhood around the cutoff where the empirical RD analysis is conducted. the weights are determined by a kernel function K(·) goal: fit the local polynomial that approximates the unknown regression functions around the cutoff. Local polynomial estimation consists of the following basic steps. Choose a polynomial order p and a kernel function K(·). Choose a bandwidth h. For observations above the cutoff (i.e., observations with Xi ≥ c), fit a weighted least squares regression of the outcome Yi For observations below the cutoff (i.e., observations with Xi &lt; c), fit a weighted least squares regression of the outcome Yi Kernel: assigns weights to units based on the distance: Triangular, uniform (simple linear regression), Epanechnikov polynomial order: 0 constant fit; increasing order means more accuracy but more variability, overfitting. in general the local linear estimator seems to deliver a good trade-off between simplicity, precision, and stability in RD settings. bandwidth: Choosing a smaller h will reduce the misspecification error (also known as “smoothing bias”) of the local polynomial approximation, but will simultaneously tend to increase the variance of the estimated coefficients because fewer observations will be available for estimation. the choice of bandwidth is said to involve a “bias-variance trade-off.” MSE-optimal bandwidth for the local polynomial RD estimate, Example These are assuming uniform kernel, no weights and polynomial degree 1… local poly-nomial point estimation is simply a weighted least-squares fit. linear reg y on x for both sides intercept 2 - intercept 1 B. linear reg Y on X + T + X*T within bandwidth coefficient of T C. rdrobust with p=1, kernel=uniform rdrobust has many more options to use fully non parametric B. Local Randomization: In a nutshell, the local randomization approach imposes conditions so that units above and below the cutoff whose score values lie in a small window around the cutoff are comparable to each other and thus can be studied “as if” they had been randomly assigned to treatment or control. When the running variable is continuous, the local randomization approach typically requires stronger assumptions than the continuity-based approach; in these cases, it is natural to use the continuity-based approach for the main RD analysis, and to use the local randomization approach as a robustness check. But in settings where the running variable is discrete or other departures from the canonical RD framework occur, the local randomization approach no longer imposes the strongest assumptions and can be a natural and useful method for analysis. https://rdpackages.github.io/references/Cattaneo-Idrobo-Titiunik_2024_CUP.pdf 9.0.1 Validation and Falsification If the RD cutoff is known to the units that will be the beneficiaries of the treatment, researchers must worry about the possibility of units actively changing or manipulating the value of their score when they miss the treatment barely Naturally, the continuity assumptions that guarantee the validity of the RD design are about unobservable features and as such are inherently untestable. 1. Predetermined Covariates and Placebo Outcomes One of the most important RD falsification tests involves examining whether, near the cutoff, treated units are similar to control units in terms of observable characteristics. if units lack the ability to precisely manipulate the score value they receive, there should be no systematic differences between units with similar values of the score Predetermined Covariates: variables determined before treatment. placebo outcomes: variables determined after treatment placebo outcomes arealways specific to each application. For example, for the study investigating the impact of clean water on child mortality, road accidents for child mortaility. Outcomes would not be affected by treatment if there is no coincidence. In the continuity-based approach, this principle means that for each predetermined covariate or placebo outcome, researchers should first choose an optimal bandwidth, and then use local polynomial techniques within that bandwidth to estimate the “treatment effect” and employ valid inference procedures such as the robust bias-corrected methods discussed previously. The fundamental idea behind this test is that, since the pre- determined covariate (or placebo outcome) could not have been affected by the treatment, the null hypothesis of no treatment effect should not be rejected if the RD design is valid. To implement this formal falsification test, we simply run rdrobust using each covariate of interest as the outcome variable. (Read page 94)[https://rdpackages.github.io/references/Cattaneo-Idrobo-Titiunik_2020_CUP.pdf] 2. Density of Running variable Check whether there is sorting. Examine whether in a local neighborhood near the cutoff, the number of observations below the cutoff is surprisingly different from the number of observations above it. if units do not have the ability to precisely manipulate the value of the score that they receive, the number of treated observations just above the cutoff should be approximately similar to the number of control observations below it. No guideline on bandwidth to be inspected, several bandwidths may be presented. Histogram would be helpful. Formal test uses binomial test. Choose a small neighborhood around the cutoff, and perform a simple Bernoulli test within that neighborhood with a probability of “success” equal to 1/2. This strategy tests whether the number of treated observations in the chosen neighborhood is compatible with what would have been observed if units had been assigned to the treatment group (i.e., to being above the cutoff) with a 50% probability. Or rddensity test. Both the continuity-based approach and the local randomization approach rely on the assumption that units that receive very similar score values on opposite sides of the cutoff are comparable to each other in all relevant aspects, except for their treatment status. The main distinction between these frameworks is how the idea of comparability is formalized: in the continuity-based framework, comparability is conceptualized as continuity of average (or some other feature of) potential outcomes near the cutoff, while in the local randomization framework, comparability is conceptualized as conditions that mimic a randomized experiment in a neighborhood around the cutoff. 3. Placebo Cutoffs Another useful falsification analysis examines treatment effects at artificial or placebo cutoff values. Evidence of continuity away from the cutoff is, of course, neither necessary nor sufficient for continuity at the cutoff, but the presence of discontinuities away from the cutoff can be interpreted as potentially casting doubt on the RD design This test replaces the true cutoff value by another value at which the treatment status does not really change, and performs estimation and inference using this artificial cutoff point. The expectation is that no significant treatment effect will occur at placebo cutoff values. To avoid “contamination” due to real treatment effects, for artificial cutoffs above the real cutoff we use only treated observations, and for artificial cutoffs below the real cutoff we use only control observations. Restricting the observations in this way guarantees that the analysis of each placebo cutoff uses only observations with the same treatment status. 4. Sensitivity to Observations near the cutoff If systematic manipulation of score values has occurred, it is natural to assume that the units closest to the cutoff are those most likely to have engaged in manipulation. The idea behind this approach is to exclude such units and then repeat the estimation and inference analysis using the remaining sample. This idea is sometimes referred to as a “donut hole” approach. In practice, it is natural to repeat this exercise a few times to assess the actual sensitivity for different amounts of excluded units. 5. Sensitivity to Bandwidth Choice The method now investigates sensitivity as units are added or removed at the end points of the neighborhood. Choosing the bandwidth is one of the most consequential decisions in RD analysis, because the bandwidth may affect the results and conclusions. In the continuity-based approach, this falsification test is implemented by changing the bandwidth used for local polynomial estimation. As h increases, bias of local polynomial estimator will increase and variance will decrease and CI will get narrower. 9.1 Regression Discontinuity Designs (RDD) Regression Discontinuity Designs (RDD) are quasi-experimental methods used to estimate causal effects when a treatment is assigned based on a cutoff point in a continuous assignment variable. The basic idea is to compare observations just above and below the cutoff, assuming they are similar except for the treatment. RDD is particularly suited to visual analysis. By plotting the running variable against the outcome variable, we can observe any “jumps” in the probability of treatment at the cutoff. These jumps indicate the treatment effect. 9.2 Key Concepts Assignment Variable: A continuous (running) variable that determines treatment assignment based on a cutoff point. Cutoff Point: The threshold value of the assignment variable that determines who receives the treatment. Treatment Group: Observations with assignment variable values above (or below) the cutoff. Control Group: Observations with assignment variable values below (or above) the cutoff. Local Average Treatment Effect (LATE): In RDD, we focus on estimating the treatment effect for observations near the cutoff. Since the probability of receiving the treatment changes abruptly at the cutoff, we compare outcomes for those just above and just below this point to estimate LATE. No Overlap/Common Support: Unlike randomized controlled trials (RCTs), RDD lacks overlap between treatment and control groups across the entire range of the running variable. Instead, it relies on extrapolation by comparing units with different values of the running variable that are close to the cutoff. As we approach the cutoff from either direction, the units become more comparable, which allows us to estimate the causal effect. Handling Extrapolation Bias: All methods used in RDD aim to address the bias arising from the need to extrapolate. These methods ensure that the comparisons made are as clean and unbiased as possible. 9.2.1 Types of RDD Sharp RDD: Treatment assignment is strictly determined by the cutoff. All units above (or below) the cutoff receive the treatment, and none below (or above) do. Fuzzy RDD: Treatment assignment is probabilistic at the cutoff. Not all units above (or below) the cutoff receive the treatment, and some units below (or above) the cutoff may receive the treatment. 9.3 Sharp RDD In Sharp RDD, the treatment is perfectly assigned based on the cutoff point. This can be represented as: \\[ D_i = \\begin{cases} 1 &amp; \\text{if } X_i \\geq c \\\\ 0 &amp; \\text{if } X_i &lt; c \\end{cases}\\] Where: \\(D_i\\) is the treatment indicator. \\(X_i\\) is the assignment variable. \\(c\\) is the cutoff point. Full compliance. only one cutoff. score is continuously distributed 9.3.1 Key Concepts The sharp RDD estimation is interpreted as an average causal effect of the treatment as the running variable approaches the cutoff in the limit, for it is only in the limit that we have overlap. This average causal effect is the local average treatment effect (LATE). Notice the role that extrapolation plays in estimating treatment effects with sharp RDD. If unit \\(i\\) is just below \\(c_0\\), then \\(D_i = 0\\). But if unit \\(i\\) is just above \\(c_0\\), then \\(D_i = 1\\). But for any value of \\(X_i\\), there are either units in the treatment group or the control group, but not both. Therefore, the RDD does not have common support, which is one of the reasons we rely on extrapolation for our estimation. Unit \\(i\\): Represents an individual observation in your dataset. \\(c_0\\): The cutoff value of the assignment variable \\(X\\) that determines treatment assignment. \\(D_i\\): The treatment indicator variable, where \\(D_i = 1\\) if the unit receives the treatment and \\(D_i = 0\\) if it does not. Common Support: A situation where there are treated and control units with similar values of the assignment variable \\(X\\). There is no common support because control and treated groups may not have same value of running variable. 9.3.1.1 Explanation In a sharp RDD: Treatment Assignment: Units are assigned to treatment or control strictly based on whether their assignment variable \\(X\\) is above or below the cutoff \\(c_0\\). If \\(X_i\\) (the assignment variable for unit \\(i\\)) is just below \\(c_0\\), then \\(D_i = 0\\) (the unit is in the control group). If \\(X_i\\) is just above \\(c_0\\), then \\(D_i = 1\\) (the unit is in the treatment group). 9.3.1.2 No Common Support in RDD No Overlap: For any given value of \\(X_i\\), units are either in the treatment group or the control group, but not both. This means that at any specific value of \\(X_i\\), you don’t have both treated and untreated units. For example, if \\(X_i\\) is exactly \\(c_0\\), you don’t have units both treated and untreated at that exact point (in practical terms, it’s often the case we look just below and just above \\(c_0\\)). 9.3.1.3 Extrapolation in RDD Extrapolation: To estimate the treatment effect at the cutoff, we essentially need to compare the outcomes of units just below and just above the cutoff. Since there are no units that have exactly the same value of \\(X\\) but different treatment statuses, we use the units close to the cutoff to infer what would happen if a unit’s treatment status were different. Example: Suppose the cutoff \\(c_0\\) is 50. Units with \\(X_i = 49.9\\) are in the control group and units with \\(X_i = 50.1\\) are in the treatment group. We compare the outcomes of these units to estimate the treatment effect. 9.3.1.4 Why Extrapolation is Needed Local Comparisons: In RDD, we rely on the assumption that units just below and just above the cutoff are very similar in all respects except for the treatment. Thus, we “extrapolate” the behavior of one group to understand the counterfactual of the other. Local Treatment Effect: This local comparison near the cutoff allows us to estimate the causal effect of the treatment precisely at \\(c_0\\). 9.3.1.5 Summary In summary, the lack of common support in RDD means we don’t have units that are both treated and untreated at the same value of the assignment variable. As a result, we rely on extrapolation, comparing units just below and just above the cutoff to estimate the treatment effect. This is because these units are assumed to be similar except for the treatment, allowing us to infer what the outcome would be if their treatment status were different. 9.3.2 Assumptions for RDD 9.3.2.1 Continuity Assumption in RDD Definition: The potential outcomes (both treated and untreated) must be continuous at the cutoff. This ensures that any jump in the outcome at the cutoff can be attributed to the treatment. Expected Potential Outcomes: The assumption states that the expected potential outcomes are continuous at the cutoff. In simpler terms, if we plot the expected outcomes of the units just below and just above the cutoff, the outcomes would form a smooth curve if there were no treatment effect. If there were no treatment, there would not be a jump at the cutoff. Ruling Out Competing Interventions: If expected potential outcomes are continuous at the cutoff, it necessarily rules out competing interventions or other factors that might cause a discontinuity at the cutoff. This is crucial because we want to attribute any observed jump in the outcome to the treatment alone, not to some other unobserved factor. Omitted Variable Bias: Continuity explicitly rules out omitted variable bias at the cutoff. This means that all other unobserved determinants of the outcome variable \\(Y\\) are smoothly related to the running variable \\(X\\). Therefore, any abrupt change at the cutoff can be confidently attributed to the treatment effect. Interpreting the Assumption Mathematically: \\(E[Y^0 | X]\\) and \\(E[Y^1 | X]\\) are the expected outcomes if the unit did not receive and did receive the treatment, respectively. The continuity assumption means that \\(E[Y^0 | X]\\) and \\(E[Y^1 | X]\\) would not exhibit a sudden jump at the cutoff \\(c_0\\) in the absence of treatment. If there is a jump at \\(c_0\\), it indicates the presence of the treatment effect because in the absence of the treatment, \\(E[Y^1 | X]\\) should not change abruptly. Example to Illustrate Continuity Assumption Imagine we are studying the effect of a scholarship program on student test scores, where the scholarship is given to students who score above a certain cutoff on a preliminary test. Continuity without Treatment: If there were no scholarship, the expected test scores of students just below and just above the cutoff should be very similar and form a smooth curve. Jump Due to Treatment: If we observe a jump in test scores exactly at the cutoff, this jump can be attributed to the effect of receiving the scholarship, assuming the continuity assumption holds. Summary The continuity assumption in RDD is crucial for identifying the causal effect of the treatment. It ensures that any observed discontinuity in the outcome at the cutoff can be attributed solely to the treatment and not to any other unobserved factors. This assumption rules out omitted variable bias at the cutoff, ensuring the reliability of the estimated treatment effect. In essence, the continuity assumption guarantees that the treatment effect is the only factor causing a jump in the outcome at the cutoff, allowing us to make causal inferences from the RDD design. 9.3.2.2 No Manipulation (sorting) Units cannot precisely manipulate the assignment variable to end up on one side of the cutoff. This ensures that the units just above and below the cutoff are comparable. 9.3.3 Estimation in Sharp RDD Parametric Approach: Fit separate linear regressions on either side of the cutoff and estimate the treatment effect as the difference in the intercepts at the cutoff. \\[ Y_i = \\alpha_1 + \\beta_1 X_i + \\epsilon_i \\quad \\text{if } X_i \\geq c \\] \\[ Y_i = \\alpha_0 + \\beta_0 X_i + \\epsilon_i \\quad \\text{if } X_i &lt; c \\] The treatment effect is \\(\\alpha_1 - \\alpha_0\\). Non-Parametric Approach: Use local polynomial regression or kernel regression to fit the data near the cutoff. This method is preferred because it makes fewer assumptions about the functional form of the relationship between the assignment variable and the outcome. cluster at running variable ( bad idea) use heteroskedastic robust standard errors kernel type, cutof window 9.3.4 Fuzzy RDD In Fuzzy RDD, the probability of receiving treatment changes discontinuously at the cutoff but is not perfectly deterministic. This can be represented as: \\[ D_i = \\begin{cases} 1 &amp; \\text{with probability } p_1 \\text{ if } X_i \\geq c \\\\ 0 &amp; \\text{with probability } p_0 \\text{ if } X_i &lt; c \\end{cases} \\] Where \\(p_1\\) and \\(p_0\\) are the probabilities of receiving treatment above and below the cutoff, respectively. 9.3.4.1 Estimation in Fuzzy RDD Instrumental Variables (IV) Approach: Use the cutoff as an instrument for actual treatment receipt. The first stage estimates the probability of treatment, and the second stage uses this to estimate the treatment effect. First stage: \\[ D_i = \\pi_0 + \\pi_1 Z_i + \\eta_i \\] Second stage: \\[ Y_i = \\alpha + \\beta \\hat{D}_i + \\epsilon_i \\] Where \\(Z_i\\) is an indicator variable equal to 1 if \\(X_i \\geq c\\) and 0 otherwise. 9.3.5 Parametric vs. Non-Parametric Applications Parametric Applications: Assume a specific functional form (e.g., linear) for the relationship between the assignment variable and the outcome. Simpler to implement but relies heavily on the correct specification of the model. Non-Parametric Applications: Make fewer assumptions about the functional form. Typically use methods like local polynomial regression or kernel regression to fit the data near the cutoff. More flexible and robust but can be more complex to implement and interpret. 9.3.6 Example Suppose a school district awards scholarships to students who score above a certain threshold on a standardized test. Sharp RDD: All students who score above 80 receive the scholarship, and none below 80 do. We compare students scoring just above 80 to those just below to estimate the effect of receiving the scholarship on academic outcomes. Fuzzy RDD: Students who score above 80 are more likely to receive the scholarship, but not all do (e.g., due to additional criteria or random factors). We use the score of 80 as an instrument to estimate the causal effect of receiving the scholarship. 9.3.7 Summary Regression Discontinuity Designs are powerful tools for causal inference in observational studies where treatment assignment is based on a cutoff. Sharp RDDs assume perfect treatment assignment based on the cutoff, while Fuzzy RDDs allow for probabilistic treatment assignment. Both parametric and non-parametric approaches can be used, with non-parametric methods generally preferred for their flexibility. Key assumptions include the continuity of potential outcomes and the inability of units to manipulate their assignment variable precisely. The reason RDD is so appealing to many is because of its ability to convincingly eliminate selection bias. Assignment variable, is often called the “running variable”—is an observable confounder since it causes both Treatment and Outcome. The assignment variable assigns treatment on the basis of a cutoff, we are never able to observe units in both treatment and control for the same value of X. We can identify causal effects for those subjects whose score is in a close neighborhood around some cutoff \\(c_o\\). 9.3.8 Challenges to Identification The requirement for RDD to estimate a causal effect are the continuity assumptions. That is, the expected potential outcomes change smoothly as a function of the running variable through the cutoff. In words, this means that the only thing that causes the outcome to change abruptly at is the treatment. But, this can be violated in practice if any of the following is true: The assignment rule is known in advance. Agents are interested in adjusting. Agents have time to adjust. The cutoff is endogenous to factors that independently cause potential outcomes to shift. There is nonrandom heaping along the running variable. Examples include retaking an exam, self-reporting income, and so on. The cutoff is endogenous. An example would be age thresholds used for policy, such as when a person turns 18 years old and faces more severe penalties for crime. This age threshold triggers the treatment (i.e., higher penalties for crime), but is also correlated with variables that affect the outcomes, such as graduating from high school and voting rights. Let’s tackle these problems separately. Although assumptions may not be tested directly, indirect evidence may be show to be persuasive. 9.3.8.1 Density Test density test is used to check whether units are sorting on the running variable. under the null, the density should be continuous at the cutoff point. Under the alternative hypothesis, the density should increase at the kink. 9.3.8.2 Covariate balance and Other placebos For RDD to be valid in your study, there must not be an observable discontinuous change in the average values of reasonably chosen covariates around the cutoff. As these are pretreatment characteristics, they should be invariant to change in treatment assignment. This test is basically what is sometimes called a placebo test. That is, you are looking for there to be no effects where there shouldn’t be any. So a third kind of test is an extension of that—just as there shouldn’t be effects at the cutoff on pretreatment values, there shouldn’t be effects on the outcome of interest at arbitrarily chosen cutoffs. Guido W. Imbens and Lemieux (2008) suggest looking at one side of the discontinuity, taking the median value of the running variable in that section, and pretending it was a discontinuity, \\(c^{i}_0\\). Then test whether there is a discontinuity in the outcome at \\(c^{i}_0\\). You do not want to find anything. 9.3.8.3 Non-random heaping in running variable Heaping occurs when there is an excess number of units at certain points along the running variable. In this case, it seems to happen at regular 100-gram intervals, likely due to hospitals rounding to the nearest integer. This pattern is unlikely to occur naturally and is almost certainly caused by either sorting or rounding. It could result from less sophisticated scales or, more concerningly, from staff rounding a child’s birth weight to 1,500 grams to make the child eligible for increased medical attention. In RDD, estimation compares means as we approach the threshold from either side, so the estimates should not be overly influenced by the observations at the threshold itself. One solution to address this issue is the “donut hole” RDD, where units in the vicinity of 1,500 grams are removed, and the model is re-estimated. 9.3.9 Examples 9.3.9.1 Close Election Regression Discontinuity Design (RDD) can be effectively used in the context of close elections to identify causal effects. The key idea is that in very close elections, the outcome of the election (winning or losing) can be considered as good as random. This randomness allows researchers to estimate the causal effect of winning an election on various outcomes. They argue that just around that cutoff, random chance determined the Democratic win—hence the random assignment of \\(D_t\\) Identifying the Impact of Political Office on Economic Policies: Scenario: Consider a study aiming to determine whether holding political office affects a politician’s subsequent policy decisions or economic outcomes in their district. RDD Approach: Researchers focus on elections decided by a very small margin of votes. For example, sample includes only observations where the Democrat vote share at time is strictly between 48 percent and 52 percent. Assumption: In close elections, the distribution of voter preferences is assumed to be similar on both sides of the cutoff (winning or losing by a small margin). This similarity allows the comparison of outcomes just above and just below the threshold. Application: By comparing districts where the incumbent barely won to those where the incumbent barely lost, researchers can isolate the effect of holding office on policy decisions and economic outcomes. 9.3.9.2 Education Policies Identifying the Impact of Scholarship Programs: Scenario: A scholarship program is awarded to students who score above a certain threshold on an entrance exam. RDD Approach: Researchers compare students who score just above the threshold (and receive the scholarship) to those who score just below (and do not receive the scholarship). Assumption: Students near the cutoff are similar in all respects except for receiving the scholarship. Application: By analyzing differences in educational attainment and future earnings between the two groups, researchers can estimate the causal impact of the scholarship program. 9.3.9.3 Health Interventions Identifying the Impact of Health Interventions: Scenario: A public health intervention is provided to individuals whose health risk score exceeds a certain threshold. RDD Approach: Researchers compare individuals who just qualify for the intervention to those who just miss the qualification. Assumption: Individuals near the threshold are comparable in health status except for receiving the intervention. Application: By examining health outcomes such as disease incidence or hospitalization rates, researchers can infer the causal effect of the health intervention. 9.3.10 Types of RDD 9.3.10.1 Sharp RDD In sharp RDD, the treatment assignment is strictly determined by whether the running variable crosses a threshold. Example: A tax credit is given to families whose income is below a certain cutoff. Families just below the cutoff receive the tax credit, while those just above do not. 9.3.10.2 Fuzzy RDD In fuzzy RDD, the probability of treatment assignment jumps at the threshold but not perfectly. Example: Eligibility for a drug rehabilitation program is determined by a cutoff on an addiction severity score, but not all eligible individuals enroll in the program. 9.3.11 Parametric vs. Non-Parametric Applications 9.3.11.1 Parametric RDD Parametric RDD involves fitting a parametric model (e.g., a polynomial regression) to estimate the relationship between the running variable and the outcome. Example: Using a polynomial regression model to estimate the impact of an education intervention on test scores. 9.3.11.2 Non-Parametric RDD Non-parametric RDD uses local polynomial regression or other non-parametric methods to estimate the treatment effect, focusing on observations near the cutoff. Example: Applying local linear regression to estimate the impact of a health intervention on patient recovery rates. 9.4 Regression Kink Design "],["fixed-effects-and-panel-data-methods.html", "Chapter 10 Fixed Effects and Panel Data Methods 10.1 Pooled Regression 10.2 Panel Data Methods", " Chapter 10 Fixed Effects and Panel Data Methods 10.1 Pooled Regression Pooled regression refers to combining cross-sectional and time series data into a single dataset and estimating a common model without accounting for individual or time-specific effects. This method assumes that all individual observations (from different time periods and entities) share the same underlying regression model. Model Form: \\[ Y_{it} = \\beta_0 + \\beta_1 X_{it} + \\epsilon_{it} \\] where: - \\(Y_{it}\\) is the dependent variable for entity \\(i\\) at time \\(t\\). - \\(X_{it}\\) is the independent variable for entity \\(i\\) at time \\(t\\). - \\(\\epsilon_{it}\\) is the error term. Limitations: Homogeneity Assumption: Assumes that the same relationship between \\(X\\) and \\(Y\\) holds for all individuals and over time, which may not be realistic. Ignored Heterogeneity: Fails to account for unobserved heterogeneity across entities or over time. 10.2 Panel Data Methods Panel data methods leverage the structure of data that follows the same entities over multiple time periods, allowing for more sophisticated modeling that accounts for individual heterogeneity. Advantages: Control for Unobserved Heterogeneity: By following the same entities over time, we can control for time-invariant characteristics of those entities. More Observations: Increases the number of data points, improving the efficiency of estimations. Dynamic Analysis: Allows studying the dynamics of change over time. 10.2.1 Fixed Effects Model The Fixed Effects (FE) model is a popular method in panel data analysis that controls for unobserved heterogeneity by allowing each entity to have its own intercept. This method removes time-invariant characteristics from the data, effectively focusing on within-entity variation. Model Form: \\[ Y_{it} = \\alpha_i + \\beta X_{it} + \\epsilon_{it} \\] where: - \\(\\alpha_i\\) is the entity-specific intercept. - \\(Y_{it}\\) and \\(X_{it}\\) are as defined above. Why Fixed Effects Work: Control for Unobserved Heterogeneity: By allowing each entity its own intercept, the FE model controls for all time-invariant differences between entities. This is particularly useful if there are omitted variables that are correlated with the included independent variables. Elimination of Bias: Time-invariant characteristics (e.g., cultural factors, institutional differences) are differenced out, reducing the risk of omitted variable bias. Within Transformation: The FE model can be estimated by demeaning the data: \\[ \\overline{Y}_{i} = \\frac{1}{T} \\sum_{t=1}^{T} Y_{it} \\] \\[ \\overline{X}_{i} = \\frac{1}{T} \\sum_{t=1}^{T} X_{it} \\] \\[ \\overline{\\epsilon}_{i} = \\frac{1}{T} \\sum_{t=1}^{T} \\epsilon_{it} \\] Then subtract these averages from the original equation: \\[ Y_{it} - \\overline{Y}_{i} = \\beta (X_{it} - \\overline{X}_{i}) + (\\epsilon_{it} - \\overline{\\epsilon}_{i}) \\] This transformation removes \\(\\alpha_i\\) from the equation, allowing for consistent estimation of \\(\\beta\\). 10.2.2 Random Effects Model Random Effects (RE) model assumes that the entity-specific intercepts are random and uncorrelated with the independent variables. It is more efficient than FE if the assumption holds, but biased if the intercepts are correlated with the regressors. Model Form: \\[ Y_{it} = \\alpha + \\beta X_{it} + u_i + \\epsilon_{it} \\] where \\(u_i\\) is the random effect. Hausman Test: Used to decide between FE and RE models. It tests whether the unique errors (random effects) are correlated with the regressors. If the null hypothesis (no correlation) is rejected, the FE model is preferred. 10.2.3 Example: Economic Growth and Education Suppose we want to study the impact of education on economic growth using panel data from different countries over several years. Pooled Regression: Assumes the relationship between education and economic growth is the same across all countries and years. Model: \\(\\text{Growth}_{it} = \\beta_0 + \\beta_1 \\text{Education}_{it} + \\epsilon_{it}\\) Limitation: Ignores country-specific factors (e.g., institutional quality) that might affect growth. Fixed Effects: Accounts for country-specific unobserved factors that are constant over time (e.g., cultural factors, geographical characteristics). Model: \\(\\text{Growth}_{it} = \\alpha_i + \\beta_1 \\text{Education}_{it} + \\epsilon_{it}\\) Interpretation: The coefficient \\(\\beta_1\\) shows the effect of education on economic growth within the same country over time, controlling for time-invariant factors. Random Effects: Assumes that the unobserved country-specific effects are uncorrelated with the independent variables. Model: \\(\\text{Growth}_{it} = \\alpha + \\beta_1 \\text{Education}_{it} + u_i + \\epsilon_{it}\\) Interpretation: The coefficient \\(\\beta_1\\) shows the overall effect of education on economic growth, assuming that the country-specific effects are random. 10.2.4 Conclusion Pooled Regression: Simple but ignores heterogeneity. Fixed Effects: Controls for unobserved time-invariant heterogeneity but only uses within-entity variation. Random Effects: More efficient if the assumption holds but biased if unobserved effects are correlated with the regressors. Understanding these methods and their assumptions is crucial for correctly modeling and interpreting data in econometrics. For your interview, be prepared to explain these concepts, their applications, and when to use each method based on the data characteristics and research question. "],["ordinary-least-squares-ols.html", "Chapter 11 Ordinary Least Squares (OLS) 11.1 Ordinary Least Squares (OLS) 11.2 Simple Linear Regression 11.3 Multivariate Regression 11.4 Generalized Linear Model 11.5 Generalised Least Square 11.6 Weighted Least Squares (WLS)", " Chapter 11 Ordinary Least Squares (OLS) 11.1 Ordinary Least Squares (OLS) Ordinary Least Squares (OLS) is a fundamental technique in regression analysis used to estimate the parameters of a linear model. For OLS estimators to be the Best Linear Unbiased Estimators (BLUE), certain assumptions must hold. Violations of these assumptions can lead to biased, inconsistent, or inefficient estimates. Here are the key assumptions and the potential consequences of their violations: 11.1.1 Linearity Assumption: The relationship between the independent variables (X) and the dependent variable (Y) is linear. Violation: If the true relationship is not linear, the OLS estimates may be biased and inefficient. This can be addressed by transforming variables, adding polynomial terms, or using other non-linear models. Note: Angrist and Pischke (2009) argue that linear regression may be useful even if the underlying CEF itself is not linear, because regression is a good approximation of the CEF. So keep an open mind as I break this down a little bit more. 11.1.2 Exogeneity Assumption: The independent variables are not correlated with the error term. Formally, \\(E(\\epsilon | X) = 0\\). Violation: If this assumption is violated (endogeneity), the OLS estimates are biased and inconsistent. This can occur due to omitted variable bias, measurement error, or simultaneity. Instrumental Variables (IV) or other techniques may be used to address endogeneity. 11.1.3 Homoscedasticity Assumption: The variance of the error term is constant across all levels of the independent variables, i.e., \\(Var(\\epsilon | X) = \\sigma^2\\). Violation: If there is heteroscedasticity (non-constant variance of errors), the OLS estimates remain unbiased, but they are no longer efficient, and the standard errors are biased, leading to unreliable hypothesis tests. Heteroscedasticity-Robust standard errors or Generalized Least Squares (GLS) can be used to address heteroscedasticity. 11.1.4 No Autocorrelation Assumption: The error terms are not correlated with each other, i.e., \\(E(\\epsilon_i \\epsilon_j) = 0\\) for \\(i \\neq j\\). Violation: If there is autocorrelation (correlated errors), the OLS estimates remain unbiased, but they are inefficient, and standard errors are biased, leading to incorrect inferences. This is common in time series data. Techniques such as Newey-West standard errors or autoregressive models can be used to correct for autocorrelation. 11.1.5 No Perfect Multicollinearity Assumption: There is no perfect linear relationship among the independent variables. Violation: Perfect multicollinearity makes it impossible to estimate the coefficients uniquely. High but imperfect multicollinearity can inflate the variances of the coefficient estimates, making them unstable and sensitive to changes in the model. This can be addressed by removing or combining collinear variables, or using techniques such as Ridge Regression. 11.1.6 Normality of Errors (for inference) Assumption: The error terms are normally distributed, particularly important for small sample sizes to conduct valid hypothesis tests. Violation: If the errors are not normally distributed, the OLS estimates are still unbiased and consistent, but the hypothesis tests may be invalid. For large samples, the Central Limit Theorem mitigates this issue, but for small samples, transformations or non-parametric methods might be necessary. 11.1.7 Practical Considerations and Tests Detecting Violations: Linearity: Scatter plots, residual plots, and tests like the RESET test. Exogeneity: Hausman test for endogeneity, instrumental variable techniques. Homoscedasticity: Residual plots, Breusch-Pagan test, White test. Autocorrelation: Durbin-Watson test, Ljung-Box test. Multicollinearity: Variance Inflation Factor (VIF), condition index. Normality: Q-Q plots, Shapiro-Wilk test, Kolmogorov-Smirnov test. Understanding these assumptions and how to address their violations is crucial for robust regression analysis and accurate inference using OLS. 11.2 Simple Linear Regression Let’s derive the Ordinary Least Squares (OLS) estimators for the simple linear regression model: \\[ Y = \\alpha + \\beta X + \\epsilon \\] Here, \\(Y\\) is the dependent variable, \\(X\\) is the independent variable, \\(\\alpha\\) (alpha) is the intercept, \\(\\beta\\) (beta) is the slope, and \\(\\epsilon\\) (epsilon) is the error term. To find the OLS estimators, we need to minimize the sum of squared residuals (SSR): \\[ SSR = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 \\] \\[ \\hat{Y}_i = \\hat{\\alpha} + \\hat{\\beta} X_i \\] Substituting \\(\\hat{Y}_i\\): \\[ SSR = \\sum_{i=1}^n (Y_i - (\\hat{\\alpha} + \\hat{\\beta} X_i))^2 \\] To minimize SSR with respect to \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\), we take partial derivatives and set them to zero: Partial derivative with respect to \\(\\hat{\\alpha}\\): \\[ \\frac{\\partial SSR}{\\partial \\hat{\\alpha}} = \\sum_{i=1}^n -2(Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i) = 0 \\] Simplifying: \\[ \\sum_{i=1}^n (Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i) = 0 \\] \\[ \\sum_{i=1}^n Y_i - n\\hat{\\alpha} - \\hat{\\beta}\\sum_{i=1}^n X_i = 0 \\] Solving for \\(\\hat{\\alpha}\\): \\[ n\\hat{\\alpha} = \\sum_{i=1}^n Y_i - \\hat{\\beta} \\sum_{i=1}^n X_i \\] \\[ \\hat{\\alpha} = \\frac{1}{n} \\sum_{i=1}^n Y_i - \\hat{\\beta} \\frac{1}{n} \\sum_{i=1}^n X_i \\] \\[ \\hat{\\alpha} = \\bar{Y} - \\hat{\\beta} \\bar{X} \\] Where \\(\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^n Y_i\\) and \\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\\). Partial derivative with respect to \\(\\hat{\\beta}\\): \\[ \\frac{\\partial SSR}{\\partial \\hat{\\beta}} = \\sum_{i=1}^n -2X_i(Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i) = 0 \\] Simplifying: \\[ \\sum_{i=1}^n X_i(Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i) = 0 \\] Substitute \\(\\hat{\\alpha}\\) from the previous result: \\[ \\sum_{i=1}^n X_i Y_i - \\hat{\\alpha}\\sum_{i=1}^n X_i - \\hat{\\beta}\\sum_{i=1}^n X_i^2 = 0 \\] \\[ \\sum_{i=1}^n X_i Y_i - (\\bar{Y} - \\hat{\\beta} \\bar{X})\\sum_{i=1}^n X_i - \\hat{\\beta}\\sum_{i=1}^n X_i^2 = 0 \\] Substitute \\(\\hat{\\alpha} = \\bar{Y} - \\hat{\\beta} \\bar{X}\\): \\[ \\sum_{i=1}^n X_i Y_i - (\\bar{Y}\\sum_{i=1}^n X_i - \\hat{\\beta} \\bar{X} \\sum_{i=1}^n X_i) - \\hat{\\beta}\\sum_{i=1}^n X_i^2 = 0 \\] \\[ \\sum_{i=1}^n X_i Y_i - \\bar{Y}\\sum_{i=1}^n X_i + \\hat{\\beta} \\bar{X} \\sum_{i=1}^n X_i - \\hat{\\beta}\\sum_{i=1}^n X_i^2 = 0 \\] \\[ \\sum_{i=1}^n X_i Y_i - \\bar{Y}\\sum_{i=1}^n X_i = \\hat{\\beta}\\left(\\sum_{i=1}^n X_i^2 - \\bar{X} \\sum_{i=1}^n X_i\\right) \\] Since \\(\\sum_{i=1}^n \\bar{X} = n \\bar{X}\\): \\[ \\sum_{i=1}^n X_i Y_i - \\bar{Y}\\sum_{i=1}^n X_i = \\hat{\\beta}\\left(\\sum_{i=1}^n X_i^2 - n \\bar{X}^2\\right) \\] Simplifying: \\[ \\sum_{i=1}^n X_i Y_i - n \\bar{X} \\bar{Y} = \\hat{\\beta}\\left(\\sum_{i=1}^n X_i^2 - n \\bar{X}^2\\right) \\] Finally, solving for \\(\\hat{\\beta}\\): \\[ \\hat{\\beta} = \\frac{\\sum_{i=1}^n X_i Y_i - n \\bar{X} \\bar{Y}}{\\sum_{i=1}^n X_i^2 - n \\bar{X}^2} \\] Alternatively, this can be written in terms of deviations from the mean: \\[ \\hat{\\beta} = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2} \\] 11.2.1 Summary The OLS estimator for the slope (\\(\\beta\\)) is: \\[ \\hat{\\beta} = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2} \\] The OLS estimator for the intercept (\\(\\alpha\\)) is: \\[ \\hat{\\alpha} = \\bar{Y} - \\hat{\\beta} \\bar{X} \\] These derivations show how the OLS estimators for \\(\\beta\\) and \\(\\alpha\\) are obtained by minimizing the sum of squared residuals in a simple linear regression model. 11.2.2 Interpreting Model Coefficients in OLS Models In Ordinary Least Squares (OLS) regression, the interpretation of coefficients depends on the functional form of the model and the nature of the variables involved. Here are some common types of models and how to interpret their coefficients. 11.2.3 1. Linear Models (Linear-Linear) Model Form: \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\) Coefficients Interpretation: Continuous Variable: \\(\\beta_1\\) represents the change in \\(Y\\) for a one-unit increase in \\(X\\). Example: If \\(Y\\) is annual income in dollars and \\(X\\) is years of education, \\(\\beta_1 = 2000\\) means an additional year of education increases income by $2000. Dummy Variable: \\(\\beta_1\\) represents the difference in \\(Y\\) between the reference category (usually coded as 0) and the category represented by the dummy variable (coded as 1). Example: If \\(Y\\) is income and \\(X\\) is a dummy variable for gender (1 for male, 0 for female), \\(\\beta_1 = 5000\\) means males earn $5000 more than females, holding other factors constant. 11.2.4 2. Log-Linear Models Model Form: \\(\\log(Y) = \\beta_0 + \\beta_1 X + \\epsilon\\) Coefficients Interpretation: Continuous Variable: \\(\\beta_1\\) represents the approximate percentage change in \\(Y\\) for a one-unit increase in \\(X\\). Example: If \\(\\log(Y)\\) is the natural log of income and \\(X\\) is years of education, \\(\\beta_1 = 0.05\\) means an additional year of education increases income by approximately 5%. Dummy Variable: \\(\\beta_1\\) represents the approximate percentage difference in \\(Y\\) between the reference category and the category represented by the dummy variable. Example: If \\(\\log(Y)\\) is the natural log of income and \\(X\\) is a dummy for gender, \\(\\beta_1 = 0.10\\) means males earn approximately 10% more than females, holding other factors constant. 11.2.5 3. Linear-Log Models Model Form: \\(Y = \\beta_0 + \\beta_1 \\log(X) + \\epsilon\\) Coefficients Interpretation: Continuous Variable: \\(\\beta_1\\) represents the change in \\(Y\\) for a 1% change in \\(X\\) (since a 1% change in \\(X\\) is approximately a change of 0.01 in \\(\\log(X)\\)). Example: If \\(Y\\) is income and \\(\\log(X)\\) is the natural log of years of experience, \\(\\beta_1 = 1000\\) means a 1% increase in experience leads to an increase in income of $10 (since \\(1000 \\times 0.01 = 10\\)). Dummy Variable: Less common in linear-log models but would be interpreted similarly to linear models. 11.2.6 4. Log-Log Models Model Form: \\(\\log(Y) = \\beta_0 + \\beta_1 \\log(X) + \\epsilon\\) Coefficients Interpretation: Continuous Variable: \\(\\beta_1\\) represents the elasticity of \\(Y\\) with respect to \\(X\\), meaning the percentage change in \\(Y\\) for a 1% change in \\(X\\). Example: If \\(\\log(Y)\\) is the natural log of income and \\(\\log(X)\\) is the natural log of years of education, \\(\\beta_1 = 0.5\\) means a 1% increase in years of education results in a 0.5% increase in income. Dummy Variable: Less common in log-log models but would be interpreted as in log-linear models. 11.2.7 Examples of Dummy and Continuous Variables 11.2.7.1 Example 1: Linear Model with Dummy and Continuous Variables Model: \\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 D + \\epsilon\\) \\(Y\\): Income \\(X_1\\): Years of education (continuous) \\(D\\): Gender (dummy, 1 for male, 0 for female) Interpretation: - \\(\\beta_1\\): Change in income for an additional year of education. - \\(\\beta_2\\): Difference in income between males and females, holding education constant. 11.2.7.2 Example 2: Log-Linear Model with Continuous Variables Model: \\(\\log(Y) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\\) \\(\\log(Y)\\): Log of income \\(X_1\\): Years of education (continuous) \\(X_2\\): Years of work experience (continuous) Interpretation: - \\(\\beta_1\\): Percentage change in income for an additional year of education. - \\(\\beta_2\\): Percentage change in income for an additional year of work experience. 11.2.7.3 Example 3: Log-Log Model with Continuous Variables Model: \\(\\log(Y) = \\beta_0 + \\beta_1 \\log(X_1) + \\beta_2 \\log(X_2) + \\epsilon\\) \\(\\log(Y)\\): Log of income \\(\\log(X_1)\\): Log of years of education \\(\\log(X_2)\\): Log of years of work experience Interpretation: - \\(\\beta_1\\): Elasticity of income with respect to education. - \\(\\beta_2\\): Elasticity of income with respect to work experience. 11.2.8 Assumptions and Considerations Linearity: The relationship between the variables should be correctly specified (linear, log-linear, etc.). Homoscedasticity: The variance of the error terms should be constant across all levels of the independent variables. No Multicollinearity: Independent variables should not be too highly correlated with each other. No Autocorrelation: The residuals should not be correlated with each other (important in time series data). Normality of Errors: The error terms should be normally distributed, especially for small sample sizes. Understanding these models and their interpretations is crucial for accurately conveying the impact of variables in econometric analyses. Being prepared to explain these interpretations clearly and provide relevant examples will be beneficial for your interview. 11.3 Multivariate Regression It will eventually become second nature for you to talk about including more variables on the right side of a regression as “controlling for” those variables. 11.4 Generalized Linear Model 11.5 Generalised Least Square 11.6 Weighted Least Squares (WLS) WLS is a special case of GLS where the variance-covariance matrix Σ of the error terms is diagonal. This means that the errors are uncorrelated but may have different variances. WLS is used specifically to address heteroscedasticity by assigning different weights to different observations based on their error variances. Weights: In WLS, each observation is assigned a weight that is inversely proportional to the variance of its error term. If an observation has a high variance, it receives a lower weight, and vice versa. "],["resampling-methods.html", "Chapter 12 Resampling methods 12.1 Randomization-Based Methods 12.2 Bootstrapping", " Chapter 12 Resampling methods 12.1 Randomization-Based Methods Athey and Imbens are prominent economists who have contributed significantly to the development and application of randomization-based methods in econometrics. These methods are particularly valuable for inferring the probability that an estimated coefficient is not simply a result of chance. Here’s an explanation of the context and importance of their work: 12.1.1 Traditional Methods vs. Randomization-Based Methods Traditional Methods: Economists often use parametric tests (like t-tests or F-tests) which rely on assumptions about the distribution of the error terms (e.g., normality, homoscedasticity). These methods use asymptotic approximations to calculate p-values, which may not always be accurate, especially in small samples or when assumptions are violated. Randomization-Based Methods: These methods, such as permutation tests, do not rely on these distributional assumptions. Instead, they use the actual data to construct the distribution of the test statistic under the null hypothesis. This approach involves reshuffling the treatment labels or data points multiple times to create a distribution of the test statistic that purely reflects random chance. 12.1.2 Why Use Randomization-Based Methods? Exact P-Values: Randomization methods can construct exact p-values, which provide a more precise measure of the probability that the observed effect could have arisen by chance. This is especially important in cases where traditional assumptions do not hold. Robustness: These methods are robust to violations of common statistical assumptions (e.g., non-normality, heteroscedasticity). They rely on the randomization process used in the experiment or observational study, ensuring that the inferences are valid regardless of the underlying data distribution. Small Samples: In small sample sizes, randomization-based methods are particularly advantageous because traditional asymptotic approximations may be unreliable. 12.1.3 How Randomization-Based Methods Work Calculate the Observed Statistic: First, calculate the test statistic (e.g., the difference in means, regression coefficient) from the observed data. Randomize Data: Randomly reassign the treatment labels or shuffle the data points many times (typically thousands). Each randomization should maintain the structure of the data but break any systematic association between treatment and outcome. Calculate Test Statistics for Randomizations: For each randomization, calculate the test statistic. This generates a distribution of the test statistic under the null hypothesis of no treatment effect. Compare Observed Statistic: Compare the observed test statistic to this distribution. The p-value is the proportion of randomized test statistics that are as extreme as or more extreme than the observed statistic. 12.1.4 Example Suppose Athey and Imbens are evaluating the effect of a training program on income: Observed Difference in Means: The difference in mean income between the treatment (received training) and control (no training) groups is calculated. Randomization: The treatment labels (received training or not) are randomly shuffled, and the difference in means is recalculated for each shuffle. This process is repeated many times. Distribution of Differences: The randomization process generates a distribution of differences in means under the null hypothesis that training has no effect. Calculate P-Value: The p-value is the proportion of differences from the randomization distribution that are as extreme or more extreme than the observed difference. 12.1.5 Contribution of Athey and Imbens Athey and Imbens’ work emphasizes the use of these robust, non-parametric methods to provide more reliable and exact p-values. Their approach is part of a broader trend in econometrics and other social sciences toward more reliable inference methods that do not heavily rely on restrictive assumptions. By focusing on randomization-based methods, they help ensure that the conclusions drawn from empirical studies are more credible and less sensitive to potential violations of traditional model assumptions. This methodology is particularly valuable in experimental economics, field experiments, and observational studies where treatment effects are being evaluated. By using randomization-based methods, researchers can make stronger causal inferences and provide more robust evidence for policy and decision-making. 12.1.5.1 Jackknife The jackknife method is a resampling technique where each observation (in this case, each treated unit) is systematically omitted from the dataset, and the analysis is repeated each time to estimate the variance of the treatment effect. This provides a robust estimate of the standard errors that accounts for the potential variability across different treated units. 12.2 Bootstrapping bootstrapping is a procedure used to estimate the variance of an estimator. In the context of inverse probability weighting, we would repeatedly draw (“with replacement”) a random sample of our original data and then use that smaller sample to calculate the sample analogs of the ATE or ATT. standard bootstrapping wild bootstrapping "],["hypotheis-testing.html", "Chapter 13 Hypotheis Testing 13.1 Concepts 13.2 Null Hypothesis 13.3 Permutation Tests 13.4 Fischer’s Exact Test", " Chapter 13 Hypotheis Testing 13.1 Concepts 13.1.1 Significance Level (α) Definition: The significance level (α) is the probability threshold used in hypothesis testing to determine whether to reject the null hypothesis. It represents the maximum probability of incorrectly rejecting the null hypothesis when it is actually true. Commonly Used Values: Typical Value: α is commonly set at 0.05, meaning there is a 5% chance of incorrectly rejecting the null hypothesis. Other Values: Researchers may choose α levels such as 0.01 or 0.10 depending on the study’s requirements and the desired balance between Type I and Type II errors. Interpretation: If the calculated p-value is less than or equal to α, the results are considered statistically significant. If the p-value is greater than α, the results are not statistically significant, suggesting that the null hypothesis cannot be rejected. 13.1.2 P-Value Definition: The p-value is the probability of obtaining test results at least as extreme as the observed results, under the assumption that the null hypothesis is true. It measures the strength of evidence against the null hypothesis. Key Points: Lower p-value: Indicates stronger evidence against the null hypothesis. A small p-value (typically ≤ α) suggests that the observed results are unlikely to occur if the null hypothesis is true, leading to rejection of the null hypothesis. Higher p-value: Indicates weaker evidence against the null hypothesis. A larger p-value (typically &gt; α) suggests that the observed results are reasonably likely to occur even if the null hypothesis is true, leading to failure to reject the null hypothesis. Interpretation: p ≤ α: The results are statistically significant, suggesting that the observed effect is unlikely due to chance alone. p &gt; α: The results are not statistically significant, suggesting that the observed effect could reasonably occur due to chance. Type I and Type II errors are concepts used in hypothesis testing and statistical decision-making, describing the errors that can occur when making conclusions about the population based on sample data. Here’s an explanation of each: 13.1.3 Type I Error Definition: A Type I error occurs when the null hypothesis (H₀) is incorrectly rejected, even though it is actually true. In other words, it represents the situation where the researcher concludes there is an effect or relationship when, in fact, there is no such effect or relationship in the population. Probability and Significance Level: - The probability of committing a Type I error is equal to the significance level (α) chosen for the hypothesis test. For example, if α is set at 0.05, there is a 5% chance of mistakenly rejecting the null hypothesis when it is true. Type I error This happens when we reject the null hypothesis when it should not be rejected. Type I error rate is the probability when Type I error happens, also known as significance level, or \\(\\alpha\\). A common value for alpha is 0.05. 13.1.4 Type II Error Definition: A Type II error occurs when the null hypothesis (H₀) is incorrectly not rejected, even though it is false. It represents the situation where the researcher fails to detect an effect or relationship that actually exists in the population. Probability and Power: - The probability of committing a Type II error is denoted as β (beta). - Power (1 - β) represents the probability of correctly rejecting the null hypothesis when it is false. - Type II error rate is influenced by factors such as sample size, effect size, and variability in the data. Type II error This happens when we fail to reject the null hypothesis when it should be rejected. Type II error rate is also known as \\(\\beta\\). 13.1.5 Relationship Between Type I and Type II Errors Inverse Relationship: As the significance level (α) decreases (making Type I errors less likely), the probability of committing a Type II error (β) tends to increase, and vice versa. Balancing Act: Researchers aim to strike a balance between Type I and Type II errors depending on the context and consequences of each error type. Power Analysis: Conducted to determine an appropriate sample size to minimize both types of errors and maximize the likelihood of detecting real effects. 13.1.6 Importance in Research and Decision Making Statistical Rigor: Understanding and controlling Type I and Type II error rates are essential for maintaining the integrity and reliability of research findings. Impact: Errors can have significant implications in fields such as medicine, psychology, economics, and policy-making, influencing decisions based on study results. In summary, Type I and Type II errors are critical concepts in hypothesis testing, highlighting the trade-offs and risks involved in drawing conclusions from sample data about the population. Proper consideration and calculation of these errors are vital for ensuring valid and meaningful research outcomes. 13.1.7 Statistical power Statistical power refers to the probability that a hypothesis test correctly rejects the null hypothesis when it should be rejected. It is denoted as \\(1 - \\beta\\), where \\(\\beta\\) is the probability of a Type II error (failing to reject the null hypothesis when it is false). A commonly accepted level of statistical power is 0.80, which corresponds to a Type II error rate (β) of 0.20. Achieving sufficient statistical power is crucial for obtaining reliable and meaningful results in research. Sample size plays a critical role in determining statistical power. For instance, when comparing two means, the calculation of statistical power can be based on factors such as the significance level (α), effect size (δ), and sample size (n). In summary, statistical power measures the ability of a study to detect a true effect or relationship, and it depends on various factors that should be carefully considered during the design and interpretation of experiments. The formula for calculating statistical power is given by: \\[ \\text{Power} = 1 - \\beta = 1 - P(\\text{Type II Error}) = P(\\text{Reject } H_0 | H_0 \\text{ is false}) \\] where: $ : $ $ H_0 : $ Increasing sample size or effect size generally increases statistical power, while reducing the significance level increases the probability of Type II Error. As sample size increases, the statistical power increases. Therefore, for our test to have desirable statistical power (usually 0.80), we want to estimate the minimum sample size required. 13.2 Null Hypothesis The debate between Fisher and Neyman on the null hypothesis in causal inference revolves around different interpretations and implications of the null hypothesis in the context of randomized experiments. 13.2.1 Fisher’s Sharp Null Hypothesis Sharp Null Hypothesis: Fisher’s sharp null hypothesis is the assertion that every single unit in the population has a treatment effect of zero. Implication: Under this null hypothesis, the treatment has no effect whatsoever on any unit. This is a very strong statement, implying that the treatment effect is exactly zero for all individuals. Testing: This allows for a precise and exact test of the null hypothesis because it specifies the potential outcomes for every unit. In this scenario, you can use randomization tests to calculate exact p-values by comparing observed outcomes to those expected under complete nullification of treatment effects. 13.2.2 Neyman’s Null Hypothesis Average Treatment Effect (ATE) Null Hypothesis: Neyman, in contrast, proposed a weaker form of the null hypothesis, which asserts that the average treatment effect (ATE) across all units is zero. Implication: This means that, on average, the treatment does not have an effect, but it allows for the possibility that some units could have positive treatment effects while others have negative effects, as long as they cancel out in the aggregate. Testing: Testing this hypothesis typically involves estimating the average treatment effect and assessing its statistical significance. This approach does not specify the exact potential outcomes for each unit, making it more general but less powerful for exact testing. 13.2.3 Key Differences Stringency: Fisher’s sharp null hypothesis is more stringent because it makes a precise statement about every unit’s treatment effect being zero. Neyman’s null hypothesis is less stringent because it only concerns the average effect across the population. Testing Methodologies: Under Fisher’s sharp null, one can use permutation or randomization tests to obtain exact p-values, as the null hypothesis specifies the exact distribution of outcomes under no treatment effect. Under Neyman’s null, one typically relies on estimations and asymptotic properties to test the significance of the estimated ATE. This involves confidence intervals and standard errors. Implications for Experimental Design: Fisher’s approach allows for stronger conclusions in terms of causality for each unit but requires stronger assumptions. Neyman’s approach provides a broader inference about the average effect, which is often more realistic in practical scenarios where treatment effects can vary across units. 13.2.4 Example to Illustrate the Difference Suppose we conduct an experiment to test the effect of a new drug on blood pressure. We have two groups: a treatment group and a control group. Fisher’s Sharp Null Hypothesis: The null hypothesis here would state that the new drug has no effect on blood pressure for every individual in the treatment group. This means if an individual’s blood pressure would be 120 without the drug, it remains 120 with the drug. If we observe a difference between the treatment and control groups, we can use randomization tests to see if this difference is likely to occur by chance under the sharp null. Neyman’s ATE Null Hypothesis: The null hypothesis here would state that the average change in blood pressure due to the drug across all individuals is zero. This allows for some individuals to experience a decrease in blood pressure and others an increase, as long as these changes average out to zero. Here, we would estimate the ATE and test if it is significantly different from zero using statistical inference methods. 13.2.5 Conclusion The debate between Fisher and Neyman highlights a fundamental difference in how causal effects are conceptualized and tested in statistics. Fisher’s sharp null hypothesis allows for precise testing with exact p-values but requires stronger assumptions, while Neyman’s ATE null hypothesis is more flexible and realistic but relies on estimation and inference methods that are less precise in defining individual treatment effects. Understanding both approaches provides a comprehensive view of hypothesis testing in the context of causal inference. 13.3 Permutation Tests A permutation test (also known as a randomization test or re-randomization test). This method is used in econometrics and statistics to test the null hypothesis when the assumptions required for traditional parametric tests (like the t-test) may not hold, particularly in the context of small sample sizes or when the distribution of the test statistic under the null hypothesis is complex or unknown. 13.3.1 When and Why to Use Permutation Tests Non-Parametric Nature: When: Permutation tests are useful when the data do not necessarily meet the assumptions of parametric tests, such as normality or homogeneity of variance. Why: Because they are non-parametric, permutation tests do not rely on the underlying distribution of the data, making them more robust in certain situations. Small Sample Sizes: When: They are particularly valuable when dealing with small sample sizes where the Central Limit Theory may not apply, and thus, the sampling distribution of the test statistic under the null hypothesis is not well-approximated by a normal distribution. Why: In small samples, traditional methods like the t-test may not be reliable. Permutation tests use the actual data to construct the distribution of the test statistic, which can provide a more accurate p-value. Exact Test: When: When you need an exact test rather than an approximate one. Why: Permutation tests generate the exact distribution of the test statistic under the null hypothesis by considering all possible reassignments of treatment labels, ensuring an accurate p-value. Complex Experimental Designs: When: In complex experimental designs where the structure of the data or the treatment assignment mechanism does not fit well with the assumptions of standard tests. Why: Permutation tests are flexible and can be adapted to a wide variety of experimental designs and data structures. 13.3.2 How to Perform a Permutation Test Here’s a step-by-step outline of the permutation test procedure: Calculate the Original Test Statistic: Compute the test statistic (e.g., the difference in means between treatment and control groups) for the observed data. Drop the Treatment Variable: Remove the treatment labels from the data, essentially pooling all the data together. Reassign Treatment Labels: Randomly reassign the treatment labels to the data, ensuring the same number of treatment and control units as in the original data. Calculate the Test Statistic for the New Assignment: Compute the test statistic for this new random assignment of treatment labels. Repeat the Process: Repeat the re-randomization and calculation of the test statistic many times (e.g., 1,000 or more) to build a distribution of the test statistic under the null hypothesis. Create the Empirical Distribution: Collect all the computed test statistics from the repeated random assignments to form an empirical distribution of the test statistic under the null hypothesis. Calculate the Empirical P-Value: Compare the original test statistic to this empirical distribution. The p-value is the proportion of test statistics in the empirical distribution that are as extreme as, or more extreme than, the original test statistic. 13.3.3 Example Calculation Observed Data: Suppose you have two groups, treatment (\\(Y_T\\)) and control (\\(Y_C\\)), and you calculate the observed difference in means, \\(\\Delta_{obs} = \\bar{Y}_T - \\bar{Y}_C\\). Reassign Labels: Randomly shuffle the combined data and reassign the treatment and control labels. Compute New Statistic: Calculate the new difference in means for this re-randomized data, \\(\\Delta_{rand}\\). Repeat: Perform steps 2 and 3, say 1,000 times, to get a distribution of \\(\\Delta_{rand}\\). Compare: Determine where \\(\\Delta_{obs}\\) falls within the distribution of \\(\\Delta_{rand}\\). If \\(\\Delta_{obs}\\) is in the extreme tails of this distribution, it suggests that the observed effect is unlikely to have occurred by random chance. P-Value: Calculate the p-value as the proportion of \\(\\Delta_{rand}\\) values that are as extreme or more extreme than \\(\\Delta_{obs}\\). 13.3.4 Conclusion Permutation tests are a powerful tool for testing hypotheses in situations where traditional assumptions may not hold. By using the actual data to generate the null distribution of the test statistic, permutation tests provide a robust, non-parametric method for hypothesis testing, ensuring accurate p-values even in complex or small-sample scenarios. 13.4 Fischer’s Exact Test Fisher’s Exact Test is a statistical test used to determine if there are nonrandom associations between two categorical variables. It is particularly useful when sample sizes are small, and the assumptions of the chi-square test (like the expected frequency in each cell being at least 5) are not met. The test is named after the famous statistician Ronald A. Fisher. 13.4.1 When to Use Fisher’s Exact Test Small Sample Sizes: Fisher’s Exact Test is often used when dealing with small sample sizes, where the chi-square test may not be appropriate. Categorical Data: It is used for categorical data to test for independence between two variables. 2x2 Contingency Tables: While the test can be extended to larger tables, it is most commonly applied to 2x2 contingency tables. 13.4.2 How Fisher’s Exact Test Works 13.4.2.1 Example Consider the following 2x2 contingency table: Treatment Control Total Success a b a + b Failure c d c + d Total a + c b + d n Where: - \\(a\\): Number of successes in the treatment group - \\(b\\): Number of successes in the control group - \\(c\\): Number of failures in the treatment group - \\(d\\): Number of failures in the control group - \\(n\\): Total number of observations 13.4.2.2 Calculating the P-Value Fisher’s Exact Test calculates the exact probability of obtaining a distribution of values in the contingency table given the observed marginal totals. The p-value is computed by summing the probabilities of all possible tables that have the same row and column totals as the observed table and have a test statistic as extreme as, or more extreme than, the observed one. The probability of any particular table is given by the hypergeometric distribution: \\[ P = \\frac{\\binom{a+b}{a} \\binom{c+d}{c}}{\\binom{n}{a+c}} \\] Where \\(\\binom{n}{k}\\) is the binomial coefficient, representing the number of ways to choose \\(k\\) successes out of \\(n\\) trials. 13.4.3 Step-by-Step Example Let’s say we have the following data from a clinical trial: Treatment Control Improved 8 2 Not Improved 1 5 We want to test if there is a significant association between the treatment and the improvement. Construct the Contingency Table: Treatment Control Total Improved 8 2 10 Not Improved 1 5 6 Total 9 7 16 Calculate the P-Value: The p-value for Fisher’s Exact Test is the sum of the probabilities of all tables that are as extreme as or more extreme than the observed table, given the marginal totals. For the given table: \\[ P = \\frac{\\binom{10}{8} \\binom{6}{1}}{\\binom{16}{9}} = \\frac{45 \\times 6}{11440} \\approx 0.0236 \\] Interpret the Result: If the p-value (0.0236) is less than the chosen significance level (commonly 0.05), we reject the null hypothesis and conclude that there is a significant association between the treatment and the improvement. 13.4.4 Conclusion Fisher’s Exact Test is a powerful tool for analyzing contingency tables, especially when sample sizes are small. It provides an exact p-value for the test of independence between two categorical variables, making it a preferred choice when the assumptions of the chi-square test are not satisfied. By using this test, researchers can make accurate inferences about the relationships between categorical variables even in studies with limited data. "],["maximum-likelihood-estimation.html", "Chapter 14 Maximum Likelihood Estimation", " Chapter 14 Maximum Likelihood Estimation very nice website [https://www.statlect.com/fundamentals-of-statistics/linear-regression-maximum-likelihood] "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

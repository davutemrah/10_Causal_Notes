
## On how parameters are calculated

### Propensity Score Matching (PSM) and Maximum Likelihood Estimation (MLE)

**Does PSM use Maximum Likelihood Estimation (MLE)?**

- Yes, PSM typically uses logistic regression (or probit regression) to estimate propensity scores, and logistic regression uses MLE to estimate coefficients.

**Does every logistic regression use MLE?**

- Yes, logistic regression commonly uses MLE to estimate the model parameters.

### Logistic Regression

#### Objective Function and Loss Function

- **Objective Function**: The objective in logistic regression is to maximize the likelihood function, i.e., the probability of observing the given sample.

- **Loss Function**: The log-likelihood function is used as the loss function in logistic regression, which is minimized (or equivalently, the negative log-likelihood is maximized).

#### Calculating Coefficients

- Coefficients in logistic regression are estimated using MLE. The likelihood function for logistic regression is:
  \[
  L(\beta) = \prod_{i=1}^n P(y_i|\mathbf{x}_i;\beta)^{y_i}(1 - P(y_i|\mathbf{x}_i;\beta))^{1 - y_i}
  \]
  where \( P(y_i|\mathbf{x}_i;\beta) = \frac{1}{1 + \exp(-\mathbf{x}_i^T \beta)} \).

  The log-likelihood function is:
  \[
  \log L(\beta) = \sum_{i=1}^n \left[ y_i \log P(y_i|\mathbf{x}_i;\beta) + (1 - y_i) \log (1 - P(y_i|\mathbf{x}_i;\beta)) \right]
  \]

  The parameters \(\beta\) are estimated by maximizing this log-likelihood function.

#### Hypothesis Testing

- **Z-test**: Logistic regression typically uses z-tests to test hypotheses about the coefficients.

Null hypothesis: The coefficient is equal to zero.

The z-statistic is calculated as the coefficient estimate divided by its standard error:

$z = \frac{\hat{\beta}}{\text{SE}(\hat{\beta})}$

The p-value is derived from the standard normal distribution.



### Ordinary Least Squares (OLS) Regression

#### Objective Function and Loss Function

- **Objective Function**: The objective in OLS regression is to minimize the sum of squared residuals.

- **Loss Function**: The loss function in OLS is the residual sum of squares (RSS):
  
$RSS = \sum_{i=1}^n (y_i - \mathbf{x}_i^T \beta)^2$

#### Calculating Coefficients

- Coefficients in OLS regression are estimated by minimizing the RSS. The normal equations derived from setting the gradient of RSS to zero are:
  
$\mathbf{\hat{\beta}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}$

  where \(\mathbf{X}\) is the design matrix of predictors and \(\mathbf{y}\) is the vector of observed outcomes.


#### Hypothesis Testing

- **T-test**: OLS regression typically uses t-tests to test hypotheses about the coefficients.
  
  - Null hypothesis: The coefficient is equal to zero.
  
  - The t-statistic is calculated as the coefficient estimate divided by its standard error:
    
    $t = \frac{\hat{\beta}}{\text{SE}(\hat{\beta})}$
    
  - The p-value is derived from the t-distribution with \(n - p - 1\) degrees of freedom (where \(p\) is the number of predictors).




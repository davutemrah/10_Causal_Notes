## Synthetic Difference-in-Differences (SynthDiD) method 



[Source Article](https://arxiv.org/abs/1812.09970)

(Arkhangelsky, Athey, Hirshberg, Imbens, Wager, 2021)


We present a new estimator for causal effects with panel data that builds on insights behind the widely used **difference in differences** and **synthetic control methods.**

Relative to these methods we find, both theoretically and empirically, that this "synthetic difference in differences" estimator has desirable robustness properties, and that it performs well in settings where the conventional estimators are commonly used in practice. 

We study the asymptotic behavior of the estimator when the systematic part of the outcome model includes latent unit factors interacted with latent time factors, and we present conditions for consistency and asymptotic normality.


### Introduction

[Reading](https://towardsdatascience.com/synthdid-101-a-beginners-guide-to-synthetic-difference-in-differences-84fed9b730ae)


**One-liner:** 

SDID blends **Synthetic Control (SCM)** and **Difference-in-Differences (DiD)** by learning **unit weights** (like SCM) and **time weights** (like DiD/event-study) to estimate an ATT in panel data. 

It reduces bias from level differences (SCM strength) *and* common shocks/time trends (DiD strength), and has a **doubly-robust flavor**: it’s consistent if either the SCM-style unit balancing or the DiD-style time balancing holds.

### What problem it solves

* **Plain DiD** can be biased when treated units differ in **levels/composition** from controls even if trends are parallel.

* **Plain SCM** can be biased when there are **uncontrolled time shocks** or with **short pre-periods** where matching levels alone isn’t enough.

* **SDID** addresses **both** by balancing **across units** *and* **across time**.

### How it works (intuition)

1. **Unit weights (SCM step):** Learn non-negative weights $w$ over control units so a weighted control “synthetic” group **matches the treated group in the pre-period** (levels/trends/predictors).

2. **Time weights (DiD step):** Learn non-negative weights $\lambda$ over time so pre- vs. post-period comparisons **down-weight noisy periods** and align common shocks.

3. **ATT via “synthetic” DiD:** Compute a **difference-in-differences** using these weights:

   $$
   \hat{\tau}_{\text{SDID}} =
   \big(\overline{Y}_{T,\text{post}} - \overline{Y}^{\,w}_{C,\text{post}}\big)
   \;-\;
   \big(\overline{Y}^{\,\lambda}_{T,\text{pre}} - \overline{Y}^{\,w,\lambda}_{C,\text{pre}}\big)
   $$

   where bars denote weighted averages over units/time.

### How it differs from related methods

* **SCM:** Builds a synthetic control and compares **post-treatment levels** of treated vs. synthetic.
  
  SDID instead compares **changes** (pre→post) of treated vs. synthetic (a DiD on top of SCM).

* **DiD:** Compares treated vs. control **changes** assuming parallel trends.
  
  SDID **reweights** controls (unit weights) and time (time weights) to **make** that assumption more plausible.

### When to use

* Panel data with **few to moderate pre-periods** and **many control units**.

* **One or a few treated units**, or a group treated at **the same time** (cohort).

* Concern about **level differences** *and* **time shocks** violating plain DiD/SCM.

### Assumptions (why it’s appealing)

* **No interference/anticipation** (standard SUTVA-style conditions).

* **Doubly-robust flavor:** Consistent if **either**

  * (i) the **DiD/parallel-trends condition** holds after learned weights, **or**

  * (ii) an **interactive fixed-effects/factor model** holds so SCM-style balancing captures confounding.

* Enough pre-period signal to learn weights (does **not** require long pre-periods, but needs some).

### Inference & diagnostics

* **Placebo/permutation tests**: apply SDID to untreated units as if treated.

* **Block bootstrap** or analytic SEs for uncertainty.

* **Pre-trend checks** with event-style plots using learned weights.

* **Sensitivity**: vary donor pool, pre-window, or weight regularization.

### Pros / Cons

**Pros**

* Handles **level** differences (SCM) and **time shocks** (DiD).

* Good **finite-sample performance**; stable with moderate pre-periods.

* Clear visualization (treated vs. synthetic paths; pre/post gaps).

**Cons**

* Works best with **limited treated cohorts** (extensions needed for complex staggered adoption).

* Requires **panel structure** and some pre-period data.

* Sensitive to donor pool choice and weight regularization (check robustness).


### TL;DR (contrast)

* **SCM:** match pre-period levels → compare post **levels**.

* **DiD:** assume parallel trends → compare **changes**.

* **SDID:** **learn weights** to balance units and time → compare **changes of synthetic vs. treated**.


### An Example:

Suppose that we are a company that sells plant-based food products, such as soy milk or soy yogurt, and we operate in multiple countries. Some countries implement new legislation that prohibits us from marketing our plant-based products as ‘milk’ or ‘yogurt’ because it is claimed that only animal products can be marketed as ‘milk’ or ‘yogurt’. Thus, due to this new regulation in some countries, we have to market soy milk as soy drink instead of soy milk, etc. We want to know the impact of this legislation on our revenue as this might help guide our lobbying efforts and marketing activities in different countries.

I simulated a balanced panel dataset that shows the revenue of our company in 30 different countries for 30 periods. Three of the countries implement this legislation in period 20. In the figure below, you can see a snapshot of the data. treat is a dummy variable indicating whether a country has implemented the legislation in a given period. revenueis the revenue in millions of EUR. You can find the simulation and estimation code in this Gist.


```
# Install and load the required packages
# devtools::install_github("synth-inference/synthdid")
library(synthdid)
library(ggplot2)
library(fixest) # Fixed-effects regression
library(data.table)
```


```
# Set seed for reproducibility
set.seed(12345)

source('sim_data.R') # Import simulation function and some utilities

dt <- sim_data()
head(dt)
```

In Data, there are 30 units (3 units treated), 30 periods (10 periods treated), all units are treated at the same time.

Next, we convert our panel data into a matrix required by the synthdid package. Given the outcome, treatment and control units and pretreatment periods, a synthetic control is created and treatment effect is estimated with synthdid_estimate function. 


```
# Convert the data into a matrix
setup = panel.matrices(dt, unit = 'country', time = 'period', 
                       outcome = 'revenue', treatment = 'treat')

# Estimate treatment effect using SynthDiD
tau.hat = synthdid_estimate(setup$Y, 
                            setup$N0,
                            setup$T0)
print(summary(tau.hat))
```

To make inference, we also need to calculate the standard errors. I use jacknife method as I have more than one treated units. Placebo method is the only option if you have one treatment unit. Given the standard errors, I also calculate the 95% confidence interval for the treatment effect. I will report these in the figure below.


When there are **multiple treated units** (more than one unit that received the treatment or intervention), one common approach to estimating standard errors is using the jackknife method. The jackknife method is a resampling technique where each observation (in this case, each treated unit) is systematically omitted from the dataset, and the analysis is repeated each time to estimate the variance of the treatment effect. This provides a robust estimate of the standard errors that accounts for the potential variability across different treated units.

On the other hand, if there is only **one treated unit** (a single unit that received the treatment), using the jackknife method becomes impractical because there are not enough units to systematically leave out and still perform meaningful resampling. In such cases, the **placebo method** becomes a viable option. 

The placebo method involves creating placebo or synthetic treated units that mimic the characteristics of the treated unit but did not actually receive the treatment. By comparing the outcomes of the actual treated unit with those of the synthetic placebo units, researchers can estimate the variability and potential impact of the treatment effect more accurately.

Therefore, the choice between the jackknife method and the placebo method depends on the number of treated units available for analysis within the synthetic control framework. Multiple treated units allow for the application of the jackknife method, whereas a single treated unit necessitates the use of the placebo method to estimate standard errors and make reliable inferences about the treatment effect.


```
# Calculate standard errors 
se = sqrt(vcov(tau.hat, method='jackknife'))
te_est <- sprintf('Point estimate for the treatment effect: %1.2f', tau.hat)
CI <- sprintf('95%% CI (%1.2f, %1.2f)', tau.hat - 1.96 * se, tau.hat + 1.96 * se)

```

```
# Plot treatment effect estimates
plot(tau.hat)
plot(tau.hat, se.method='jackknife')
```




In the image below, the estimation results are displayed. Observe how the treated countries and the synthetic control exhibit fairly parallel trends on average (it might not look like a perfect parallel trends but that is not necessary for the sake of this example). The average for treated countries is more variable, primarily due to the presence of only three such countries, resulting in less smooth trends. Transparent gray lines represent different control countries. Following the treatment in period 20, a decline in revenue is observed in the treated countries, estimated to be 0.51 million EUR as indicated in the graph. This means that the new regulation has a negative impact on our company’s revenues and necessary actions should be taken to prevent further declines.


```
# Check the number of treatment and control countries to report
num_treated <- length(unique(dt[treat==1]$country))
num_control <- length(unique(dt$country))-num_treated

# Create spaghetti plot with top 10 control units
top.controls = synthdid_controls(tau.hat)[1:10, , drop=FALSE]
plot(tau.hat, spaghetti.units=rownames(top.controls),
     trajectory.linetype = 1, line.width=.75, 
     trajectory.alpha=.9, effect.alpha=.9,
     diagram.alpha=1, onset.alpha=.9, ci.alpha = .3, spaghetti.line.alpha =.2,
     spaghetti.label.alpha = .1, overlay = 1) + 
  labs(x = 'Period', y = 'Revenue', title = 'Estimation Results', 
       subtitle = paste0(te_est, ', ', CI, '.'), 
       caption = paste0('The number of treatment and control units: ', num_treated, ' and ', num_control, '.'))
```


Let’s plot the weights use to estimate the synthetic control.


```
# Plot control unit contributions
synthdid_units_plot(tau.hat, se.method='jackknife') +
  labs(x = 'Country', y = 'Treatment effect', 
       caption = 'The black horizontal line shows the actual effect; 
       the gray ones show the endpoints of a 95% confidence interval.')
ggsave('../figures/unit_weights.png')
```

In the image below, you can observe how each country is weighted to construct the synthetic control. The treatment effects differ based on the untreated country selected as the control unit.


```
# Check for pre-treatment parallel trends
plot(tau.hat, overlay=1, se.method='jackknife')
ggsave('../figures/results_simple.png')
```



```
# Check the number of treatment and control countries to report
num_treated <- length(unique(dt[treat==1]$country))
num_control <- length(unique(dt$country))-num_treated


# Create spaghetti plot with top 10 control units
top.controls = synthdid_controls(tau.hat)[1:10, , drop=FALSE]
plot(tau.hat, spaghetti.units=rownames(top.controls),
     trajectory.linetype = 1, line.width=.75, 
     trajectory.alpha=.9, effect.alpha=.9,
     diagram.alpha=1, onset.alpha=.9, ci.alpha = .3, spaghetti.line.alpha	=.2,
     spaghetti.label.alpha = .1, overlay = 1) + 
  labs(x = 'Period', y = 'Revenue', title = 'Estimation Results', 
       subtitle = paste0(te_est, ', ', CI, '.'), 
       caption = paste0('The number of treatment and control units: ', num_treated, ' and ', num_control, '.'))
ggsave('../figures/results.png')

fe <- feols(revenue~treat, dt, cluster = 'country', panel.id = 'country', 
      fixef = c('country', 'period'))
summary(fe)
```




### Conclusion

Now that we understand more about SynthDiD let’s talk about pros and cons of this method. 

There are some advantages and disadvantages to SynthDiD like every method. Here are some pros and cons to keep in mind when getting started with this method.

#### Advantages of SynthDiD method:

- The synthetic control method is usually used for a few treated and control units and needs long, balanced data before treatment. SynthDiD, on the other hand, works well even with a short data period before treatment, unlike the synthetic control method [4].


- This method is being preferred especially because it doesn’t have a strict parallel trends assumption (PTA) requirement like DiD.

- SynthDiD guarantees a suitable quantity of control units, considers possible pre-intervention patterns, and may accommodate a degree of endogenous treatment timing [4].

#### Disadvantages of SynthDiD method:

- Can be computationally expensive (even with only one treated group/block).

- Requires a balanced panel (i.e., you can only use units observed for all time periods) and that the treatment timing is identical for all treated units.

- Requires enough pre-treatment periods for good estimation, so, if you don’t have enough pre-treatment period might be better to use just the regular DiD.

- Computing and comparing the average treatment effects for subgroups is tricky. One option is to split the sample into subgroups and compute the average treatment effects for each subgroup.

- Implementing SynthDiD where the treatment timing varies might be tricky. In the case of staggered treatment timing, as one solution, one can estimate the average treatment effect for each treatment cohort and then aggregate cohort-specific average treatment effects to an overall average treatment effects.
H
ere are also some other points that you might want to know when getting started.

**Things to note:**

SynthDiD employs regularized ridge regression (L2) while ensuring that the resulting weights have a sum of one.

In the process of pretreatment matching, SynthDiD tries to determine the average treatment effect across the entire sample. This approach might cause individual time period estimates to be less precise. Nonetheless, the overall average yields an unbiased evaluation.

The standard errors for the treatment effects are estimated with jacknife or if a cohort has only one treated unit with placebo method.

The estimator is considered consistent and asymptotically normal, given that the combination of the number of control units and pretreatment periods is sufficiently large relative to the combination of the number of treated units and posttreatment periods.

In practice, pre-treatment variables play a minor role in Synthetic DiD, as lagged outcomes hold more predictive power, making the treatment of these variables less critical.

**Conclusion**

In this blog post, I introduce the SynthDiD method and discuss its relationship with traditional DiD and SCM. SynthDiD combines the strengths of both SCM and DiD, allowing for causal inference with large panels even when the pretreatment period is short. I demonstrate the method using the synthdid package in R. Although it has several advantages, such as not requiring a strict parallel trends assumption, it also has drawbacks, like being computationally expensive and requiring a balanced panel. Overall, SynthDiD is a valuable tool for researchers interested in estimating causal effects using observational data, providing an alternative to traditional DiD and SCM methods.
